{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Introduction to RNA-Seq: From quality control to pathway analysis Learning outcomes General learning outcomes After this course, you will be able to: Describe advantages and pitfalls of RNA sequencing. Design your own experiment. Perform the downstream analysis using command line software (QC, mapping, counting, differential expression analysis, pathway analysis, etc). Critically assess the quality of your results at each step of the downstream analysis. Detect significantly differentially-expressed genes between conditions. Learning experiences To reach the learning outcomes, we will use lectures and exercises. During lectures, do not hesitate to ask any questions as we progress through the slides. During exercises, you are free to discuss with other participants. Exercises are provided with solutions. How you use them to your advantage is up to you.","title":"Home"},{"location":"index.html#introduction-to-rna-seq-from-quality-control-to-pathway-analysis","text":"","title":"Introduction to RNA-Seq: From quality control to pathway analysis"},{"location":"index.html#learning-outcomes","text":"","title":"Learning outcomes"},{"location":"index.html#general-learning-outcomes","text":"After this course, you will be able to: Describe advantages and pitfalls of RNA sequencing. Design your own experiment. Perform the downstream analysis using command line software (QC, mapping, counting, differential expression analysis, pathway analysis, etc). Critically assess the quality of your results at each step of the downstream analysis. Detect significantly differentially-expressed genes between conditions.","title":"General learning outcomes"},{"location":"index.html#learning-experiences","text":"To reach the learning outcomes, we will use lectures and exercises. During lectures, do not hesitate to ask any questions as we progress through the slides. During exercises, you are free to discuss with other participants. Exercises are provided with solutions. How you use them to your advantage is up to you.","title":"Learning experiences"},{"location":"course_schedule.html","text":"Day 1 start end subject 9:00 9:15 Welcome 9:15 10:45 RNAseq - technologies and design 10:45 11:00 BREAK 11:00 11:30 Server login + unix fresh up 11:30 12:30 Quality control 12:30 13:30 LUNCH BREAK 13:30 14:30 Sequence Trimming and adapter removal 14:30 15:30 Reads mapping : indexing genome 15:30 15:45 BREAK 15:45 17:00 Reads mapping : mapping Day 2 start end subject 9:00 9:15 Recap of yesterday 9:15 10:30 Read counting 10:30 10:45 BREAK 10:30 12:30 Differential Expression Inference - theory 12:30 13:30 LUNCH BREAK 13:30 17:00 Differential Expression Inference - practice","title":"Course schedule"},{"location":"course_schedule.html#day-1","text":"start end subject 9:00 9:15 Welcome 9:15 10:45 RNAseq - technologies and design 10:45 11:00 BREAK 11:00 11:30 Server login + unix fresh up 11:30 12:30 Quality control 12:30 13:30 LUNCH BREAK 13:30 14:30 Sequence Trimming and adapter removal 14:30 15:30 Reads mapping : indexing genome 15:30 15:45 BREAK 15:45 17:00 Reads mapping : mapping","title":"Day 1"},{"location":"course_schedule.html#day-2","text":"start end subject 9:00 9:15 Recap of yesterday 9:15 10:30 Read counting 10:30 10:45 BREAK 10:30 12:30 Differential Expression Inference - theory 12:30 13:30 LUNCH BREAK 13:30 17:00 Differential Expression Inference - practice","title":"Day 2"},{"location":"precourse.html","text":"Precourse preparations On top of a thirst for knowledge, and a working Internet connection, here is what you will need for the course : NGS As announced in the course registration webpage , we expect participants to already have a basic knowledge in Next Generation Sequencing (NGS) techniques. UNIX Practical knowledge of the UNIX command line is also required to be able to follow this course, given that that the tools used to process sequenced reads use this interface. If you are unsure about your capabilities or feel a bit rusty, we strongly recommend you spend some time practicing before the course : in our experience, the more comfortable you are with UNIX, the more you will be able to focus on the RNA-seq during the course, and the more you will gain from it. You may refer to the SIB\u2019s UNIX e-learning module R A basic knowledge of the R language is required to perform most analytical steps after reads have been mapped and quantified : differential gene expression, gene set enrichment, over-representation analysis. If you are not familiar with R, we recommend the SIB First Steps with R course , or you can pick one among this list Software To replicate the technical condition of today\u2019s real-life data analysis, we will perform our computations on a distant HPC cluster. To access it: macOS / Linux : you can use your pre-installed terminal. Windows : you should install a terminal which lets you do ssh (for instance mobaXterm ). Additionally, a graphical client for file transfer to and from the distant server can be useful. MobaXterm integrates this functionality, so if you use it there is no need for additional software. Otherwise, we recommend FileZilla .","title":"Precourse preparations"},{"location":"precourse.html#precourse-preparations","text":"On top of a thirst for knowledge, and a working Internet connection, here is what you will need for the course :","title":"Precourse preparations"},{"location":"precourse.html#ngs","text":"As announced in the course registration webpage , we expect participants to already have a basic knowledge in Next Generation Sequencing (NGS) techniques.","title":"NGS"},{"location":"precourse.html#unix","text":"Practical knowledge of the UNIX command line is also required to be able to follow this course, given that that the tools used to process sequenced reads use this interface. If you are unsure about your capabilities or feel a bit rusty, we strongly recommend you spend some time practicing before the course : in our experience, the more comfortable you are with UNIX, the more you will be able to focus on the RNA-seq during the course, and the more you will gain from it. You may refer to the SIB\u2019s UNIX e-learning module","title":"UNIX"},{"location":"precourse.html#r","text":"A basic knowledge of the R language is required to perform most analytical steps after reads have been mapped and quantified : differential gene expression, gene set enrichment, over-representation analysis. If you are not familiar with R, we recommend the SIB First Steps with R course , or you can pick one among this list","title":"R"},{"location":"precourse.html#software","text":"To replicate the technical condition of today\u2019s real-life data analysis, we will perform our computations on a distant HPC cluster. To access it: macOS / Linux : you can use your pre-installed terminal. Windows : you should install a terminal which lets you do ssh (for instance mobaXterm ). Additionally, a graphical client for file transfer to and from the distant server can be useful. MobaXterm integrates this functionality, so if you use it there is no need for additional software. Otherwise, we recommend FileZilla .","title":"Software"},{"location":"days/DE.html","text":"Once the reads have been mapped and counted, one can assess the differential expression of genes between different conditions. During this lesson, you will learn to : describe the different steps of data normalization and modelling commonly used for RNA-seq data. detect significantly differentially-expressed genes using either edgeR or DESeq2. perform downstream analysis on gene sets, such as annotation (e.g. GO terms or Reactome pathways) over-representation. Material Download the presentation Rstudio website Note RStudio is set to be rebranded as Posit after October 2022. edgeR user\u2019s guide DESeq2 vignette Connexion to the Rstudio server Note This step is intended only for users who attend the course with a teacher. Otherwise you will have to rely on your own installation of Rstudio. The analysis of the read count data will be done on an RStudio instance, using the R language and some relevant Bioconductor libraries. As you start your session on the RStudio server, please make sure that you know where your data is situated with respect to your working directory (use getwd() and setwd() to respectively : know what your working is, and change it as necessary). Differential Expression Inference Use either edgeR or DESeq2 to conduct a differential expression analysis on the Ruhland2016 and/or Liu2015 dataset. You can find the expression matrices on the server at: /shared/data/Solutions/Ruhland2016/countFiles/featureCounts_Ruhland2016.counts.txt and /shared/data/Solutions/Liu2015/countFiles/featureCounts_Liu2015.counts.txt Or you may download them : Liu2015 count matrix Ruhland2016 count matrix Note Generally, users find the syntax and workflow of DESeq2 easier for getting started. If you have the time, conduct a differential expression analysis using both DESeq2 and edgeR. Follow the vignettes/user\u2019s guide! They are the most up-to-date and generally contain everything a newcomer might need, including worked-out examples. DESeq2 DESeq2 vignette read in the data # setup library ( DESeq2 ) library ( ggplot2 ) # reading the counts files - adapt the file path to your situation raw_counts <- read.table ( '/shared/data/Solutions/Ruhland2016/countFiles/featureCounts_Ruhland2016.counts.txt' , skip = 1 , sep = \"\\t\" , header = T ) # setting up row names as ensembl gene ids row.names ( raw_counts ) = raw_counts $ Geneid ## looking at the beginning of that table raw_counts [ 1 : 5 , 1 : 5 ] # removing these first columns to keep only the sample counts raw_counts = raw_counts [ , -1 : -6 ] # changing column names names ( raw_counts ) = gsub ( '_.*' , '' , gsub ( '.*.SRR[0-9]{7}_' , '' , names ( raw_counts ) ) ) # some checking of what we just read head ( raw_counts ); tail ( raw_counts ); dim ( raw_counts ) colSums ( raw_counts ) # total number of counted reads per sample preprocessing ## telling DESeq2 what the experimental design was # note: by default, the 1st level is considered to be the reference/control/WT/... treatment <- factor ( c ( rep ( \"EtOH\" , 3 ), rep ( \"TAM\" , 3 )), levels = c ( \"EtOH\" , \"TAM\" ) ) colData <- data.frame ( treatment , row.names = colnames ( raw_counts )) colData ## creating the DESeq data object & positing the model dds <- DESeqDataSetFromMatrix ( countData = raw_counts , colData = colData , design = ~ treatment ) dim ( dds ) ## filter low count genes. Here, only keep genes with at least 2 samples where there are at least 5 reads. idx <- rowSums ( counts ( dds , normalized = FALSE ) >= 5 ) >= 2 dds.f <- dds [ idx , ] dim ( dds.f ) # we go from 55414 to 19378 genes Around 19k genes pass our minimum expression threshold, quite typical for a bulk Mouse RNA-seq experiment. estimate dispersion / model fitting # we perform the estimation of dispersions dds.f <- DESeq ( dds.f ) # we plot the estimate of the dispersions # * black dot : raw # * red dot : local trend # * blue : corrected plotDispEsts ( dds.f ) # extracting results for the treatment versus control contrast res <- results ( dds.f ) This plot is not easy to interpret. It represents the amount of dispersion at different levels of expression. It is directly linked to our ability to detect differential expression. Here it looks about normal compared to typical bulk RNA-seq experiments : the dispersion is comparatively larger for lowly expressed genes. looking at the results # adds estimate of the LFC the results table. # This shrunk logFC estimate is more robust than the raw value head ( coef ( dds.f )) # the second column corresponds to the difference between the 2 conditions res.lfc <- lfcShrink ( dds.f , coef = 2 , res = res ) #plotting to see the difference. par ( mfrow = c ( 2 , 1 )) DESeq2 :: plotMA ( res ) DESeq2 :: plotMA ( res.lfc ) # -> with shrinkage, the significativeness and logFC are more consistent par ( mfrow = c ( 1 , 1 )) Without the shrinkage, we can see that for low counts we can see a high log-fold change but non significant (ie. we see a large difference but with variance is also so high that this observation may be due to chance only). The shrinkage corrects this and the relationship between logFC and significance is smoother. # we apply the variance stabilising transformation to make the read counts comparable across libraries # (nb : this is not needed for DESeq DE analysis, but rather for visualisations that compare expression across samples, such as PCA. This replaces normal PCA scaling) vst.dds.f <- vst ( dds.f , blind = FALSE ) vst.dds.f.counts <- assay ( vst.dds.f ) plotPCA ( vst.dds.f , intgroup = c ( \"treatment\" )) The first axis (58% of the variance) seems linked to the grouping of interest. ## ggplot2-based volcano plot library ( ggplot2 ) FDRthreshold = 0.01 logFCthreshold = 1.0 # add a column of NAs res.lfc $ diffexpressed <- \"NO\" # if log2Foldchange > 1 and pvalue < 0.01, set as \"UP\" res.lfc $ diffexpressed [ res.lfc $ log2FoldChange > logFCthreshold & res.lfc $ padj < FDRthreshold ] <- \"UP\" # if log2Foldchange < 1 and pvalue < 0.01, set as \"DOWN\" res.lfc $ diffexpressed [ res.lfc $ log2FoldChange < - logFCthreshold & res.lfc $ padj < FDRthreshold ] <- \"DOWN\" ggplot ( data = data.frame ( res.lfc ) , aes ( x = log2FoldChange , y = - log10 ( padj ) , col = diffexpressed ) ) + geom_point () + geom_vline ( xintercept = c ( - logFCthreshold , logFCthreshold ), col = \"red\" ) + geom_hline ( yintercept =- log10 ( FDRthreshold ), col = \"red\" ) + scale_color_manual ( values = c ( \"blue\" , \"grey\" , \"red\" )) table ( res.lfc $ diffexpressed ) DOWN NO UP 131 19002 245 library ( pheatmap ) topVarGenes <- head ( order ( rowVars ( vst.dds.f.counts ), decreasing = TRUE ), 20 ) mat <- vst.dds.f.counts [ topVarGenes , ] #scaled counts of the top genes mat <- mat - rowMeans ( mat ) # centering pheatmap ( mat ) # saving results to file # note: a CSV file can be imported into Excel write.csv ( res , 'Ruhland2016.DESeq2.results.csv' ) EdgeR edgeR user\u2019s guide read in the data library ( edgeR ) library ( ggplot2 ) # reading the counts files - adapt the file path to your situation raw_counts <- read.table ( '.../Ruhland2016_featureCount_result.counts' , skip = 1 , sep = \"\\t\" , header = T ) # setting up row names as ensembl gene ids row.names ( raw_counts ) = raw_counts $ Geneid # removing these first columns to keep only the sample counts raw_counts = raw_counts [ , -1 : -6 ] # changing column names names ( raw_counts ) = gsub ( '_.*' , '' , gsub ( '.*.SRR[0-9]{7}_' , '' , names ( raw_counts ) ) ) # some checking of what we just read head ( raw_counts ); tail ( raw_counts ); dim ( raw_counts ) colSums ( raw_counts ) # total number of counted reads per sample edgeR object preprocessing # setting up the experimental design AND the model # -> the first 3 samples form a group, the 3 remaining are the other group treatment <- c ( rep ( \"EtOH\" , 3 ), rep ( \"TAM\" , 3 )) dge.f.design <- model.matrix ( ~ treatment ) # creating the edgeR DGE object dge.all <- DGEList ( counts = raw_counts , group = treatment ) # filtering by expression level. See ?filterByExpr for details keep <- filterByExpr ( dge.all ) dge.f <- dge.all [ keep , keep.lib.sizes = FALSE ] table ( keep ) keep FALSE TRUE 39702 15712 Around 16k genes are sufficiently expressed to be retained. #normalization dge.f <- calcNormFactors ( dge.f ) dge.f $ samples Each sample has been associated with a normalization factor. edgeR model fitting # estimate of the dispersion dge.f <- estimateDisp ( dge.f , dge.f.design , robust = T ) plotBCV ( dge.f ) This plot is not easy to interpret. It represents the amount of biological variation at different levels of expression. It is directly linked to our ability to detect differential expression. Here it looks about normal compared to other bulk RNA-seq experiments : the variation is comparatively larger for lowly expressed genes. # testing for differential expression. # This method is recommended when you only have 2 groups to compare dge.f.et <- exactTest ( dge.f ) topTags ( dge.f.et ) # printing the genes where the p-value of differential expression if the lowest Comparison of groups: TAM-EtOH logFC logCPM PValue FDR ENSMUSG00000050272 -8.522762 4.988067 2.554513e-28 3.851950e-24 ENSMUSG00000075014 3.890079 5.175181 2.036909e-25 1.535728e-21 ENSMUSG00000009185 3.837786 6.742422 1.553964e-22 7.810743e-19 ENSMUSG00000075015 3.778523 3.274463 2.106799e-22 7.942107e-19 ENSMUSG00000028339 -5.692069 6.372980 4.593720e-16 1.385374e-12 ENSMUSG00000040111 -2.141221 6.771538 4.954522e-15 1.245154e-11 ENSMUSG00000041695 4.123972 1.668247 6.057909e-15 1.304960e-11 ENSMUSG00000072941 3.609170 7.080257 1.807618e-14 3.407135e-11 ENSMUSG00000000120 -6.340146 6.351489 2.507019e-14 4.200371e-11 ENSMUSG00000034981 3.727969 5.244841 3.934957e-14 5.933521e-11 # see how many genes are DE summary ( decideTests ( dge.f.et , p.value = 0.01 )) # let's use 0.01 as a threshold TAM-EtOH Down 109 NotSig 15393 Up 210 The comparison is TAM-EtOH, so \u201cUp\u201d, corresponds to a higher in group TAM compared to group EtOH. edgeR looking at differentially-expressed genes ## plot all the logFCs versus average count size. Significantly DE genes are colored par ( mfrow = c ( 1 , 1 )) plotMD ( dge.f.et ) # lines at a log2FC of 1/-1, corresponding to a shift in expression of x2 abline ( h = c ( -1 , 1 ), col = \"blue\" ) ## Volcano plot allGenes = topTags ( dge.f.et , n = nrow ( dge.f.et $ table ) ) $ table FDRthreshold = 0.01 logFCthreshold = 1.0 # add a column of NAs allGenes $ diffexpressed <- \"NO\" # if log2Foldchange > 1 and pvalue < 0.01, set as \"UP\" allGenes $ diffexpressed [ allGenes $ logFC > logFCthreshold & allGenes $ FDR < FDRthreshold ] <- \"UP\" # if log2Foldchange < 1 and pvalue < 0.01, set as \"DOWN\" allGenes $ diffexpressed [ allGenes $ logFC < - logFCthreshold & allGenes $ FDR < FDRthreshold ] <- \"DOWN\" ggplot ( data = allGenes , aes ( x = logFC , y = - log10 ( FDR ) , col = diffexpressed ) ) + geom_point () + geom_vline ( xintercept = c ( - logFCthreshold , logFCthreshold ), col = \"red\" ) + geom_hline ( yintercept =- log10 ( FDRthreshold ), col = \"red\" ) + scale_color_manual ( values = c ( \"blue\" , \"grey\" , \"red\" )) ## writing the table of results write.csv ( allGenes , 'Ruhland2016.edgeR.results.csv' ) edgeR extra stuff # how to extract log CPM logcpm <- cpm ( dge.f , prior.count = 2 , log = TRUE ) # there is another fitting method reliying on quasi-likelihood, which is useful when the model is more complex (ie. more than 1 factor with 2 levels) dge.f.QLfit <- glmQLFit ( dge.f , dge.f.design ) dge.f.qlt <- glmQLFTest ( dge.f.QLfit , coef = 2 ) # you can see the results are relatively different. The order of genes changes a bit, and the p-values are more profoundly affected topTags ( dge.f.et ) topTags ( dge.f.qlt ) ## let's see how much the two methods agree: par ( mfrow = c ( 1 , 2 )) plot ( dge.f.et $ table $ logFC , dge.f.qlt $ table $ logFC , xlab = 'exact test logFC' , ylab = 'quasi-likelihood test logFC' ) print ( paste ( 'logFC pearson correlation coefficient :' , cor ( dge.f.et $ table $ logFC , dge.f.qlt $ table $ logFC ) ) ) plot ( log10 ( dge.f.et $ table $ PValue ), log10 ( dge.f.qlt $ table $ PValue ) , xlab = 'exact test p-values (log10)' , ylab = 'quasi-likelihood test p-values (log10)' ) print ( paste ( \"P-values spearman correlation coefficient\" , cor ( log10 ( dge.f.et $ table $ PValue ), log10 ( dge.f.qlt $ table $ PValue ) , method = 'spearman' ))) \"logFC pearson correlation coefficient : 0.999997655536736\" \"P-values spearman correlation coefficient 0.993238670517236\" The logFC are highly correlated. FDRs show less correlation but their rank are higly correlated : they come in a very similar order. Downstream analysis : over-representation analysis Having lists of differentially-expressed genes is quite interesting in itself, however when there are many DE genes, it can be interesting to map these results onto curated sets of genes associated with known biological functions. Here, we propose to use clusterProfiler , which regroups several enrichment detection algorithms onto several databases. We recommend you get inspiration from their very nice vignette/e-book to perform your own analyses. The proposed correction will concern the results obtained with DESeq2 on the Ruhland2016 dataset. analysis with clusterProfiler We begin by reading the results of the DE analysis. Adapt this to your own analysis. Beware that edgeR and DESeq2 use different column names in their result tables (log2FoldChange/logFC , padj/FDR). library ( AnnotationHub ) library ( AnnotationDbi ) library ( clusterProfiler ) library ( ReactomePA ) library ( org.Mm.eg.db ) res = read.csv ( 'Ruhland2016.DESeq2.results.csv' , row.names = 1 ) #let's define significance as padj <0.01 & abs(lfc) > 1 res $ sig = abs ( res $ log2FoldChange ) > 1 & res $ padj < 0.01 table ( res $ sig ) Number of non-significant/significant genes FALSE TRUE 18569 401 Translating gene ENSEMBL names to their entrezID (this is what clusterProfiler uses), as well as Symbol (named used by most biologist). genes_universe <- bitr ( rownames ( res ), fromType = \"ENSEMBL\" , toType = c ( \"ENTREZID\" , \"SYMBOL\" ), OrgDb = \"org.Mm.eg.db\" ) head ( genes_universe ) #ENSEMBL ENTREZID SYMBOL #2 ENSMUSG00000033845 27395 Mrpl15 #4 ENSMUSG00000025903 18777 Lypla1 #5 ENSMUSG00000033813 21399 Tcea1 #7 ENSMUSG00000002459 58175 Rgs20 #8 ENSMUSG00000033793 108664 Atp6v1h #9 ENSMUSG00000025907 12421 Rb1cc1 dim ( genes_universe ) # 15878 3 length ( rownames ( res )) # 19378 genes_DE <- bitr ( rownames ( res )[ which ( res $ sig == T )], fromType = \"ENSEMBL\" , toType = c ( \"ENTREZID\" , \"SYMBOL\" ), OrgDb = \"org.Mm.eg.db\" ) dim ( genes_DE ) # 387 3 # GO \"biological process (BP)\" enrichment ego_bp <- enrichGO ( gene = as.character ( unique ( genes_DE $ ENTREZID )), universe = as.character ( unique ( genes_universe $ ENTREZID )), OrgDb = org.Mm.eg.db , ont = \"BP\" , pAdjustMethod = \"BH\" , pvalueCutoff = 0.01 , qvalueCutoff = 0.05 , readable = TRUE ) head ( ego_bp ) dotplot ( ego_bp , showCategory = 20 ) # sample plot, but with adjusted p-value as x-axis #dotplot(ego_bp, x = \"p.adjust\", showCategory = 20) # Reactome pathways enrichment reactome.enrich <- enrichPathway ( gene = as.character ( unique ( genes_DE $ ENTREZID )), organism = \"mouse\" , pAdjustMethod = \"BH\" , qvalueCutoff = 0.01 , readable = T , universe = genes_universe $ ENTREZID ) dotplot ( reactome.enrich , x = \"p.adjust\" ) Additional : importing counts from salmon with tximport The tximport R packages offers a fairly simple set of functions to get transcript-level expression quantification from salmon or kallisto into a differential gene expression analysis. Task : import salmon transcript-level quantification in R in order to perform a DE analysis on it using either edgeR or DESeq2. Additional: compare the results with the ones obtained from STAR-aligned reads. The tximport vignette is a very good guide for this task. If you have not computed them, you can find files with expression quantifications in : /shared/data/Solutions/Liu2015/ and /shared/data/Solutions/Ruhland2016/","title":"Differential Expression Inference"},{"location":"days/DE.html#material","text":"Download the presentation Rstudio website Note RStudio is set to be rebranded as Posit after October 2022. edgeR user\u2019s guide DESeq2 vignette","title":"Material"},{"location":"days/DE.html#connexion-to-the-rstudio-server","text":"Note This step is intended only for users who attend the course with a teacher. Otherwise you will have to rely on your own installation of Rstudio. The analysis of the read count data will be done on an RStudio instance, using the R language and some relevant Bioconductor libraries. As you start your session on the RStudio server, please make sure that you know where your data is situated with respect to your working directory (use getwd() and setwd() to respectively : know what your working is, and change it as necessary).","title":"Connexion to the Rstudio server"},{"location":"days/DE.html#differential-expression-inference","text":"Use either edgeR or DESeq2 to conduct a differential expression analysis on the Ruhland2016 and/or Liu2015 dataset. You can find the expression matrices on the server at: /shared/data/Solutions/Ruhland2016/countFiles/featureCounts_Ruhland2016.counts.txt and /shared/data/Solutions/Liu2015/countFiles/featureCounts_Liu2015.counts.txt Or you may download them : Liu2015 count matrix Ruhland2016 count matrix Note Generally, users find the syntax and workflow of DESeq2 easier for getting started. If you have the time, conduct a differential expression analysis using both DESeq2 and edgeR. Follow the vignettes/user\u2019s guide! They are the most up-to-date and generally contain everything a newcomer might need, including worked-out examples.","title":"Differential Expression Inference"},{"location":"days/DE.html#deseq2","text":"DESeq2 vignette read in the data # setup library ( DESeq2 ) library ( ggplot2 ) # reading the counts files - adapt the file path to your situation raw_counts <- read.table ( '/shared/data/Solutions/Ruhland2016/countFiles/featureCounts_Ruhland2016.counts.txt' , skip = 1 , sep = \"\\t\" , header = T ) # setting up row names as ensembl gene ids row.names ( raw_counts ) = raw_counts $ Geneid ## looking at the beginning of that table raw_counts [ 1 : 5 , 1 : 5 ] # removing these first columns to keep only the sample counts raw_counts = raw_counts [ , -1 : -6 ] # changing column names names ( raw_counts ) = gsub ( '_.*' , '' , gsub ( '.*.SRR[0-9]{7}_' , '' , names ( raw_counts ) ) ) # some checking of what we just read head ( raw_counts ); tail ( raw_counts ); dim ( raw_counts ) colSums ( raw_counts ) # total number of counted reads per sample preprocessing ## telling DESeq2 what the experimental design was # note: by default, the 1st level is considered to be the reference/control/WT/... treatment <- factor ( c ( rep ( \"EtOH\" , 3 ), rep ( \"TAM\" , 3 )), levels = c ( \"EtOH\" , \"TAM\" ) ) colData <- data.frame ( treatment , row.names = colnames ( raw_counts )) colData ## creating the DESeq data object & positing the model dds <- DESeqDataSetFromMatrix ( countData = raw_counts , colData = colData , design = ~ treatment ) dim ( dds ) ## filter low count genes. Here, only keep genes with at least 2 samples where there are at least 5 reads. idx <- rowSums ( counts ( dds , normalized = FALSE ) >= 5 ) >= 2 dds.f <- dds [ idx , ] dim ( dds.f ) # we go from 55414 to 19378 genes Around 19k genes pass our minimum expression threshold, quite typical for a bulk Mouse RNA-seq experiment. estimate dispersion / model fitting # we perform the estimation of dispersions dds.f <- DESeq ( dds.f ) # we plot the estimate of the dispersions # * black dot : raw # * red dot : local trend # * blue : corrected plotDispEsts ( dds.f ) # extracting results for the treatment versus control contrast res <- results ( dds.f ) This plot is not easy to interpret. It represents the amount of dispersion at different levels of expression. It is directly linked to our ability to detect differential expression. Here it looks about normal compared to typical bulk RNA-seq experiments : the dispersion is comparatively larger for lowly expressed genes. looking at the results # adds estimate of the LFC the results table. # This shrunk logFC estimate is more robust than the raw value head ( coef ( dds.f )) # the second column corresponds to the difference between the 2 conditions res.lfc <- lfcShrink ( dds.f , coef = 2 , res = res ) #plotting to see the difference. par ( mfrow = c ( 2 , 1 )) DESeq2 :: plotMA ( res ) DESeq2 :: plotMA ( res.lfc ) # -> with shrinkage, the significativeness and logFC are more consistent par ( mfrow = c ( 1 , 1 )) Without the shrinkage, we can see that for low counts we can see a high log-fold change but non significant (ie. we see a large difference but with variance is also so high that this observation may be due to chance only). The shrinkage corrects this and the relationship between logFC and significance is smoother. # we apply the variance stabilising transformation to make the read counts comparable across libraries # (nb : this is not needed for DESeq DE analysis, but rather for visualisations that compare expression across samples, such as PCA. This replaces normal PCA scaling) vst.dds.f <- vst ( dds.f , blind = FALSE ) vst.dds.f.counts <- assay ( vst.dds.f ) plotPCA ( vst.dds.f , intgroup = c ( \"treatment\" )) The first axis (58% of the variance) seems linked to the grouping of interest. ## ggplot2-based volcano plot library ( ggplot2 ) FDRthreshold = 0.01 logFCthreshold = 1.0 # add a column of NAs res.lfc $ diffexpressed <- \"NO\" # if log2Foldchange > 1 and pvalue < 0.01, set as \"UP\" res.lfc $ diffexpressed [ res.lfc $ log2FoldChange > logFCthreshold & res.lfc $ padj < FDRthreshold ] <- \"UP\" # if log2Foldchange < 1 and pvalue < 0.01, set as \"DOWN\" res.lfc $ diffexpressed [ res.lfc $ log2FoldChange < - logFCthreshold & res.lfc $ padj < FDRthreshold ] <- \"DOWN\" ggplot ( data = data.frame ( res.lfc ) , aes ( x = log2FoldChange , y = - log10 ( padj ) , col = diffexpressed ) ) + geom_point () + geom_vline ( xintercept = c ( - logFCthreshold , logFCthreshold ), col = \"red\" ) + geom_hline ( yintercept =- log10 ( FDRthreshold ), col = \"red\" ) + scale_color_manual ( values = c ( \"blue\" , \"grey\" , \"red\" )) table ( res.lfc $ diffexpressed ) DOWN NO UP 131 19002 245 library ( pheatmap ) topVarGenes <- head ( order ( rowVars ( vst.dds.f.counts ), decreasing = TRUE ), 20 ) mat <- vst.dds.f.counts [ topVarGenes , ] #scaled counts of the top genes mat <- mat - rowMeans ( mat ) # centering pheatmap ( mat ) # saving results to file # note: a CSV file can be imported into Excel write.csv ( res , 'Ruhland2016.DESeq2.results.csv' )","title":"DESeq2"},{"location":"days/DE.html#edger","text":"edgeR user\u2019s guide read in the data library ( edgeR ) library ( ggplot2 ) # reading the counts files - adapt the file path to your situation raw_counts <- read.table ( '.../Ruhland2016_featureCount_result.counts' , skip = 1 , sep = \"\\t\" , header = T ) # setting up row names as ensembl gene ids row.names ( raw_counts ) = raw_counts $ Geneid # removing these first columns to keep only the sample counts raw_counts = raw_counts [ , -1 : -6 ] # changing column names names ( raw_counts ) = gsub ( '_.*' , '' , gsub ( '.*.SRR[0-9]{7}_' , '' , names ( raw_counts ) ) ) # some checking of what we just read head ( raw_counts ); tail ( raw_counts ); dim ( raw_counts ) colSums ( raw_counts ) # total number of counted reads per sample edgeR object preprocessing # setting up the experimental design AND the model # -> the first 3 samples form a group, the 3 remaining are the other group treatment <- c ( rep ( \"EtOH\" , 3 ), rep ( \"TAM\" , 3 )) dge.f.design <- model.matrix ( ~ treatment ) # creating the edgeR DGE object dge.all <- DGEList ( counts = raw_counts , group = treatment ) # filtering by expression level. See ?filterByExpr for details keep <- filterByExpr ( dge.all ) dge.f <- dge.all [ keep , keep.lib.sizes = FALSE ] table ( keep ) keep FALSE TRUE 39702 15712 Around 16k genes are sufficiently expressed to be retained. #normalization dge.f <- calcNormFactors ( dge.f ) dge.f $ samples Each sample has been associated with a normalization factor. edgeR model fitting # estimate of the dispersion dge.f <- estimateDisp ( dge.f , dge.f.design , robust = T ) plotBCV ( dge.f ) This plot is not easy to interpret. It represents the amount of biological variation at different levels of expression. It is directly linked to our ability to detect differential expression. Here it looks about normal compared to other bulk RNA-seq experiments : the variation is comparatively larger for lowly expressed genes. # testing for differential expression. # This method is recommended when you only have 2 groups to compare dge.f.et <- exactTest ( dge.f ) topTags ( dge.f.et ) # printing the genes where the p-value of differential expression if the lowest Comparison of groups: TAM-EtOH logFC logCPM PValue FDR ENSMUSG00000050272 -8.522762 4.988067 2.554513e-28 3.851950e-24 ENSMUSG00000075014 3.890079 5.175181 2.036909e-25 1.535728e-21 ENSMUSG00000009185 3.837786 6.742422 1.553964e-22 7.810743e-19 ENSMUSG00000075015 3.778523 3.274463 2.106799e-22 7.942107e-19 ENSMUSG00000028339 -5.692069 6.372980 4.593720e-16 1.385374e-12 ENSMUSG00000040111 -2.141221 6.771538 4.954522e-15 1.245154e-11 ENSMUSG00000041695 4.123972 1.668247 6.057909e-15 1.304960e-11 ENSMUSG00000072941 3.609170 7.080257 1.807618e-14 3.407135e-11 ENSMUSG00000000120 -6.340146 6.351489 2.507019e-14 4.200371e-11 ENSMUSG00000034981 3.727969 5.244841 3.934957e-14 5.933521e-11 # see how many genes are DE summary ( decideTests ( dge.f.et , p.value = 0.01 )) # let's use 0.01 as a threshold TAM-EtOH Down 109 NotSig 15393 Up 210 The comparison is TAM-EtOH, so \u201cUp\u201d, corresponds to a higher in group TAM compared to group EtOH. edgeR looking at differentially-expressed genes ## plot all the logFCs versus average count size. Significantly DE genes are colored par ( mfrow = c ( 1 , 1 )) plotMD ( dge.f.et ) # lines at a log2FC of 1/-1, corresponding to a shift in expression of x2 abline ( h = c ( -1 , 1 ), col = \"blue\" ) ## Volcano plot allGenes = topTags ( dge.f.et , n = nrow ( dge.f.et $ table ) ) $ table FDRthreshold = 0.01 logFCthreshold = 1.0 # add a column of NAs allGenes $ diffexpressed <- \"NO\" # if log2Foldchange > 1 and pvalue < 0.01, set as \"UP\" allGenes $ diffexpressed [ allGenes $ logFC > logFCthreshold & allGenes $ FDR < FDRthreshold ] <- \"UP\" # if log2Foldchange < 1 and pvalue < 0.01, set as \"DOWN\" allGenes $ diffexpressed [ allGenes $ logFC < - logFCthreshold & allGenes $ FDR < FDRthreshold ] <- \"DOWN\" ggplot ( data = allGenes , aes ( x = logFC , y = - log10 ( FDR ) , col = diffexpressed ) ) + geom_point () + geom_vline ( xintercept = c ( - logFCthreshold , logFCthreshold ), col = \"red\" ) + geom_hline ( yintercept =- log10 ( FDRthreshold ), col = \"red\" ) + scale_color_manual ( values = c ( \"blue\" , \"grey\" , \"red\" )) ## writing the table of results write.csv ( allGenes , 'Ruhland2016.edgeR.results.csv' ) edgeR extra stuff # how to extract log CPM logcpm <- cpm ( dge.f , prior.count = 2 , log = TRUE ) # there is another fitting method reliying on quasi-likelihood, which is useful when the model is more complex (ie. more than 1 factor with 2 levels) dge.f.QLfit <- glmQLFit ( dge.f , dge.f.design ) dge.f.qlt <- glmQLFTest ( dge.f.QLfit , coef = 2 ) # you can see the results are relatively different. The order of genes changes a bit, and the p-values are more profoundly affected topTags ( dge.f.et ) topTags ( dge.f.qlt ) ## let's see how much the two methods agree: par ( mfrow = c ( 1 , 2 )) plot ( dge.f.et $ table $ logFC , dge.f.qlt $ table $ logFC , xlab = 'exact test logFC' , ylab = 'quasi-likelihood test logFC' ) print ( paste ( 'logFC pearson correlation coefficient :' , cor ( dge.f.et $ table $ logFC , dge.f.qlt $ table $ logFC ) ) ) plot ( log10 ( dge.f.et $ table $ PValue ), log10 ( dge.f.qlt $ table $ PValue ) , xlab = 'exact test p-values (log10)' , ylab = 'quasi-likelihood test p-values (log10)' ) print ( paste ( \"P-values spearman correlation coefficient\" , cor ( log10 ( dge.f.et $ table $ PValue ), log10 ( dge.f.qlt $ table $ PValue ) , method = 'spearman' ))) \"logFC pearson correlation coefficient : 0.999997655536736\" \"P-values spearman correlation coefficient 0.993238670517236\" The logFC are highly correlated. FDRs show less correlation but their rank are higly correlated : they come in a very similar order.","title":"EdgeR"},{"location":"days/DE.html#downstream-analysis-over-representation-analysis","text":"Having lists of differentially-expressed genes is quite interesting in itself, however when there are many DE genes, it can be interesting to map these results onto curated sets of genes associated with known biological functions. Here, we propose to use clusterProfiler , which regroups several enrichment detection algorithms onto several databases. We recommend you get inspiration from their very nice vignette/e-book to perform your own analyses. The proposed correction will concern the results obtained with DESeq2 on the Ruhland2016 dataset. analysis with clusterProfiler We begin by reading the results of the DE analysis. Adapt this to your own analysis. Beware that edgeR and DESeq2 use different column names in their result tables (log2FoldChange/logFC , padj/FDR). library ( AnnotationHub ) library ( AnnotationDbi ) library ( clusterProfiler ) library ( ReactomePA ) library ( org.Mm.eg.db ) res = read.csv ( 'Ruhland2016.DESeq2.results.csv' , row.names = 1 ) #let's define significance as padj <0.01 & abs(lfc) > 1 res $ sig = abs ( res $ log2FoldChange ) > 1 & res $ padj < 0.01 table ( res $ sig ) Number of non-significant/significant genes FALSE TRUE 18569 401 Translating gene ENSEMBL names to their entrezID (this is what clusterProfiler uses), as well as Symbol (named used by most biologist). genes_universe <- bitr ( rownames ( res ), fromType = \"ENSEMBL\" , toType = c ( \"ENTREZID\" , \"SYMBOL\" ), OrgDb = \"org.Mm.eg.db\" ) head ( genes_universe ) #ENSEMBL ENTREZID SYMBOL #2 ENSMUSG00000033845 27395 Mrpl15 #4 ENSMUSG00000025903 18777 Lypla1 #5 ENSMUSG00000033813 21399 Tcea1 #7 ENSMUSG00000002459 58175 Rgs20 #8 ENSMUSG00000033793 108664 Atp6v1h #9 ENSMUSG00000025907 12421 Rb1cc1 dim ( genes_universe ) # 15878 3 length ( rownames ( res )) # 19378 genes_DE <- bitr ( rownames ( res )[ which ( res $ sig == T )], fromType = \"ENSEMBL\" , toType = c ( \"ENTREZID\" , \"SYMBOL\" ), OrgDb = \"org.Mm.eg.db\" ) dim ( genes_DE ) # 387 3 # GO \"biological process (BP)\" enrichment ego_bp <- enrichGO ( gene = as.character ( unique ( genes_DE $ ENTREZID )), universe = as.character ( unique ( genes_universe $ ENTREZID )), OrgDb = org.Mm.eg.db , ont = \"BP\" , pAdjustMethod = \"BH\" , pvalueCutoff = 0.01 , qvalueCutoff = 0.05 , readable = TRUE ) head ( ego_bp ) dotplot ( ego_bp , showCategory = 20 ) # sample plot, but with adjusted p-value as x-axis #dotplot(ego_bp, x = \"p.adjust\", showCategory = 20) # Reactome pathways enrichment reactome.enrich <- enrichPathway ( gene = as.character ( unique ( genes_DE $ ENTREZID )), organism = \"mouse\" , pAdjustMethod = \"BH\" , qvalueCutoff = 0.01 , readable = T , universe = genes_universe $ ENTREZID ) dotplot ( reactome.enrich , x = \"p.adjust\" )","title":"Downstream analysis : over-representation analysis"},{"location":"days/DE.html#additional-importing-counts-from-salmon-with-tximport","text":"The tximport R packages offers a fairly simple set of functions to get transcript-level expression quantification from salmon or kallisto into a differential gene expression analysis. Task : import salmon transcript-level quantification in R in order to perform a DE analysis on it using either edgeR or DESeq2. Additional: compare the results with the ones obtained from STAR-aligned reads. The tximport vignette is a very good guide for this task. If you have not computed them, you can find files with expression quantifications in : /shared/data/Solutions/Liu2015/ and /shared/data/Solutions/Ruhland2016/","title":"Additional : importing counts from salmon with tximport"},{"location":"days/counting.html","text":"Read counting refers to the quantification of an \u201cexpression level\u201d , or abundance, from reads mapped onto a reference genome/transcriptome. This expression level can take several forms, such as a count, or a fraction (RPKM/TPM), and concern different entities (exon, transcript, genes) depending on your biological application. During this lesson, you will learn to: differentiate between different levels of counting and their relevance for different questions. perform read counting at the gene level for Differential Gene expression. Material Download the presentation featureCounts website Read counting with featureCounts The featureCount website provides several useful command-line examples to get started. For more details on the algorithm behavior (with multi/overlapping reads for instance), you can refer to the package\u2019s User\u2019s guide (go to the read summarization chapter). Task : Decide which parameters are appropriate for counting reads from the Ruhland dataset. Assume you are interested in determining which genes are differentially expressed. Count the reads from one of your BAM files using featureCount. How do the featureCount-derived counts compare to those from STAR ? You can find bam files at /shared/data/Solutions/Liu2015/STAR_Liu2015 and /shared/data/Solutions/Ruhland2016/STAR_Ruhland2016 featureCount requirements : 400M RAM / BAM file featureCount requirements : 2 min CPU time / BAM file featureCounts script #!/usr/bin/bash #SBATCH --job-name=featurecount #SBATCH --time=00:30:00 #SBATCH --cpus-per-task=8 #SBATCH --mem=4G #SBATCH -o count.o #SBATCH -e count.e G_GTF=/shared/data/DATA/Mus_musculus.GRCm39.105.gtf inFOLDER=/shared/data/Solutions/Ruhland2016/STAR_Ruhland2016 outFOLDER=featureCOUNT_Ruhland2016 ml subread mkdir -p $outFOLDER featureCounts -T 8 -a $G_GTF -t exon -g gene_id -o featureCounts_Ruhland2016.counts.txt \\ $inFOLDER/SRR3180535_EtOH1_1.fastq.gzAligned.sortedByCoord.out.bam \\ $inFOLDER/SRR3180536_EtOH2_1.fastq.gzAligned.sortedByCoord.out.bam \\ $inFOLDER/SRR3180537_EtOH3_1.fastq.gzAligned.sortedByCoord.out.bam \\ $inFOLDER/SRR3180538_TAM1_1.fastq.gzAligned.sortedByCoord.out.bam \\ $inFOLDER/SRR3180539_TAM2_1.fastq.gzAligned.sortedByCoord.out.bam \\ $inFOLDER/SRR3180540_TAM3_1.fastq.gzAligned.sortedByCoord.out.bam comparison with STAR counts You can use this little R script to check they are the same : fc = read.table( \"featureCounts_SRR3180535_EtOH1_1.counts.txt\" , header =T) rownames( fc ) = fc$Geneid head( fc ) star = read.table( \"SRR3180535_EtOH1_1.fastq.gzReadsPerGene.out.tab\") rownames( star ) = star$V1 head( star ) star_count = star[ rownames( fc ) , 'V2' ] fC_count = fc$STAR_Ruhland2016.SRR3180535_EtOH1_1.fastq.gzAligned.sortedByCoord.out.bam plot(log10( star_count + 1), log10(fC_count+1) ) quantile( star_count - fC_count)","title":"Read counting"},{"location":"days/counting.html#material","text":"Download the presentation featureCounts website","title":"Material"},{"location":"days/counting.html#read-counting-with-featurecounts","text":"The featureCount website provides several useful command-line examples to get started. For more details on the algorithm behavior (with multi/overlapping reads for instance), you can refer to the package\u2019s User\u2019s guide (go to the read summarization chapter). Task : Decide which parameters are appropriate for counting reads from the Ruhland dataset. Assume you are interested in determining which genes are differentially expressed. Count the reads from one of your BAM files using featureCount. How do the featureCount-derived counts compare to those from STAR ? You can find bam files at /shared/data/Solutions/Liu2015/STAR_Liu2015 and /shared/data/Solutions/Ruhland2016/STAR_Ruhland2016 featureCount requirements : 400M RAM / BAM file featureCount requirements : 2 min CPU time / BAM file featureCounts script #!/usr/bin/bash #SBATCH --job-name=featurecount #SBATCH --time=00:30:00 #SBATCH --cpus-per-task=8 #SBATCH --mem=4G #SBATCH -o count.o #SBATCH -e count.e G_GTF=/shared/data/DATA/Mus_musculus.GRCm39.105.gtf inFOLDER=/shared/data/Solutions/Ruhland2016/STAR_Ruhland2016 outFOLDER=featureCOUNT_Ruhland2016 ml subread mkdir -p $outFOLDER featureCounts -T 8 -a $G_GTF -t exon -g gene_id -o featureCounts_Ruhland2016.counts.txt \\ $inFOLDER/SRR3180535_EtOH1_1.fastq.gzAligned.sortedByCoord.out.bam \\ $inFOLDER/SRR3180536_EtOH2_1.fastq.gzAligned.sortedByCoord.out.bam \\ $inFOLDER/SRR3180537_EtOH3_1.fastq.gzAligned.sortedByCoord.out.bam \\ $inFOLDER/SRR3180538_TAM1_1.fastq.gzAligned.sortedByCoord.out.bam \\ $inFOLDER/SRR3180539_TAM2_1.fastq.gzAligned.sortedByCoord.out.bam \\ $inFOLDER/SRR3180540_TAM3_1.fastq.gzAligned.sortedByCoord.out.bam comparison with STAR counts You can use this little R script to check they are the same : fc = read.table( \"featureCounts_SRR3180535_EtOH1_1.counts.txt\" , header =T) rownames( fc ) = fc$Geneid head( fc ) star = read.table( \"SRR3180535_EtOH1_1.fastq.gzReadsPerGene.out.tab\") rownames( star ) = star$V1 head( star ) star_count = star[ rownames( fc ) , 'V2' ] fC_count = fc$STAR_Ruhland2016.SRR3180535_EtOH1_1.fastq.gzAligned.sortedByCoord.out.bam plot(log10( star_count + 1), log10(fC_count+1) ) quantile( star_count - fC_count)","title":"Read counting with featureCounts"},{"location":"days/design.html","text":"Designing your experiment is the first step. Design is crucial as it conditions the sort of questions that you can ask from your data, as well as the confidence you may have in the answers. Knowing about the sequencing technologies, their strengths and limitations, as well as the RNA-seq analysis pipeline, are the keys to design a successful RNA-seq experiment. After having completed this chapter you will be able to: describe different sequencing technologies and their application in RNA-seq. differentiate between technical and biological replicates. choose an appropriate sequencing depth and number of replicates depending on your scientific question. Material Download the presentation","title":"RNAseq - technologies and design"},{"location":"days/design.html#material","text":"Download the presentation","title":"Material"},{"location":"days/mapping.html","text":"Once you are happy with your read sequences in your FASTQ files, you can use a mapper software to align the reads to the genome and thereby find where they originated from. At the end of this lesson, you will be able to : identify the differences between a local aligner and a pseudo aligner. perform genome indexing appropriate to your data. map your RNA-seq data onto the genome. Material Download the presentation STAR website Building a reference genome index Before any mapping can be achieved, you must first index the genome want to map to. To do this with STAR, you need two files: a fasta file containing the sequences of the chromosome (or genome contigs) a gtf file containing annotations (ie. where the genes and exons are) We will be using the Ensembl references, with their accompanying GTF annotations. Note While the data are already on the server here, in practice or if you are following this course without a teacher, you can grab the reference genome data from the Ensembl ftp website . In particular, you will want a mouse DNA fasta file and gtf file [release-104 at the time we are linking this. Checking for more recent release is recommended, but may slightly alter the results]. Task : Using STAR, build a genome index for chromosome 19 of Mus musculus using the associated GTF Important notes : the module name for this aligner is star . .fasta and .gtf files are in : /shared/data/DATA/Mouse_chr19/ . refer to the manual to determine which options to use. the --genomeDir parameter is the folder where the indexed genome will be output to. this job should require less than 4Gb and 30min to run. Note While your indexing job is running, you can read ahead in STAR\u2019s manual to prepare the next step : mapping your reads onto the indexed reference genome. STAR indexing script #!/usr/bin/bash #SBATCH --job-name=star-build #SBATCH --time=00:30:00 #SBATCH --cpus-per-task=4 #SBATCH --mem=4G #SBATCH -o star-build.o #SBATCH -e star-build.e G_FASTA = /shared/data/DATA/Mouse_chr19/Mus_musculus.GRCm38.dna.chromosome.19.fa G_GTF = /shared/data/DATA/Mouse_chr19/Mus_musculus.GRCm38.101.chr19.gtf ml star mkdir -p STAR_references STAR --runMode genomeGenerate \\ --genomeDir STAR_references \\ --genomeFastaFiles $G_FASTA \\ --sjdbGTFfile $G_GTF \\ --runThreadN 4 \\ --genomeSAindexNbases 11 \\ --sjdbOverhang 49 Extra task : Determine how you would add an additional feature to your reference, for example for a novel transcript not described by the standard reference. Answer Edit the gtf file to add your additional feature(s), following the proper format . Mapping reads onto the reference Task : Using STAR, align ONE of the FASTQ files from the Ruhland2016 study against the mouse genome. Use the full indexed genome at /shared/data/DATA/Mouse_STAR_index/ , rather than the one we just made. IMPORTANT : use the following option in your STAR command: --outTmpDir /tmp/${SLURM_JOB_USER}_${SLURM_JOB_ID}/ . You can use the manual to look up what this option does. The slurm variables ensure a distinct directory is created in /tmp/ for each user and for each job. Generate a BAM file sorted by coordinate. Generate a geneCounts file. Mapping reads and generating a sorted BAM from one of the Ruhland2016 et al. FASTQ files should take about 20 minutes. Note Take the time to read the parts of the STAR manual which concern you : a bit of planning ahead can save you a lot of time-consuming/headache-inducing trial and error on your script. Warning Remember : request a maximum of 30G and 8 CPUs for 1 hour. STAR mapping script #!/usr/bin/bash #SBATCH --job-name=star-aln #SBATCH --time=01:00:00 #SBATCH --cpus-per-task=8 #SBATCH --mem=30G #SBATCH -o star-aln.o #SBATCH -e star-aln.e ml star outDIR=STAR_Ruhland2016 mkdir -p $outDIR dataDIR=/shared/data/DATA/Ruhland2016 genomeDIR=/shared/data/DATA/Mouse_STAR_index STAR --runThreadN 8 --genomeDir $genomeDIR \\ --outSAMtype BAM SortedByCoordinate --outReadsUnmapped Fastx \\ --outFileNamePrefix $outDIR/SRR3180535_EtOH1_1 \\ --quantMode GeneCounts \\ --readFilesIn $dataDIR/SRR3180535_EtOH1_1.fastq.gz --readFilesCommand zcat \\ The options of STAR are : \u2013runThreadN 8 : 8 threads to go faster. \u2013genomeDir $genomeDIR : path of the genome to map to. \u2013outSAMtype BAM SortedByCoordinate : output a coordinate-sorted BAM file. \u2013outReadsUnmapped Fastx : output the non-mapping reads (in case we want to analyse them). \u2013outFileNamePrefix $outDIR/$fastqFILE : prefix of output files. \u2013quantMode GeneCounts : will create a file with counts of reads per gene. \u2013readFilesIn $dataDIR/$fastqFILE : input read file. \u2013readFilesCommand zcat : command to unzip the input file. advanced : STAR mapping script with array job The following sets up an array of tasks to align all samples. Source file : Ruhland2016.fastqFiles.txt : SRR3180535_EtOH1_1.fastq.gz SRR3180536_EtOH2_1.fastq.gz SRR3180537_EtOH3_1.fastq.gz SRR3180538_TAM1_1.fastq.gz SRR3180539_TAM2_1.fastq.gz SRR3180540_TAM3_1.fastq.gz sbatch script : #!/usr/bin/bash #SBATCH --job-name=star-aln #SBATCH --time=01:00:00 #SBATCH --cpus-per-task=8 #SBATCH --mem=30G #SBATCH -o star-aln.%a.o #SBATCH -e star-aln.%a.e #SBATCH --array 1-1%1 ml star outDIR=STAR_Ruhland2016 mkdir -p $outDIR dataDIR=/shared/data/DATA/Ruhland2016 sourceFILE=Ruhland2016.fastqFiles.txt fastqFILE=`sed -n ${SLURM_ARRAY_TASK_ID}p $sourceFILE` genomeDIR=/shared/data/DATA/Mouse_STAR_index STAR --runThreadN 8 --genomeDir $genomeDIR \\ --outSAMtype BAM SortedByCoordinate --outReadsUnmapped Fastx \\ --outFileNamePrefix $outDIR/$fastqFILE \\ --quantMode GeneCounts \\ --readFilesIn $dataDIR/$fastqFILE --readFilesCommand zcat \\ The options of STAR are : \u2013runThreadN 8 : 8 threads to go faster. \u2013genomeDir $genomeDIR : path of the genome to map to. \u2013outSAMtype BAM SortedByCoordinate : output a coordinate-sorted BAM file. \u2013outReadsUnmapped Fastx : output the non-mapping reads (in case we want to analyse them). \u2013outFileNamePrefix $outDIR/$fastqFILE : prefix of output files. \u2013quantMode GeneCounts : will create a file with counts of reads per gene. \u2013readFilesIn $dataDIR/$fastqFILE : input read file. \u2013readFilesCommand zcat : command to unzip the input file. QC on the aligned reads You can call MultiQC on the STAR output folder to gather a report on the individual alignments. Here we\u2019ve aligned a single sample, but usually this would cover all your samples. Task : use multiqc to generate a QC report on the results of your mapping. Evaluate the alignment statistics. Do you consider this to be a good alignment? How many unmapped reads are there? Where might this come from, and how would you determine this? What could you say about library strandedness ? script and answers #!/usr/bin/bash #SBATCH --job-name=multiqc #SBATCH --time=00:30:00 #SBATCH --cpus-per-task=1 #SBATCH --mem=1G #SBATCH -o multiqc_star_Ruhland2016.o #SBATCH -e multiqc_star_Ruhland2016.e mkdir -p STAR_MULTIQC_Ruhland2016/ multiqc -o STAR_MULTIQC_Ruhland2016/ STAR_Ruhland2016/ Result : Download the report ADDITIONNAL : STAR 2-Pass Genome annotations are incomplete, particularly for complex eukaryotes : there are many as-of-yet unannotated splice junctions. The first pass of STAR can create a splice junction database, containing both known and novel junctions. This splice junction database can, in turn, be used to guide an improved second round of alignment, using a command like: STAR <1st round options> --sjdbFileChrStartEnd sample_SJ.out.tab Task : run STAR in this STAR-2pass mode on the same sample as before and evaluate the results. script #!/usr/bin/bash #SBATCH --job-name=star-aln2 #SBATCH --time=01:00:00 #SBATCH --cpus-per-task=8 #SBATCH --mem=30G #SBATCH -o star-aln-2pass.%a.o #SBATCH -e star-aln-2pass.%a.e #SBATCH --array 1-1%1 ml star outDIR=STAR_Ruhland2016 mkdir -p $outDIR dataDIR=/shared/data/DATA/Ruhland2016 sourceFILE=Ruhland2016.fastqFiles.txt fastqFILE=`sed -n ${SLURM_ARRAY_TASK_ID}p $sourceFILE` genomeDIR=/shared/data/DATA/Mouse_STAR_index STAR --runThreadN 8 --genomeDir $genomeDIR \\ --outSAMtype BAM SortedByCoordinate \\ --outFileNamePrefix $outDIR/$fastqFILE.2Pass. \\ --outReadsUnmapped Fastx --quantMode GeneCounts \\ --sjdbFileChrStartEnd $outDIR/${fastqFILE}SJ.out.tab \\ --readFilesIn $dataDIR/$fastqFILE --readFilesCommand zcat \\ ADDITIONAL : pseudo-aligning with salmon salmon website salmon can allow you to quantify transcript expression without explicitly aligning the sequenced reads onto the reference genome with its gene and splice junction annotations, but to a simplification of the corresponding transcriptome, thus saving computational resources. We refer you to the tool\u2019s documentation in order to see how the reference index is computed . Task : run salmon to quantify the expression of either the Ruhland or Liu dataset. Use the tool documentation to craft your command line. precomputed indices can be found in /shared/data/Mouse_salmon_index and /shared/data/Human_salmon_index . script #!/usr/bin/bash #SBATCH --job-name=salmonRuhland #SBATCH --time=01:00:00 #SBATCH --cpus-per-task=8 #SBATCH --mem=30G #SBATCH -o salmon_ruhland2016.%a.o #SBATCH -e salmon_ruhland2016.%a.e #SBATCH --array 1-6%1 ml salmon outDIR=salmon_Ruhland2016 mkdir -p $outDIR dataDIR=/shared/data/DATA/Ruhland2016 sourceFILE=Ruhland2016.fastqFiles.txt fastqFILE=`sed -n ${SLURM_ARRAY_TASK_ID}p $sourceFILE` genomeDIR=/shared/data/DATA/Mouse_salmon_index salmon quant -i $genomeDIR -l A \\ -r $dataDIR/$fastqFILE \\ -p 8 --validateMappings --gcBias --seqBias \\ -o $outDIR","title":"Reads mapping"},{"location":"days/mapping.html#material","text":"Download the presentation STAR website","title":"Material"},{"location":"days/mapping.html#building-a-reference-genome-index","text":"Before any mapping can be achieved, you must first index the genome want to map to. To do this with STAR, you need two files: a fasta file containing the sequences of the chromosome (or genome contigs) a gtf file containing annotations (ie. where the genes and exons are) We will be using the Ensembl references, with their accompanying GTF annotations. Note While the data are already on the server here, in practice or if you are following this course without a teacher, you can grab the reference genome data from the Ensembl ftp website . In particular, you will want a mouse DNA fasta file and gtf file [release-104 at the time we are linking this. Checking for more recent release is recommended, but may slightly alter the results]. Task : Using STAR, build a genome index for chromosome 19 of Mus musculus using the associated GTF Important notes : the module name for this aligner is star . .fasta and .gtf files are in : /shared/data/DATA/Mouse_chr19/ . refer to the manual to determine which options to use. the --genomeDir parameter is the folder where the indexed genome will be output to. this job should require less than 4Gb and 30min to run. Note While your indexing job is running, you can read ahead in STAR\u2019s manual to prepare the next step : mapping your reads onto the indexed reference genome. STAR indexing script #!/usr/bin/bash #SBATCH --job-name=star-build #SBATCH --time=00:30:00 #SBATCH --cpus-per-task=4 #SBATCH --mem=4G #SBATCH -o star-build.o #SBATCH -e star-build.e G_FASTA = /shared/data/DATA/Mouse_chr19/Mus_musculus.GRCm38.dna.chromosome.19.fa G_GTF = /shared/data/DATA/Mouse_chr19/Mus_musculus.GRCm38.101.chr19.gtf ml star mkdir -p STAR_references STAR --runMode genomeGenerate \\ --genomeDir STAR_references \\ --genomeFastaFiles $G_FASTA \\ --sjdbGTFfile $G_GTF \\ --runThreadN 4 \\ --genomeSAindexNbases 11 \\ --sjdbOverhang 49 Extra task : Determine how you would add an additional feature to your reference, for example for a novel transcript not described by the standard reference. Answer Edit the gtf file to add your additional feature(s), following the proper format .","title":"Building a reference genome index"},{"location":"days/mapping.html#mapping-reads-onto-the-reference","text":"Task : Using STAR, align ONE of the FASTQ files from the Ruhland2016 study against the mouse genome. Use the full indexed genome at /shared/data/DATA/Mouse_STAR_index/ , rather than the one we just made. IMPORTANT : use the following option in your STAR command: --outTmpDir /tmp/${SLURM_JOB_USER}_${SLURM_JOB_ID}/ . You can use the manual to look up what this option does. The slurm variables ensure a distinct directory is created in /tmp/ for each user and for each job. Generate a BAM file sorted by coordinate. Generate a geneCounts file. Mapping reads and generating a sorted BAM from one of the Ruhland2016 et al. FASTQ files should take about 20 minutes. Note Take the time to read the parts of the STAR manual which concern you : a bit of planning ahead can save you a lot of time-consuming/headache-inducing trial and error on your script. Warning Remember : request a maximum of 30G and 8 CPUs for 1 hour. STAR mapping script #!/usr/bin/bash #SBATCH --job-name=star-aln #SBATCH --time=01:00:00 #SBATCH --cpus-per-task=8 #SBATCH --mem=30G #SBATCH -o star-aln.o #SBATCH -e star-aln.e ml star outDIR=STAR_Ruhland2016 mkdir -p $outDIR dataDIR=/shared/data/DATA/Ruhland2016 genomeDIR=/shared/data/DATA/Mouse_STAR_index STAR --runThreadN 8 --genomeDir $genomeDIR \\ --outSAMtype BAM SortedByCoordinate --outReadsUnmapped Fastx \\ --outFileNamePrefix $outDIR/SRR3180535_EtOH1_1 \\ --quantMode GeneCounts \\ --readFilesIn $dataDIR/SRR3180535_EtOH1_1.fastq.gz --readFilesCommand zcat \\ The options of STAR are : \u2013runThreadN 8 : 8 threads to go faster. \u2013genomeDir $genomeDIR : path of the genome to map to. \u2013outSAMtype BAM SortedByCoordinate : output a coordinate-sorted BAM file. \u2013outReadsUnmapped Fastx : output the non-mapping reads (in case we want to analyse them). \u2013outFileNamePrefix $outDIR/$fastqFILE : prefix of output files. \u2013quantMode GeneCounts : will create a file with counts of reads per gene. \u2013readFilesIn $dataDIR/$fastqFILE : input read file. \u2013readFilesCommand zcat : command to unzip the input file. advanced : STAR mapping script with array job The following sets up an array of tasks to align all samples. Source file : Ruhland2016.fastqFiles.txt : SRR3180535_EtOH1_1.fastq.gz SRR3180536_EtOH2_1.fastq.gz SRR3180537_EtOH3_1.fastq.gz SRR3180538_TAM1_1.fastq.gz SRR3180539_TAM2_1.fastq.gz SRR3180540_TAM3_1.fastq.gz sbatch script : #!/usr/bin/bash #SBATCH --job-name=star-aln #SBATCH --time=01:00:00 #SBATCH --cpus-per-task=8 #SBATCH --mem=30G #SBATCH -o star-aln.%a.o #SBATCH -e star-aln.%a.e #SBATCH --array 1-1%1 ml star outDIR=STAR_Ruhland2016 mkdir -p $outDIR dataDIR=/shared/data/DATA/Ruhland2016 sourceFILE=Ruhland2016.fastqFiles.txt fastqFILE=`sed -n ${SLURM_ARRAY_TASK_ID}p $sourceFILE` genomeDIR=/shared/data/DATA/Mouse_STAR_index STAR --runThreadN 8 --genomeDir $genomeDIR \\ --outSAMtype BAM SortedByCoordinate --outReadsUnmapped Fastx \\ --outFileNamePrefix $outDIR/$fastqFILE \\ --quantMode GeneCounts \\ --readFilesIn $dataDIR/$fastqFILE --readFilesCommand zcat \\ The options of STAR are : \u2013runThreadN 8 : 8 threads to go faster. \u2013genomeDir $genomeDIR : path of the genome to map to. \u2013outSAMtype BAM SortedByCoordinate : output a coordinate-sorted BAM file. \u2013outReadsUnmapped Fastx : output the non-mapping reads (in case we want to analyse them). \u2013outFileNamePrefix $outDIR/$fastqFILE : prefix of output files. \u2013quantMode GeneCounts : will create a file with counts of reads per gene. \u2013readFilesIn $dataDIR/$fastqFILE : input read file. \u2013readFilesCommand zcat : command to unzip the input file.","title":"Mapping reads onto the reference"},{"location":"days/mapping.html#qc-on-the-aligned-reads","text":"You can call MultiQC on the STAR output folder to gather a report on the individual alignments. Here we\u2019ve aligned a single sample, but usually this would cover all your samples. Task : use multiqc to generate a QC report on the results of your mapping. Evaluate the alignment statistics. Do you consider this to be a good alignment? How many unmapped reads are there? Where might this come from, and how would you determine this? What could you say about library strandedness ? script and answers #!/usr/bin/bash #SBATCH --job-name=multiqc #SBATCH --time=00:30:00 #SBATCH --cpus-per-task=1 #SBATCH --mem=1G #SBATCH -o multiqc_star_Ruhland2016.o #SBATCH -e multiqc_star_Ruhland2016.e mkdir -p STAR_MULTIQC_Ruhland2016/ multiqc -o STAR_MULTIQC_Ruhland2016/ STAR_Ruhland2016/ Result : Download the report","title":"QC on the aligned reads"},{"location":"days/mapping.html#additionnal-star-2-pass","text":"Genome annotations are incomplete, particularly for complex eukaryotes : there are many as-of-yet unannotated splice junctions. The first pass of STAR can create a splice junction database, containing both known and novel junctions. This splice junction database can, in turn, be used to guide an improved second round of alignment, using a command like: STAR <1st round options> --sjdbFileChrStartEnd sample_SJ.out.tab Task : run STAR in this STAR-2pass mode on the same sample as before and evaluate the results. script #!/usr/bin/bash #SBATCH --job-name=star-aln2 #SBATCH --time=01:00:00 #SBATCH --cpus-per-task=8 #SBATCH --mem=30G #SBATCH -o star-aln-2pass.%a.o #SBATCH -e star-aln-2pass.%a.e #SBATCH --array 1-1%1 ml star outDIR=STAR_Ruhland2016 mkdir -p $outDIR dataDIR=/shared/data/DATA/Ruhland2016 sourceFILE=Ruhland2016.fastqFiles.txt fastqFILE=`sed -n ${SLURM_ARRAY_TASK_ID}p $sourceFILE` genomeDIR=/shared/data/DATA/Mouse_STAR_index STAR --runThreadN 8 --genomeDir $genomeDIR \\ --outSAMtype BAM SortedByCoordinate \\ --outFileNamePrefix $outDIR/$fastqFILE.2Pass. \\ --outReadsUnmapped Fastx --quantMode GeneCounts \\ --sjdbFileChrStartEnd $outDIR/${fastqFILE}SJ.out.tab \\ --readFilesIn $dataDIR/$fastqFILE --readFilesCommand zcat \\","title":"ADDITIONNAL : STAR 2-Pass"},{"location":"days/mapping.html#additional-pseudo-aligning-with-salmon","text":"salmon website salmon can allow you to quantify transcript expression without explicitly aligning the sequenced reads onto the reference genome with its gene and splice junction annotations, but to a simplification of the corresponding transcriptome, thus saving computational resources. We refer you to the tool\u2019s documentation in order to see how the reference index is computed . Task : run salmon to quantify the expression of either the Ruhland or Liu dataset. Use the tool documentation to craft your command line. precomputed indices can be found in /shared/data/Mouse_salmon_index and /shared/data/Human_salmon_index . script #!/usr/bin/bash #SBATCH --job-name=salmonRuhland #SBATCH --time=01:00:00 #SBATCH --cpus-per-task=8 #SBATCH --mem=30G #SBATCH -o salmon_ruhland2016.%a.o #SBATCH -e salmon_ruhland2016.%a.e #SBATCH --array 1-6%1 ml salmon outDIR=salmon_Ruhland2016 mkdir -p $outDIR dataDIR=/shared/data/DATA/Ruhland2016 sourceFILE=Ruhland2016.fastqFiles.txt fastqFILE=`sed -n ${SLURM_ARRAY_TASK_ID}p $sourceFILE` genomeDIR=/shared/data/DATA/Mouse_salmon_index salmon quant -i $genomeDIR -l A \\ -r $dataDIR/$fastqFILE \\ -p 8 --validateMappings --gcBias --seqBias \\ -o $outDIR","title":"ADDITIONAL : pseudo-aligning with salmon"},{"location":"days/quality_control.html","text":"Quality Control is the essential first step to perform once you receive your data from your sequencing facility, typically as .fastq or .fastq.gz files. During this session, you will learn to : create QC report for a single file with fastqc aggregate multiple QC reports using multiqc interpret the QC reports for an entire RNA-seq experiment Note Although we aim to present tools as stable as possible, software evolve and their precise interface can change with time. We strongly recommend you consult each command\u2019s help page or manual before launching them. To this end, we provide links to each tool\u2019s website. This can also be useful to you if you are following this course without access to a compute cluster and have to install these tools on your machine. Material Download the presentation FastQC website MultiQC website Meet the datasets We will be working with two datasets from the following studies: Liu et al. (2015) \u201cRNA-Seq identifies novel myocardial gene expression signatures of heart failure\u201d Genomics 105(2):83-89 https://doi.org/10.1016/j.ygeno.2014.12.002 GSE57345 Samples of Homo sapiens heart left ventricles : 3 with heart failure, 3 without 6 samples of paired-end reads Ruhland et al. (2016) \u201cStromal senescence establishes an immunosuppressive microenvironment that drives tumorigenesis\u201d Nature Communications 7:11762 https://dx.doi.org/10.1038/ncomms11762 GSE78128 Samples of Mus musculus skin fibroblasts : 3 non-senescent (EtOH), 3 senescent (TAM) 6 samples of single-end reads Retrieving published datasets Note If you are following this course with a teacher, then the for the data is already on the server. There is no need to download it again. Most NGS data is deposited at the Short Read Archive (SRA) hosted by the NCBI, with links from the Gene Expression Omnibus (GEO) Several steps are required to retrieve data from a published study : find GEO or SRA identifier from publication. find the \u201crun\u201d identifiers for each sample (SRR). use SRA Toolkit to dump data from the SRR repository to FASTQ files. For example, on the Liu2015 dataset : locate GEO ID look it up on GEO website: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE57345 Click Run Selector down the bottom. Copy/paste the SRR IDs from the table, or use the download accession list button to get a file listing them. From the results of your search , select all relevant runs Click on \u201cAccession List\u201d in the Select table use fastq-dump (part of the SRA Toolkit ) on the downloaded accession list. For example: fastq-dump --gzip --skip-technical --readids --split-files --clip SRR1272191 Note You\u2019ll need to know the nature of the dataset (library type, paired vs single end, etc.) before analysing it. fastq-dump takes a very long time More information about fastq-dump FastQC : a report for a single fastq file FastQC is a nice tool to get a variety of QC measures from files such as .fastq , .bam or .sam files. Although it has many options, the default parameters are often enough for our purpose : fastqc -o <output_directory> file1.fastq file2.fastq ... fileN.fastq FastQC is reasonably intelligent, and will try to recognise the file format and uncompress it if necessary (so no need to decompress manually). Task: Write one or more slurm-compatible sbatch scripts in your home directory that run FastQC analysis on each FASTQ file from the two datasets. These are accessible at : /shared/data/DATA/Liu2015/ and /shared/data/DATA/Ruhland2016 . Look at at least one of the QC report. What are your conclusions ? Would you want to perform some operations on the reads such as low-quality bases trimming, removal of adapters ? Warning Make sure your script writes the fastqc output to a folder within your own home directory. Important points: in your script, don\u2019t forget to load fastqc : ml fastqc . there is no need to copy the read files to your home directory (in fact, it is good practice not to: it would create data redundancy, and we won\u2019t have enough space left on the disk anyway\u2026). FastQC RAM requirements : 1Gb is more than enough. FastQC time requirements : ~ 5min / read file. try to make sure FastQC outputs all reports in the same directory, this will save time for the next step ;-). Note Reminder : to get the data from the distant server to your machine, you may use an SFTP client (filezilla, mobaXterm), or the command line tool from your machine : scp login@xx.xx.xx:~/path/to/file.txt . Liu2015 FastQC sbatch script Here is an sbatch script for one sample: #!/usr/bin/bash #SBATCH --job-name=fastqc #SBATCH --time=01:00:00 #SBATCH --cpus-per-task=1 #SBATCH --mem=1G #SBATCH -o fastqc_Liu2015.o #SBATCH -e fastqc_Liu2015.e dataDir = /shared/data/DATA/Liu2015 ml fastqc # creating the output folder mkdir -p FASTQC_Liu2015/ fastqc -o FASTQC_Liu2015/ $dataDir /SRR1272187_1.fastq.gz You could either have: one sbatch script per sample (recommended), OR put the fastqc commands for all the samples in the same script (not recommended). The first is recommended because you can submit both scripts at once and they will then run in parallel, whereas with the second option the samples would be handled sequentially and the overall job would take much longer to finish. Ruhland2016 FastQC sbatch script Here is an sbatch script for one sample: #!/usr/bin/bash #SBATCH --job-name=fastqc #SBATCH --time=01:00:00 #SBATCH --cpus-per-task=1 #SBATCH --mem=1G #SBATCH -o fastqc_Ruhland2016.o #SBATCH -e fastqc_Ruhland2016.e dataDir = /shared/data/DATA/Ruhland2016 ml fastqc # creating the output folder mkdir -p FASTQC_Ruhland2016/ fastqc -o FASTQC_Ruhland2016/ $dataDir /SRR3180540_TAM3_1.fastq.gz You could either have: one sbatch script per sample (recommended), OR put the fastqc commands for all the samples in the same script (not recommended). The first is recommended because you can submit both scripts at once and they will then run in parallel, whereas with the second option the samples would be handled sequentially and the overall job would take much longer to finish. Alternative sbatch script using array job Here is a solution where all files from a same dataset can be processed in parallel (recommended) by using slurm array jobs. First, have a file named Ruhland2016.fastqFiles.txt containing the sample fastq file names : SRR3180535_EtOH1_1.fastq.gz SRR3180536_EtOH2_1.fastq.gz SRR3180537_EtOH3_1.fastq.gz SRR3180538_TAM1_1.fastq.gz SRR3180539_TAM2_1.fastq.gz SRR3180540_TAM3_1.fastq.gz Then, in the same folder, you can create this sbatch script : #!/usr/bin/bash #SBATCH --job-name=fastqc #SBATCH --time=00:30:00 #SBATCH --cpus-per-task=1 #SBATCH --mem=1G #SBATCH -o fastqc_Ruhland2016.%a.o #SBATCH -e fastqc_Ruhland2016.%a.e #SBATCH --array 1-6%6 ml fastqc dataDir = /shared/data/DATA/Ruhland2016 sourceFILE = Ruhland2016.fastqFiles.txt ## retrieving 1 filename from Ruhland2016.fastqFiles.txt fastqFILE = ` sed -n ${ SLURM_ARRAY_TASK_ID } p $sourceFILE ` mkdir -p FASTQC_Ruhland2016/ fastqc -o FASTQC_Ruhland2016/ $dataDir / $fastqFILE When submitted with sbatch , this script will spawn 6 tasks in parallel, each with a different value of ${SLURM_ARRAY_TASK_ID} . This is the recommended option : this allows you to launch all your job in parallel with a single script. Interpretation of a report Download an annotated report MultiQC : grouping multiple reports In practice, you likely will have more than a couple of samples (maybe even more than 30 or 50\u2026) to handle: individually consulting and comparing the QC reports of each would be tedious. MultiQC is a tool that lets you combine multiple reports in a single, interactive document that let you explore your data easily. Here, we will be focusing on grouping FastQC reports, but MultiQC can also be applied to the output or logs of other bioinformatics tools, such as mappers, as we will see later. In its default usage, multiqc only needs to be provided a path where it will find all the individual reports, and it will scan them and write a report named multiqc_report.html . Although the default behaviour is quite appropriate, with a couple of options we get a slightly better control over the output: * --interactive : forces the plot to be interactive even when there is a lot of samples (this option can lead to larger html files). * -f <filename> : specify the name of the output file name. For instance, a possible command line could be : multiqc -f multiQCreports/Liu2015_multiqc.html --interactive Liu2015_fastqc/ There are many additional info which let you customize your report. Use multiqc --help or visit their documentation webpage to learn more. Task: Write an sbatch script to run MultiQC for each dataset. Look at the QC reports. What are your conclusions ? Important points: MultiQC RAM requirements : 1Gb should be more than enough. MultiQC time requirements : ~ 1min / read file. Exceptionally, there is no need to load multiqc as a module (it is not part of ComputeCanada and we installed it directly on the cluster, on other clusters it may not be the same). Use multiqc --help to check the different options MultiQC sbatch script This is the script for the Ruhland2016 dataset. It presumes that the fastqc reports can be found in FASTQC_Ruhland2016/ #!/usr/bin/bash #SBATCH --job-name=multiqc #SBATCH --time=00:30:00 #SBATCH --cpus-per-task=1 #SBATCH --mem=1G #SBATCH -o multiqc_Ruhland2016.o #SBATCH -e multiqc_Ruhland2016.e mkdir -p MULTIQC_Ruhland2016/ multiqc -o MULTIQC_Ruhland2016/ FASTQC_Ruhland2016/ Interpretation of a report We will interpret the report for the Liu2015 data. The PHRED quality of reads drop below 30 around base 75. All samples seem affected. One sample seems to have some quality drops at specific timepoints/positions. Mean quality scores are on average fairly high, but some reads exhibit low values. Most samples do not deviate too much from the expected curve. The two samples colored in orange and red have a mode for a very specific value. This may be indicative of contamination, retaining specific rRNA, or adapter sequence content. Ns are present at specific positions in specific samples, in particular for one sample. This is reminiscent of the PHRED quality curves at the top of the report. It seems some flowcells had a problem at specific time-point/positions. This is colored red because this would be a problem if the data was coming from genomic DNA sequencing. However here we are in the context of RNA-seq : some transcripts are present in a large number of copies in the samples, and consequently it is expected that some sequences are over-represented. We see a clear trend of adapter contamination as we get closer to the reads\u2019 end. Note the y-scale though : we never go above a 6% content per sample. Overall, we can conclude that these samples all suffer from some adapter content and a lower quality toward the reads\u2019 second half. Furthermore, a few samples have a peculiar N pattern between bases 20 and 30. It is then strongly advised to either : perform some trimming : remove adapter sequences + cut reads when average quality becomes too low use a mapper that takes base quality in account AND is able to ignore adapter sequence (and even then, you could try mapping on both trimmed and untrimmed data to see which is the best)","title":"Quality control"},{"location":"days/quality_control.html#material","text":"Download the presentation FastQC website MultiQC website","title":"Material"},{"location":"days/quality_control.html#meet-the-datasets","text":"We will be working with two datasets from the following studies: Liu et al. (2015) \u201cRNA-Seq identifies novel myocardial gene expression signatures of heart failure\u201d Genomics 105(2):83-89 https://doi.org/10.1016/j.ygeno.2014.12.002 GSE57345 Samples of Homo sapiens heart left ventricles : 3 with heart failure, 3 without 6 samples of paired-end reads Ruhland et al. (2016) \u201cStromal senescence establishes an immunosuppressive microenvironment that drives tumorigenesis\u201d Nature Communications 7:11762 https://dx.doi.org/10.1038/ncomms11762 GSE78128 Samples of Mus musculus skin fibroblasts : 3 non-senescent (EtOH), 3 senescent (TAM) 6 samples of single-end reads","title":"Meet the datasets"},{"location":"days/quality_control.html#retrieving-published-datasets","text":"Note If you are following this course with a teacher, then the for the data is already on the server. There is no need to download it again. Most NGS data is deposited at the Short Read Archive (SRA) hosted by the NCBI, with links from the Gene Expression Omnibus (GEO) Several steps are required to retrieve data from a published study : find GEO or SRA identifier from publication. find the \u201crun\u201d identifiers for each sample (SRR). use SRA Toolkit to dump data from the SRR repository to FASTQ files. For example, on the Liu2015 dataset : locate GEO ID look it up on GEO website: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE57345 Click Run Selector down the bottom. Copy/paste the SRR IDs from the table, or use the download accession list button to get a file listing them. From the results of your search , select all relevant runs Click on \u201cAccession List\u201d in the Select table use fastq-dump (part of the SRA Toolkit ) on the downloaded accession list. For example: fastq-dump --gzip --skip-technical --readids --split-files --clip SRR1272191 Note You\u2019ll need to know the nature of the dataset (library type, paired vs single end, etc.) before analysing it. fastq-dump takes a very long time More information about fastq-dump","title":"Retrieving published datasets"},{"location":"days/quality_control.html#fastqc-a-report-for-a-single-fastq-file","text":"FastQC is a nice tool to get a variety of QC measures from files such as .fastq , .bam or .sam files. Although it has many options, the default parameters are often enough for our purpose : fastqc -o <output_directory> file1.fastq file2.fastq ... fileN.fastq FastQC is reasonably intelligent, and will try to recognise the file format and uncompress it if necessary (so no need to decompress manually). Task: Write one or more slurm-compatible sbatch scripts in your home directory that run FastQC analysis on each FASTQ file from the two datasets. These are accessible at : /shared/data/DATA/Liu2015/ and /shared/data/DATA/Ruhland2016 . Look at at least one of the QC report. What are your conclusions ? Would you want to perform some operations on the reads such as low-quality bases trimming, removal of adapters ? Warning Make sure your script writes the fastqc output to a folder within your own home directory. Important points: in your script, don\u2019t forget to load fastqc : ml fastqc . there is no need to copy the read files to your home directory (in fact, it is good practice not to: it would create data redundancy, and we won\u2019t have enough space left on the disk anyway\u2026). FastQC RAM requirements : 1Gb is more than enough. FastQC time requirements : ~ 5min / read file. try to make sure FastQC outputs all reports in the same directory, this will save time for the next step ;-). Note Reminder : to get the data from the distant server to your machine, you may use an SFTP client (filezilla, mobaXterm), or the command line tool from your machine : scp login@xx.xx.xx:~/path/to/file.txt . Liu2015 FastQC sbatch script Here is an sbatch script for one sample: #!/usr/bin/bash #SBATCH --job-name=fastqc #SBATCH --time=01:00:00 #SBATCH --cpus-per-task=1 #SBATCH --mem=1G #SBATCH -o fastqc_Liu2015.o #SBATCH -e fastqc_Liu2015.e dataDir = /shared/data/DATA/Liu2015 ml fastqc # creating the output folder mkdir -p FASTQC_Liu2015/ fastqc -o FASTQC_Liu2015/ $dataDir /SRR1272187_1.fastq.gz You could either have: one sbatch script per sample (recommended), OR put the fastqc commands for all the samples in the same script (not recommended). The first is recommended because you can submit both scripts at once and they will then run in parallel, whereas with the second option the samples would be handled sequentially and the overall job would take much longer to finish. Ruhland2016 FastQC sbatch script Here is an sbatch script for one sample: #!/usr/bin/bash #SBATCH --job-name=fastqc #SBATCH --time=01:00:00 #SBATCH --cpus-per-task=1 #SBATCH --mem=1G #SBATCH -o fastqc_Ruhland2016.o #SBATCH -e fastqc_Ruhland2016.e dataDir = /shared/data/DATA/Ruhland2016 ml fastqc # creating the output folder mkdir -p FASTQC_Ruhland2016/ fastqc -o FASTQC_Ruhland2016/ $dataDir /SRR3180540_TAM3_1.fastq.gz You could either have: one sbatch script per sample (recommended), OR put the fastqc commands for all the samples in the same script (not recommended). The first is recommended because you can submit both scripts at once and they will then run in parallel, whereas with the second option the samples would be handled sequentially and the overall job would take much longer to finish. Alternative sbatch script using array job Here is a solution where all files from a same dataset can be processed in parallel (recommended) by using slurm array jobs. First, have a file named Ruhland2016.fastqFiles.txt containing the sample fastq file names : SRR3180535_EtOH1_1.fastq.gz SRR3180536_EtOH2_1.fastq.gz SRR3180537_EtOH3_1.fastq.gz SRR3180538_TAM1_1.fastq.gz SRR3180539_TAM2_1.fastq.gz SRR3180540_TAM3_1.fastq.gz Then, in the same folder, you can create this sbatch script : #!/usr/bin/bash #SBATCH --job-name=fastqc #SBATCH --time=00:30:00 #SBATCH --cpus-per-task=1 #SBATCH --mem=1G #SBATCH -o fastqc_Ruhland2016.%a.o #SBATCH -e fastqc_Ruhland2016.%a.e #SBATCH --array 1-6%6 ml fastqc dataDir = /shared/data/DATA/Ruhland2016 sourceFILE = Ruhland2016.fastqFiles.txt ## retrieving 1 filename from Ruhland2016.fastqFiles.txt fastqFILE = ` sed -n ${ SLURM_ARRAY_TASK_ID } p $sourceFILE ` mkdir -p FASTQC_Ruhland2016/ fastqc -o FASTQC_Ruhland2016/ $dataDir / $fastqFILE When submitted with sbatch , this script will spawn 6 tasks in parallel, each with a different value of ${SLURM_ARRAY_TASK_ID} . This is the recommended option : this allows you to launch all your job in parallel with a single script. Interpretation of a report Download an annotated report","title":"FastQC : a report for a single fastq file"},{"location":"days/quality_control.html#multiqc-grouping-multiple-reports","text":"In practice, you likely will have more than a couple of samples (maybe even more than 30 or 50\u2026) to handle: individually consulting and comparing the QC reports of each would be tedious. MultiQC is a tool that lets you combine multiple reports in a single, interactive document that let you explore your data easily. Here, we will be focusing on grouping FastQC reports, but MultiQC can also be applied to the output or logs of other bioinformatics tools, such as mappers, as we will see later. In its default usage, multiqc only needs to be provided a path where it will find all the individual reports, and it will scan them and write a report named multiqc_report.html . Although the default behaviour is quite appropriate, with a couple of options we get a slightly better control over the output: * --interactive : forces the plot to be interactive even when there is a lot of samples (this option can lead to larger html files). * -f <filename> : specify the name of the output file name. For instance, a possible command line could be : multiqc -f multiQCreports/Liu2015_multiqc.html --interactive Liu2015_fastqc/ There are many additional info which let you customize your report. Use multiqc --help or visit their documentation webpage to learn more. Task: Write an sbatch script to run MultiQC for each dataset. Look at the QC reports. What are your conclusions ? Important points: MultiQC RAM requirements : 1Gb should be more than enough. MultiQC time requirements : ~ 1min / read file. Exceptionally, there is no need to load multiqc as a module (it is not part of ComputeCanada and we installed it directly on the cluster, on other clusters it may not be the same). Use multiqc --help to check the different options MultiQC sbatch script This is the script for the Ruhland2016 dataset. It presumes that the fastqc reports can be found in FASTQC_Ruhland2016/ #!/usr/bin/bash #SBATCH --job-name=multiqc #SBATCH --time=00:30:00 #SBATCH --cpus-per-task=1 #SBATCH --mem=1G #SBATCH -o multiqc_Ruhland2016.o #SBATCH -e multiqc_Ruhland2016.e mkdir -p MULTIQC_Ruhland2016/ multiqc -o MULTIQC_Ruhland2016/ FASTQC_Ruhland2016/ Interpretation of a report We will interpret the report for the Liu2015 data. The PHRED quality of reads drop below 30 around base 75. All samples seem affected. One sample seems to have some quality drops at specific timepoints/positions. Mean quality scores are on average fairly high, but some reads exhibit low values. Most samples do not deviate too much from the expected curve. The two samples colored in orange and red have a mode for a very specific value. This may be indicative of contamination, retaining specific rRNA, or adapter sequence content. Ns are present at specific positions in specific samples, in particular for one sample. This is reminiscent of the PHRED quality curves at the top of the report. It seems some flowcells had a problem at specific time-point/positions. This is colored red because this would be a problem if the data was coming from genomic DNA sequencing. However here we are in the context of RNA-seq : some transcripts are present in a large number of copies in the samples, and consequently it is expected that some sequences are over-represented. We see a clear trend of adapter contamination as we get closer to the reads\u2019 end. Note the y-scale though : we never go above a 6% content per sample. Overall, we can conclude that these samples all suffer from some adapter content and a lower quality toward the reads\u2019 second half. Furthermore, a few samples have a peculiar N pattern between bases 20 and 30. It is then strongly advised to either : perform some trimming : remove adapter sequences + cut reads when average quality becomes too low use a mapper that takes base quality in account AND is able to ignore adapter sequence (and even then, you could try mapping on both trimmed and untrimmed data to see which is the best)","title":"MultiQC : grouping multiple reports"},{"location":"days/server_login.html","text":"To conduct the practicals of this course, we will be using a dedicated High Performance Computing cluster. This matches the reality of most NGS workflows, which cannot be completed in a reasonable time on a single machine. To interact with this cluster, you will have to log in to a distant head node . From there you will be able to distribute your computational tasks to the cluster using a job scheduler called Slurm. This page will cover our first contact with the distant cluster. You will learn to : understand a typical computer cluster architecture. connect to the server. use the command line to perform basic operations on the head node. exchange files between the server and your own machine. submit a job to the cluster. Note If you are doing this course on your own, then the distant server provided within the course will not be available. Feel free to ignore or adapt any of the following steps to your own situation. The computing cluster The computing cluster follows an architecture that enables several users to distribute computational tasks among several machines which share a number of resources, such as a common file system. Users do not access each machine individually, but rather connect to a head node . From there, they can interact with the cluster using the job scheduler (here slurm). The job scheduler\u2019s role is to manage where and how to run the jobs of all users, such that waiting time is minimized and resource usage is optimized. Warning Everyone is connected to the same head node. Do not perform compute-intensive tasks on it or you will slow everyone down! Connect to the server Say you want to connect to cluster with address xx.xx.xx.xx and your login is login . Warning If you are doing this course with a teacher, use the link, login and password provided before or during the course. The first step will be to open a terminal Mac Open a terminal, for instance with the application Xterm, or Xquartz. Linux Open a new terminal. Windows Open the application mobaXterm (or any ssh-enabling terminal aplpication you prefer). On mobaXterm, click on \u201cStart a local Terminal\u201d. In the terminal type the following command: ssh login@xx.xx.xx.xx When prompted for your password, type it and press Enter. Note There is no cursor or \u2018\u25cf\u2019 character appearing while you type your password. This is normal. After a few seconds, you should be logged into the head node and ready to begin. Using command line on the cluster Now that you are in the head node, it is time to get acquainted with your environment and to prepare the upcoming practicals. We will also use this as a short reminder about the UNIX command line. You can also refer to this nice Linux Command Line Cheat Sheet . At any time, you can get the location (folder) your terminal is in at by typing the \u201cprint working directory\u201d command: pwd When you start a session on a distant computer, you are placed in your home directory. So the cluster should return something like: /shared/home/<login> Creating a directory Use the command line to create a repository called day1 where you will put all materials relating to this first day. Answer mkdir day1 Move to that directory. Answer cd day1 The directory /shared/data/ contains data and solutions for most practicals. Check the content of that directory. Answer ls /shared/data/ Note You don\u2019t need to move to that directory to list its contents! Copy the script fastqc_Liu2015_SRR1272187_1.sh from /shared/data/Solutions/Liu2015 into your current directory. Answer cp /shared/data/Solutions/Liu2015/fastqc_Liu2015_SRR1272187_1.sh . Print the content of this script to the screen. Answer more fastqc_Liu2015_SRR1272187_1.sh output: #!/usr/bin/bash #SBATCH --job-name=fastqc #SBATCH --time=00:30:00 #SBATCH --cpus-per-task=1 #SBATCH --mem=1G #SBATCH -o fastqc_Liu2015.o #SBATCH -e fastqc_Liu2015.e ml fastqc dataDir=/shared/data/DATA/Liu2015 mkdir -p FASTQC_Liu2015/ fastqc -o FASTQC_Liu2015/ $dataDir/SRR1272187_1.fastq.gz We\u2019ll see what all this means soon. Creating and editing a file To edit files on the distant server, we will use the command line editor nano . It is far from the most complete or efficient one, but it can be found on most servers and is arguably among the easiest to start with. Note Alternatively, feel free to use any other CLI editor you prefer, such as vi . To start editing a file named test.txt , type : nano test.txt You will be taken to the nano interface : Type in your favorite movie quote, and then exit by pressing Ctrl+x ( control+x on a Mac keyboard), and then y and Enter when prompted to save the modifications you just made. You can check that your modifications were saved by typing more test.txt Exchanging files with the server Whether you want to transfer some data to the cluster or retrieve the results of your latest computation, it is important to be able to exchange files with the distant server. There exists several alternatives, depending on your platform and preferences. command line We will use scp . To copy a file from the server to your machine, use this syntax on a terminal in your local machine (open a new terminal if necessary). scp <login>@<server-adress>:/path/to/file/on/server/file.txt /local/destination/ For example, to copy the file test.txt you just created in the folder day1/ , to your current (local) working directory : scp login@xx.xx.xx.xx:~/day1/file.txt . To copy a file from your machine to the server (NB: here ~ will be interpreted as your home directory, this is a useful and time-saving shorthand): scp /path/to/file/local/file.txt <login>@<server-adress>:/destination/on/server/ graphical alternative There are nice and free software with graphical user interfaces, such as filezilla , to help you manage exchanges with the distant server. Feel free to install and experiment with it during the break. mobaXterm If you are using mobaXterm, the left panel should provide a graphical SFTP browser in the left sidebar which allows you to browse and drag and drop files directly from/to the remote server. Submitting jobs Jobs can be submitted to the compute cluster using sbatch scripts , which contain 2 parts : information for the job scheduler: how much RAM / CPUs do I need ? where to write the logs of my job ? bash commands corresponding to your task But an example is worth a thousand words : #!/usr/bin/bash #SBATCH --job-name=test #SBATCH --time=00:30:00 #SBATCH --cpus-per-task=1 #SBATCH --mem=1G #SBATCH -o test_log.o echo \"looking at the size of the elements of /shared/data/\" sleep 10 # making the script wait for 10 seconds - this is just so we can see it later on. # `du` is \"disk usage\", a command that returns the size of a folder structure. du -h -d 2 /shared/data/ The lines beginning by #SBATCH specify options to the job scheduler: The first line indicates what shell should be used to interpret the commands. #SBATCH --job-name=test : the job name #SBATCH --time=00:30:00 : time reserved for the job : 30min. #SBATCH --cpus-per-task=1 : CPUs for the job #SBATCH --mem=1G : memory for the job #SBATCH -o test_log.o : file to write output or error messages Warning Your job will fail as soon as it takes more time or RAM than requested. You might need to test it to find the appropriate values. Copy this script inside a new file named mySbatchScript.sh , then submit it to the job scheduler using : sbatch mySbatchScript.sh Afterward, use the command squeue to monitor the jobs submitted to the cluster. Locate your job and wait for it to be accepted ( RUNNING status), and then to complete (the job disappears from the output of squeue ). Check the output of your job in the output file. Note When there are a lot of jobs, squeue -u <username> will limit the list to those of the specified user. Advanced cluster usage : loading modules During our various analysis, we will call onto numerous software. Fortunately, in most case we do not have to install each of these ourselves onto the cluster : they have already been packaged and prepared to be made available to you or your code. However, by default these are not loaded and you have to explicitely load the module containing the software you want in your script (or in the interactive shell session). Question: Why aren\u2019t all the module already pre-loaded ? Answer Many toolsets have dependencies toward different, sometimes incompatible libraries. Packaging each tool independently and loading them separately circumvents this as you only load what you need, and you can always unload a toolset if you need to load another, incompatible, toolset. Modules are managed with the module command. Basic commands are : module list : lists currently loaded modules module load <modulename> alias ml <modulename> : loads module <modulename> module unload <modulename> : unloads module <modulename> module purge : unloads all loaded modules module avail : lists all modules available for loading module keyword <KW> : lists all modules available for loading which contains <KW> Try it for yourself: soon, we will need the fastqc software. If we type in the terminal: fastqc this should give you an error, telling you there is no such command. Now, if we load it first ml fastqc # shortcut for \"module load fastqc\" fastqc Now you should not have any error. Note: our module provider is ComputeCanada, which has a lot of available software . To avoid storing all these on our cluster, each time a new module is loaded, it is fetched first on the compute canada servers, so sometimes it can take a bit of time to load a module for the first time. Advanced cluster usage : job array Often, we have to repeat a similar analysis on a number of files, or for a number of different parameters. Rather than writing each sbatch script individually, we can rely on job arrays to facilitate our task. Say you want to execute a command, on 10 files (for example, map the reads of 10 samples). You first create a file containing the name of your files (one per line); let\u2019s call it readFiles.txt . Then, you write an sbatch array job script: #!/usr/bin/bash #SBATCH --job-name=test_array #SBATCH --time=00:30:00 #SBATCH --cpus-per-task=1 #SBATCH --mem=1G #SBATCH -o test_array_log.%a.o #SBATCH --array 1-10%5 echo \"job array id\" $SLURM_ARRAY_TASK_ID # sed -n <X>p <file> : retrieve line <X> of file # so the next line grabs the file name corresponding to our job array task id and stores it in the variable ReadFileName ReadFileName = ` sed -n ${ SLURM_ARRAY_TASK_ID } p readFiles.txt ` # here we would put the mapping command or whatever echo $ReadFileName Some things have changed compared to the previous sbatch script : #SBATCH --array 1-10%5 : will spawn independent tasks with ids from 1 to 10, and will manage them so that at most 5 run at the same time. #SBATCH -o test_array_log.%a.o : the %a will take the value of the array task id. So we will have 1 log file per task (so 10 files). $SLURM_ARRAY_TASK_ID : changes value between the different tasks. This is what we use to execute the same script on different files (using sed -n ${SLURM_ARRAY_TASK_ID}p )","title":"Server login + unix fresh up"},{"location":"days/server_login.html#the-computing-cluster","text":"The computing cluster follows an architecture that enables several users to distribute computational tasks among several machines which share a number of resources, such as a common file system. Users do not access each machine individually, but rather connect to a head node . From there, they can interact with the cluster using the job scheduler (here slurm). The job scheduler\u2019s role is to manage where and how to run the jobs of all users, such that waiting time is minimized and resource usage is optimized. Warning Everyone is connected to the same head node. Do not perform compute-intensive tasks on it or you will slow everyone down!","title":"The computing cluster"},{"location":"days/server_login.html#connect-to-the-server","text":"Say you want to connect to cluster with address xx.xx.xx.xx and your login is login . Warning If you are doing this course with a teacher, use the link, login and password provided before or during the course. The first step will be to open a terminal Mac Open a terminal, for instance with the application Xterm, or Xquartz. Linux Open a new terminal. Windows Open the application mobaXterm (or any ssh-enabling terminal aplpication you prefer). On mobaXterm, click on \u201cStart a local Terminal\u201d. In the terminal type the following command: ssh login@xx.xx.xx.xx When prompted for your password, type it and press Enter. Note There is no cursor or \u2018\u25cf\u2019 character appearing while you type your password. This is normal. After a few seconds, you should be logged into the head node and ready to begin.","title":"Connect to the server"},{"location":"days/server_login.html#using-command-line-on-the-cluster","text":"Now that you are in the head node, it is time to get acquainted with your environment and to prepare the upcoming practicals. We will also use this as a short reminder about the UNIX command line. You can also refer to this nice Linux Command Line Cheat Sheet . At any time, you can get the location (folder) your terminal is in at by typing the \u201cprint working directory\u201d command: pwd When you start a session on a distant computer, you are placed in your home directory. So the cluster should return something like: /shared/home/<login>","title":"Using command line on the cluster"},{"location":"days/server_login.html#creating-a-directory","text":"Use the command line to create a repository called day1 where you will put all materials relating to this first day. Answer mkdir day1 Move to that directory. Answer cd day1 The directory /shared/data/ contains data and solutions for most practicals. Check the content of that directory. Answer ls /shared/data/ Note You don\u2019t need to move to that directory to list its contents! Copy the script fastqc_Liu2015_SRR1272187_1.sh from /shared/data/Solutions/Liu2015 into your current directory. Answer cp /shared/data/Solutions/Liu2015/fastqc_Liu2015_SRR1272187_1.sh . Print the content of this script to the screen. Answer more fastqc_Liu2015_SRR1272187_1.sh output: #!/usr/bin/bash #SBATCH --job-name=fastqc #SBATCH --time=00:30:00 #SBATCH --cpus-per-task=1 #SBATCH --mem=1G #SBATCH -o fastqc_Liu2015.o #SBATCH -e fastqc_Liu2015.e ml fastqc dataDir=/shared/data/DATA/Liu2015 mkdir -p FASTQC_Liu2015/ fastqc -o FASTQC_Liu2015/ $dataDir/SRR1272187_1.fastq.gz We\u2019ll see what all this means soon.","title":"Creating a directory"},{"location":"days/server_login.html#creating-and-editing-a-file","text":"To edit files on the distant server, we will use the command line editor nano . It is far from the most complete or efficient one, but it can be found on most servers and is arguably among the easiest to start with. Note Alternatively, feel free to use any other CLI editor you prefer, such as vi . To start editing a file named test.txt , type : nano test.txt You will be taken to the nano interface : Type in your favorite movie quote, and then exit by pressing Ctrl+x ( control+x on a Mac keyboard), and then y and Enter when prompted to save the modifications you just made. You can check that your modifications were saved by typing more test.txt","title":"Creating and editing a file"},{"location":"days/server_login.html#exchanging-files-with-the-server","text":"Whether you want to transfer some data to the cluster or retrieve the results of your latest computation, it is important to be able to exchange files with the distant server. There exists several alternatives, depending on your platform and preferences. command line We will use scp . To copy a file from the server to your machine, use this syntax on a terminal in your local machine (open a new terminal if necessary). scp <login>@<server-adress>:/path/to/file/on/server/file.txt /local/destination/ For example, to copy the file test.txt you just created in the folder day1/ , to your current (local) working directory : scp login@xx.xx.xx.xx:~/day1/file.txt . To copy a file from your machine to the server (NB: here ~ will be interpreted as your home directory, this is a useful and time-saving shorthand): scp /path/to/file/local/file.txt <login>@<server-adress>:/destination/on/server/ graphical alternative There are nice and free software with graphical user interfaces, such as filezilla , to help you manage exchanges with the distant server. Feel free to install and experiment with it during the break. mobaXterm If you are using mobaXterm, the left panel should provide a graphical SFTP browser in the left sidebar which allows you to browse and drag and drop files directly from/to the remote server.","title":"Exchanging files with the server"},{"location":"days/server_login.html#submitting-jobs","text":"Jobs can be submitted to the compute cluster using sbatch scripts , which contain 2 parts : information for the job scheduler: how much RAM / CPUs do I need ? where to write the logs of my job ? bash commands corresponding to your task But an example is worth a thousand words : #!/usr/bin/bash #SBATCH --job-name=test #SBATCH --time=00:30:00 #SBATCH --cpus-per-task=1 #SBATCH --mem=1G #SBATCH -o test_log.o echo \"looking at the size of the elements of /shared/data/\" sleep 10 # making the script wait for 10 seconds - this is just so we can see it later on. # `du` is \"disk usage\", a command that returns the size of a folder structure. du -h -d 2 /shared/data/ The lines beginning by #SBATCH specify options to the job scheduler: The first line indicates what shell should be used to interpret the commands. #SBATCH --job-name=test : the job name #SBATCH --time=00:30:00 : time reserved for the job : 30min. #SBATCH --cpus-per-task=1 : CPUs for the job #SBATCH --mem=1G : memory for the job #SBATCH -o test_log.o : file to write output or error messages Warning Your job will fail as soon as it takes more time or RAM than requested. You might need to test it to find the appropriate values. Copy this script inside a new file named mySbatchScript.sh , then submit it to the job scheduler using : sbatch mySbatchScript.sh Afterward, use the command squeue to monitor the jobs submitted to the cluster. Locate your job and wait for it to be accepted ( RUNNING status), and then to complete (the job disappears from the output of squeue ). Check the output of your job in the output file. Note When there are a lot of jobs, squeue -u <username> will limit the list to those of the specified user.","title":"Submitting jobs"},{"location":"days/server_login.html#advanced-cluster-usage-loading-modules","text":"During our various analysis, we will call onto numerous software. Fortunately, in most case we do not have to install each of these ourselves onto the cluster : they have already been packaged and prepared to be made available to you or your code. However, by default these are not loaded and you have to explicitely load the module containing the software you want in your script (or in the interactive shell session). Question: Why aren\u2019t all the module already pre-loaded ? Answer Many toolsets have dependencies toward different, sometimes incompatible libraries. Packaging each tool independently and loading them separately circumvents this as you only load what you need, and you can always unload a toolset if you need to load another, incompatible, toolset. Modules are managed with the module command. Basic commands are : module list : lists currently loaded modules module load <modulename> alias ml <modulename> : loads module <modulename> module unload <modulename> : unloads module <modulename> module purge : unloads all loaded modules module avail : lists all modules available for loading module keyword <KW> : lists all modules available for loading which contains <KW> Try it for yourself: soon, we will need the fastqc software. If we type in the terminal: fastqc this should give you an error, telling you there is no such command. Now, if we load it first ml fastqc # shortcut for \"module load fastqc\" fastqc Now you should not have any error. Note: our module provider is ComputeCanada, which has a lot of available software . To avoid storing all these on our cluster, each time a new module is loaded, it is fetched first on the compute canada servers, so sometimes it can take a bit of time to load a module for the first time.","title":"Advanced cluster usage : loading modules"},{"location":"days/server_login.html#advanced-cluster-usage-job-array","text":"Often, we have to repeat a similar analysis on a number of files, or for a number of different parameters. Rather than writing each sbatch script individually, we can rely on job arrays to facilitate our task. Say you want to execute a command, on 10 files (for example, map the reads of 10 samples). You first create a file containing the name of your files (one per line); let\u2019s call it readFiles.txt . Then, you write an sbatch array job script: #!/usr/bin/bash #SBATCH --job-name=test_array #SBATCH --time=00:30:00 #SBATCH --cpus-per-task=1 #SBATCH --mem=1G #SBATCH -o test_array_log.%a.o #SBATCH --array 1-10%5 echo \"job array id\" $SLURM_ARRAY_TASK_ID # sed -n <X>p <file> : retrieve line <X> of file # so the next line grabs the file name corresponding to our job array task id and stores it in the variable ReadFileName ReadFileName = ` sed -n ${ SLURM_ARRAY_TASK_ID } p readFiles.txt ` # here we would put the mapping command or whatever echo $ReadFileName Some things have changed compared to the previous sbatch script : #SBATCH --array 1-10%5 : will spawn independent tasks with ids from 1 to 10, and will manage them so that at most 5 run at the same time. #SBATCH -o test_array_log.%a.o : the %a will take the value of the array task id. So we will have 1 log file per task (so 10 files). $SLURM_ARRAY_TASK_ID : changes value between the different tasks. This is what we use to execute the same script on different files (using sed -n ${SLURM_ARRAY_TASK_ID}p )","title":"Advanced cluster usage : job array"},{"location":"days/trimming.html","text":"Following a QC analysis on sequencing results, one may detect stretches of low quality bases along reads, or a contamination by adapter sequence. Depending on your research question and the software you use for mapping, you may have to remove these bad quality / spurious sequences out of your data. During this block, you will learn to : trim your data with trimmomatic Material Download the presentation Trimmomatic website to trim or not to trim ? There are several ways to deal with poor quality bases or adapter contamination in reads, and several terms are used in the field, sometimes very loosely. We can talk about: Trimming : to remove a part of, or the entirety of, a read (for quality reasons). Hard trimming : trim with a high standard of quality (eg. remove everything with QUAL<30). Soft trimming: trim with a low standard of quality (eg. remove everything with QUAL<10). Clipping : to remove the end part of a read (typically because of adapter content). Hard clipping: actually removing the end of the read from the file (ie. with trimmomatic). Soft clipping: ignoring the end of the read at mapping time (ie. what STAR does). If the data will be used to perform transcriptome assembly, or variant analysis, then it must be trimmed . In contrast, for applications based on counting reads , such as Differential Expression analysis , most aligners, such as STAR , HISAT2 , salmon , and kallisto , can handle bad quality sequences and adapter content by soft-clipping, and consequently they usually do not need trimming. In fact, trimming can be detrimental to the number of successfully quantified reads [ William et al. 2016 ]. Nevertheless, it is usually recommended to perform some amount of soft clipping ( eg. kallisto , salmon ). If possible, we recommend to perform the mapping for both the raw data and the trimmed one, in order to compare the results for both, and choose the best. Question: what could be a good metric to choose the best between the trimmed and untrimmed ? Answer The number of uniquely mapped reads is generally what would matter in differential expression analysis. Of course, this means that you can only choose after you have mapped both the trimmed and the untrimmed reads. trimming with Trimmomatic The trimmomatic website gives very good examples of their software usage for both paired-end ( PE ) and single-end ( SE ) reads. We recommend you read their quick-start section attentively. Task : Conduct a soft trimming on the Liu2015 data. Extra (if you have the time) : run a QC analysis on your trimmmed reads and compare with the raw ones. Important notes : when you do ml trimmomatic , you will receive a little message which tells you how to launch the software. Adapter sequences can be found in /shared/data/DATA/adapters/ . Trimmomatic RAM requirements : ~0.5G / CPU. Trimmomatic time requirements : ~ 10 min/ read file . Trimmomatic script The Liu2015 dataset has paired-end reads and we have to take that into account during trimming. For a soft-trimming, we chose the following options : SLIDINGWINDOW:4:20 Perform a sliding window trimming, cutting once the average quality within the window falls below a threshold. 4 : windowSize: specifies the number of bases to average across 20 : requiredQuality: specifies the average quality required. ILLUMINACLIP:/shared/home/SHARED/DATA/adapters/TruSeq3-PE.fa:2:30:10 Cut adapter and other Illumina-specific sequences from the read. Cut adapter and other illumina-specific sequences from the read. 2 : seedMismatches: specifies the maximum mismatch count which will still allow a full match to be performed 30 : palindromeClipThreshold: specifies how accurate the match between the two \u2018adapter ligated\u2019 reads must be for PE palindrome read alignment. 10 : simpleClipThreshold: specifies how accurate the match between any adapter etc. sequence must be against a read. Here is a script for a single sample : #!/bin/bash #SBATCH --job-name=trim #SBATCH --time=01:00:00 #SBATCH --cpus-per-task=4 #SBATCH --mem-per-cpu=4G #SBATCH -o trim.o #SBATCH -e trim.e ml trimmomatic dataDIR = /shared/data/DATA/Liu2015 trimmomatic = \"java -jar $EBROOTTRIMMOMATIC /trimmomatic-0.39.jar\" outDIR = Liu2015_trimmed_reads mkdir -p $outDIR $trimmomatic PE -threads 4 -phred33 \\ $dataDIR /SRR1272187_1.fastq.gz \\ $dataDIR /SRR1272187_2.fastq.gz \\ $outDIR /SRR1272187_NFLV_trimmed_paired_1.fastq $outDIR /SRR1272187_NFLV_trimmed_unpaired_1.fastq \\ $outDIR /SRR1272187_NFLV_trimmed_paired_2.fastq $outDIR /SRR1272187_NFLV_trimmed_unpaired_2.fastq \\ SLIDINGWINDOW:4:20 ILLUMINACLIP:/shared/data/DATA/adapters/TruSeq3-PE.fa:2:30:10 ## compressing the resulting fastq files to save some space. gzip $outDIR /SRR1272187_NFLV_trimmed_paired_1.fastq gzip $outDIR /SRR1272187_NFLV_trimmed_unpaired_1.fastq gzip $outDIR /SRR1272187_NFLV_trimmed_paired_2.fastq gzip $outDIR /SRR1272187_NFLV_trimmed_unpaired_2.fastq","title":"Sequence trimming"},{"location":"days/trimming.html#material","text":"Download the presentation Trimmomatic website","title":"Material"},{"location":"days/trimming.html#to-trim-or-not-to-trim","text":"There are several ways to deal with poor quality bases or adapter contamination in reads, and several terms are used in the field, sometimes very loosely. We can talk about: Trimming : to remove a part of, or the entirety of, a read (for quality reasons). Hard trimming : trim with a high standard of quality (eg. remove everything with QUAL<30). Soft trimming: trim with a low standard of quality (eg. remove everything with QUAL<10). Clipping : to remove the end part of a read (typically because of adapter content). Hard clipping: actually removing the end of the read from the file (ie. with trimmomatic). Soft clipping: ignoring the end of the read at mapping time (ie. what STAR does). If the data will be used to perform transcriptome assembly, or variant analysis, then it must be trimmed . In contrast, for applications based on counting reads , such as Differential Expression analysis , most aligners, such as STAR , HISAT2 , salmon , and kallisto , can handle bad quality sequences and adapter content by soft-clipping, and consequently they usually do not need trimming. In fact, trimming can be detrimental to the number of successfully quantified reads [ William et al. 2016 ]. Nevertheless, it is usually recommended to perform some amount of soft clipping ( eg. kallisto , salmon ). If possible, we recommend to perform the mapping for both the raw data and the trimmed one, in order to compare the results for both, and choose the best. Question: what could be a good metric to choose the best between the trimmed and untrimmed ? Answer The number of uniquely mapped reads is generally what would matter in differential expression analysis. Of course, this means that you can only choose after you have mapped both the trimmed and the untrimmed reads.","title":"to trim or not to trim ?"},{"location":"days/trimming.html#trimming-with-trimmomatic","text":"The trimmomatic website gives very good examples of their software usage for both paired-end ( PE ) and single-end ( SE ) reads. We recommend you read their quick-start section attentively. Task : Conduct a soft trimming on the Liu2015 data. Extra (if you have the time) : run a QC analysis on your trimmmed reads and compare with the raw ones. Important notes : when you do ml trimmomatic , you will receive a little message which tells you how to launch the software. Adapter sequences can be found in /shared/data/DATA/adapters/ . Trimmomatic RAM requirements : ~0.5G / CPU. Trimmomatic time requirements : ~ 10 min/ read file . Trimmomatic script The Liu2015 dataset has paired-end reads and we have to take that into account during trimming. For a soft-trimming, we chose the following options : SLIDINGWINDOW:4:20 Perform a sliding window trimming, cutting once the average quality within the window falls below a threshold. 4 : windowSize: specifies the number of bases to average across 20 : requiredQuality: specifies the average quality required. ILLUMINACLIP:/shared/home/SHARED/DATA/adapters/TruSeq3-PE.fa:2:30:10 Cut adapter and other Illumina-specific sequences from the read. Cut adapter and other illumina-specific sequences from the read. 2 : seedMismatches: specifies the maximum mismatch count which will still allow a full match to be performed 30 : palindromeClipThreshold: specifies how accurate the match between the two \u2018adapter ligated\u2019 reads must be for PE palindrome read alignment. 10 : simpleClipThreshold: specifies how accurate the match between any adapter etc. sequence must be against a read. Here is a script for a single sample : #!/bin/bash #SBATCH --job-name=trim #SBATCH --time=01:00:00 #SBATCH --cpus-per-task=4 #SBATCH --mem-per-cpu=4G #SBATCH -o trim.o #SBATCH -e trim.e ml trimmomatic dataDIR = /shared/data/DATA/Liu2015 trimmomatic = \"java -jar $EBROOTTRIMMOMATIC /trimmomatic-0.39.jar\" outDIR = Liu2015_trimmed_reads mkdir -p $outDIR $trimmomatic PE -threads 4 -phred33 \\ $dataDIR /SRR1272187_1.fastq.gz \\ $dataDIR /SRR1272187_2.fastq.gz \\ $outDIR /SRR1272187_NFLV_trimmed_paired_1.fastq $outDIR /SRR1272187_NFLV_trimmed_unpaired_1.fastq \\ $outDIR /SRR1272187_NFLV_trimmed_paired_2.fastq $outDIR /SRR1272187_NFLV_trimmed_unpaired_2.fastq \\ SLIDINGWINDOW:4:20 ILLUMINACLIP:/shared/data/DATA/adapters/TruSeq3-PE.fa:2:30:10 ## compressing the resulting fastq files to save some space. gzip $outDIR /SRR1272187_NFLV_trimmed_paired_1.fastq gzip $outDIR /SRR1272187_NFLV_trimmed_unpaired_1.fastq gzip $outDIR /SRR1272187_NFLV_trimmed_paired_2.fastq gzip $outDIR /SRR1272187_NFLV_trimmed_unpaired_2.fastq","title":"trimming with Trimmomatic"}]}
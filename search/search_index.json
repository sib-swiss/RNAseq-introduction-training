{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction to RNA-Seq: From quality control to pathway analysis","text":""},{"location":"#learning-outcomes","title":"Learning outcomes","text":""},{"location":"#general-learning-outcomes","title":"General learning outcomes","text":"<p>After this course, you will be able to:</p> <ul> <li>Describe advantages and pitfalls of RNA sequencing.</li> <li>Design your own experiment.</li> <li>Perform the downstream analysis using command line software   (QC, mapping, counting, differential expression analysis, pathway analysis, etc).</li> <li>Critically assess the quality of your results at each step of the downstream analysis.</li> <li>Detect significantly differentially-expressed genes between conditions.</li> </ul>"},{"location":"#learning-experiences","title":"Learning experiences","text":"<p>To reach the learning outcomes, we will use lectures and exercises.  During lectures, do not hesitate to ask any questions as we progress through the slides. During exercises, you are free to discuss with other participants. </p> <p>Exercises are provided with solutions. How you use them to your advantage is up to you.</p>"},{"location":"course_schedule/","title":"Course schedule","text":""},{"location":"course_schedule/#day-1","title":"Day 1","text":"start end subject 9:00 9:15 Welcome 9:15 10:45 RNAseq - technologies and design 10:45 11:00 BREAK 11:00 11:30 Server login + unix fresh up 11:30 12:30 Quality control 12:30 13:30 LUNCH BREAK 13:30 14:30 Sequence Trimming and adapter removal 14:30 15:30 Reads mapping : indexing genome 15:30 15:45 BREAK 15:45 17:00 Reads mapping : mapping"},{"location":"course_schedule/#day-2","title":"Day 2","text":"start end subject 9:00 9:15 Recap of yesterday 9:15 10:30 Read counting 10:30 10:45 BREAK 10:30 12:30 Differential Expression Inference - theory  12:30 13:30 LUNCH BREAK 13:30 17:00 Differential Expression Inference - practice"},{"location":"precourse/","title":"Precourse preparations","text":"<p>On top of a thirst for knowledge, and a working Internet connection, here is what you will need for the course : </p>"},{"location":"precourse/#ngs","title":"NGS","text":"<p>As announced in the course registration webpage, we expect participants to already have a basic knowledge in Next Generation Sequencing (NGS) techniques. </p>"},{"location":"precourse/#unix","title":"UNIX","text":"<p>Practical knowledge of the UNIX command line is also required to be able to follow this course, given that that the tools used to process sequenced reads use this interface.</p> <p>If you are unsure about your capabilities or feel a bit rusty, we strongly recommend you spend some time practicing before the course : in our experience, the more comfortable you are with UNIX, the more you will be able to focus on the RNA-seq during the course, and the more you will gain from it.</p> <p>You may refer to the SIB\u2019s UNIX e-learning module</p>"},{"location":"precourse/#r","title":"R","text":"<p>A basic knowledge of the R language is required to perform most analytical steps after reads have been mapped and quantified : differential gene expression, gene set enrichment, over-representation analysis.</p> <p>If you are not familiar with R, we recommend the SIB First Steps with R course, or you can pick one among this list</p>"},{"location":"precourse/#software","title":"Software","text":"<p>To replicate the technical condition of today\u2019s real-life data analysis, we will perform our computations on a distant HPC cluster. To access it: </p> <ul> <li>macOS / Linux : you can use your pre-installed terminal.</li> <li>Windows : you should install a terminal which lets you do ssh (for instance mobaXterm). </li> </ul> <p>Additionally, a graphical client for file transfer to and from the distant server can be useful. MobaXterm integrates this functionality, so if you use it there is no need for additional software.  Otherwise, we recommend FileZilla.</p>"},{"location":"slides_notes/","title":"Slides notes","text":"<p>This document contains notes about the course slides.</p> <p>This is a somewhat internal document, so expect a fairly draft-ish and concise style.</p>"},{"location":"slides_notes/#01-overview","title":"01 Overview","text":"<p>slides 3 - 7  * the central dogma of molecular biology is know to be not so simple  * RNA is not only a messenger but may have an effect  * most of these elements interacts and regulates one-another   * alternative splicing in eukaryots adds a layer of possibilities to all this  * main takeaway maybe : measuring RNA is a proxy for protein levels, which is a proxy for protein activity , which is a proxy for the physiological state of the cell</p> <p>slides 8 - 9  * non-exhaustive list of sequencing possibilities   * https://liorpachter.wordpress.com/seq/</p> <p>slides 10-12</p> <ul> <li>RNAseq, the challenges</li> <li>slide 10 : from human gff, includes ncRNAs</li> <li> <p>slide 11 : </p> <ul> <li>data source: https://gtexportal.org/home/datasets (V8 gene TPMs)</li> <li>left: 1 sample -&gt; from 1 to $10^5$ TPM </li> <li>right: 50 random samples -&gt; 10% of genes contribute 90% of the transcripts</li> <li>NB: mammalian cell 10-30pg RNA/cell , around 360 000 mRNA molecules (source: https://www.qiagen.com/us/resources/faq?id=06a192c2-e72d-42e8-9b40-3171e1eb4cb8&amp;lang=en )</li> </ul> </li> <li> <p>slide 12 : important considerations as well</p> </li> </ul> <p>slide 13  * Illumina : market leader 50-600bp (generally 50-100), 0.1 (nextseq) to 3 (Hiseq) billion reads  * Ion torrent : 600bp, 260M reads  * Pacbio : 10-30kb N50 , 4M CCS reads   * nanopore : theory single molecule, practice: variable (N50 &gt;100kb on ultra-long kit, up to 4.2Mb )</p> <p>slides 14-22 : describe different technologies</p> <p>Ion Torrent :     - cell sequentially flooded with A T G C </p> <p>PacBio SMRT     - DNApol at bottom of Zero-Mode-Waveguide      - fluorescent dye on dNTPs</p> <p>Illumina seq :     - formation of clusters with the same sequence      - SBS : labelled nucleotides have reversible terminators, so only 1 base is incorporated at a time.</p> <p>slide 23: paired end sequencing https://www.france-genomique.org/technological-expertises/whole-genome/sequencage-a-courtes-lectures-par-clusterisation/?lang=en</p> <p>slide 24: stranded sequencing https://www.ecseq.com/support/ngs/how-do-strand-specific-sequencing-protocols-work</p> <p>slide 25-26: RIN , RNA purification</p> <p>slide 27-34: sequencing depth and replicates  * slide 33-34 : this pattern applies to low- mid- and high- expressors (see their supp doc)</p> <p>slide 35-42: schematic analysis  * slide 35 : before the sequencing  * slide 36 : basic analysis  * slide 37 : basic analysis with trimmed reads  * slide 38 : main QC steps  * slide 39 : QC steps provide feedback on previous steps  * slide 40 : analysis for variant calling / isoform descriptions / \u2026  * slide 41 : when no reference genome: de novo assembly  * slide 42 : the analysis which we\u2019ll do during this course</p>"},{"location":"days/DE/","title":"Differential Expression Inference","text":"<p>Once the reads have been mapped and counted, one can assess the differential expression of genes between different conditions.</p> <p>During this lesson, you will learn to :</p> <ul> <li>describe the different steps of data normalization and modelling commonly used for RNA-seq data.</li> <li>detect significantly differentially-expressed genes using either edgeR or DESeq2.</li> <li>perform downstream analysis on gene sets, such as annotation (e.g. GO terms or Reactome pathways) over-representation.</li> </ul>"},{"location":"days/DE/#material","title":"Material","text":"<p> Download the presentation</p> <p>Rstudio website</p> <p>Note</p> <p>RStudio is set to be rebranded as Posit after October 2022.</p> <p>edgeR user\u2019s guide</p> <p>DESeq2 vignette</p>"},{"location":"days/DE/#connexion-to-the-rstudio-server","title":"Connexion to the Rstudio server","text":"<p>Note</p> <p>This step is intended only for users who attend the course with a teacher. Otherwise you will have to rely on your own installation of Rstudio.</p> <p>The analysis of the read count data will be done on an RStudio instance, using the R language and some relevant Bioconductor libraries.</p> <p>As you start your session on the RStudio server, please make sure that you know where your data is situated with respect to your working directory (use <code>getwd()</code> and <code>setwd()</code> to respectively : know what your working is, and change it as necessary).</p>"},{"location":"days/DE/#differential-expression-inference","title":"Differential Expression Inference","text":"<p>Use either edgeR or DESeq2 to conduct a differential expression analysis on the Ruhland2016 and/or Liu2015 dataset.</p> <p>You can find the expression matrices on the server at: <code>/shared/data/Solutions/Ruhland2016/countFiles/featureCounts_Ruhland2016.counts.txt</code> and <code>/shared/data/Solutions/Liu2015/countFiles/featureCounts_Liu2015.counts.txt</code></p> <p>Or you may download them :</p> <p>Liu2015 count matrix</p> <p>Ruhland2016 count matrix</p> <p>Note</p> <ul> <li>Generally, users find the syntax and workflow of DESeq2 easier for getting started.</li> <li>If you have the time, conduct a differential expression analysis using both DESeq2 and edgeR.</li> <li>Follow the vignettes/user\u2019s guide! They are the most up-to-date and generally contain everything a newcomer might need, including worked-out examples.</li> </ul>"},{"location":"days/DE/#deseq2","title":"DESeq2","text":"<p>DESeq2 vignette</p> read in the data <pre><code># setup\nlibrary(DESeq2)\nlibrary(ggplot2)\n\n\n# reading the counts files - adapt the file path to your situation\nraw_counts &lt;-read.table('/shared/data/Solutions/Ruhland2016/countFiles/featureCounts_Ruhland2016.counts.txt' , skip=1 , sep=\"\\t\" , header=T)\n\n# setting up row names as ensembl gene ids\nrow.names(raw_counts) = raw_counts$Geneid\n\n## looking at the beginning of that table\nraw_counts[1:5,1:5]\n\n# removing these first columns to keep only the sample counts\nraw_counts = raw_counts[ ,  -1:-6  ] # changing column names\nnames( raw_counts) = gsub('_.*', '', gsub('.*.SRR[0-9]{7}_', '', names(raw_counts) ) )\n\n# some checking of what we just read\nhead(raw_counts); tail(raw_counts); dim(raw_counts)\ncolSums(raw_counts) # total number of counted reads per sample\n</code></pre> preprocessing <pre><code>## telling DESeq2 what the experimental design was\n# note: by default, the 1st level is considered to be the reference/control/WT/...\ntreatment &lt;- factor( c(rep(\"EtOH\",3), rep(\"TAM\",3)), levels=c(\"EtOH\", \"TAM\") )\ncolData &lt;- data.frame(treatment, row.names = colnames(raw_counts))\ncolData\n\n## creating the DESeq data object &amp; positing the model\ndds &lt;- DESeqDataSetFromMatrix(\ncountData = raw_counts, colData = colData, design = ~ treatment)\ndim(dds)\n\n## filter low count genes. Here, only keep genes with at least 2 samples where there are at least 5 reads.\nidx &lt;- rowSums(counts(dds, normalized=FALSE) &gt;= 5) &gt;= 2\ndds.f &lt;- dds[idx, ]\ndim(dds.f)\n\n# we go from 55414 to 19378 genes\n</code></pre> <p>Around 19k genes pass our minimum expression threshold, quite typical for a bulk Mouse RNA-seq experiment.</p> estimate dispersion / model fitting <pre><code># we perform the estimation of dispersions \ndds.f &lt;- DESeq(dds.f)\n\n# we plot the estimate of the dispersions\n# * black dot : raw\n# * red dot : local trend\n# * blue : corrected\nplotDispEsts(dds.f)\n\n# extracting results for the treatment versus control contrast\nres &lt;- results(dds.f)\n</code></pre> <p></p> <p>This plot is not easy to interpret. It represents the amount of dispersion at different levels of expression. It is directly linked to our ability to detect differential expression.</p> <p>Here it looks about normal compared to typical bulk RNA-seq experiments : the dispersion is comparatively larger for lowly expressed genes.</p> looking at the results <pre><code># adds estimate of the LFC the results table. \n# This shrunk logFC estimate is more robust than the raw value\n\nhead(coef(dds.f)) # the second column corresponds to the difference between the 2 conditions\nres.lfc &lt;- lfcShrink(dds.f, coef=2, res=res)\n\n#plotting to see the difference.  \npar(mfrow=c(2,1))\nDESeq2::plotMA(res)\nDESeq2::plotMA(res.lfc)\n# -&gt; with shrinkage, the significativeness and logFC are more consistent\npar(mfrow=c(1,1))\n</code></pre> <p></p> <p>Without the shrinkage, we can see that for low counts we can see a high log-fold change but non significant (ie. we see a large difference but with variance is also so high that this observation may be due to chance only).</p> <p>The shrinkage corrects this and the relationship between logFC and significance is smoother.</p> <p><pre><code># we apply the variance stabilising transformation to make the read counts comparable across libraries\n# (nb : this is not needed for DESeq DE analysis, but rather for visualisations that compare expression across samples, such as PCA. This replaces normal PCA scaling)\nvst.dds.f &lt;- vst(dds.f, blind = FALSE)\nvst.dds.f.counts &lt;- assay(vst.dds.f)\n\nplotPCA(vst.dds.f, intgroup = c(\"treatment\"))\n</code></pre> </p> <p>The first axis (58% of the variance) seems linked to the grouping of interest.</p> <pre><code>## ggplot2-based volcano plot\nlibrary(ggplot2)\n\nFDRthreshold = 0.01\nlogFCthreshold = 1.0\n# add a column of NAs\nres.lfc$diffexpressed &lt;- \"NO\"\n# if log2Foldchange &gt; 1 and pvalue &lt; 0.01, set as \"UP\" \nres.lfc$diffexpressed[res.lfc$log2FoldChange &gt; logFCthreshold &amp; res.lfc$padj &lt; FDRthreshold] &lt;- \"UP\"\n# if log2Foldchange &lt; 1 and pvalue &lt; 0.01, set as \"DOWN\"\nres.lfc$diffexpressed[res.lfc$log2FoldChange &lt; -logFCthreshold &amp; res.lfc$padj &lt; FDRthreshold] &lt;- \"DOWN\"\n\nggplot( data = data.frame( res.lfc ) , aes( x=log2FoldChange , y = -log10(padj) , col =diffexpressed ) ) + geom_point() + geom_vline(xintercept=c(-logFCthreshold, logFCthreshold), col=\"red\") +\ngeom_hline(yintercept=-log10(FDRthreshold), col=\"red\") +\nscale_color_manual(values=c(\"blue\", \"grey\", \"red\"))\n\ntable(res.lfc$diffexpressed)\n</code></pre> <pre><code> DOWN    NO    UP \n  131 19002   245 \n</code></pre> <p></p> <pre><code>library(pheatmap)\ntopVarGenes &lt;- head(order(rowVars(vst.dds.f.counts), decreasing = TRUE), 20)\nmat  &lt;- vst.dds.f.counts[ topVarGenes, ] #scaled counts of the top genes\nmat  &lt;- mat - rowMeans(mat)  # centering\npheatmap(mat)\n</code></pre> <p></p> <pre><code># saving results to file\n# note: a CSV file can be imported into Excel\nwrite.csv( res ,'Ruhland2016.DESeq2.results.csv' )\n</code></pre>"},{"location":"days/DE/#edger","title":"EdgeR","text":"<p>edgeR user\u2019s guide</p> read in the data <pre><code>library(edgeR)\nlibrary(ggplot2)\n\n# reading the counts files - adapt the file path to your situation\nraw_counts &lt;- read.table('.../Ruhland2016_featureCount_result.counts' , skip=1 , sep=\"\\t\" , header=T)\n\n# setting up row names as ensembl gene ids\nrow.names(raw_counts) = raw_counts$Geneid\n\n# removing these first columns to keep only the sample counts\nraw_counts = raw_counts[ ,  -1:-6  ] # changing column names\nnames( raw_counts) = gsub('_.*', '', gsub('.*.SRR[0-9]{7}_', '', names(raw_counts) ) )\n\n# some checking of what we just read\nhead(raw_counts); tail(raw_counts); dim(raw_counts)\ncolSums(raw_counts) # total number of counted reads per sample\n</code></pre> edgeR object preprocessing <pre><code># setting up the experimental design AND the model\n#  -&gt; the first 3 samples form a group, the 3 remaining are the other group\ntreatment &lt;-  c(rep(\"EtOH\",3), rep(\"TAM\",3))\ndge.f.design &lt;- model.matrix(~ treatment)\n\n# creating the edgeR DGE object\ndge.all &lt;- DGEList(counts = raw_counts , group = treatment)  # filtering by expression level. See ?filterByExpr for details\nkeep &lt;- filterByExpr(dge.all)\ndge.f &lt;- dge.all[keep, keep.lib.sizes=FALSE]\ntable( keep )\n</code></pre> <pre><code>keep\nFALSE  TRUE \n39702 15712 \n</code></pre> <p>Around 16k genes are sufficiently expressed to be retained.</p> <pre><code>#normalization\ndge.f &lt;- calcNormFactors(dge.f)\ndge.f$samples\n</code></pre> <p>Each sample has been associated with a normalization factor.</p> edgeR model fitting <p><pre><code># estimate of the dispersion\ndge.f &lt;- estimateDisp(dge.f,dge.f.design , robust = T)\nplotBCV(dge.f)\n</code></pre> </p> <p>This plot is not easy to interpret. It represents the amount of biological variation at different levels of expression. It is directly linked to our ability to detect differential expression.</p> <p>Here it looks about normal compared to other bulk RNA-seq experiments : the variation is comparatively larger for lowly expressed genes.</p> <pre><code># testing for differential expression. \n# This method is recommended when you only have 2 groups to compare\ndge.f.et &lt;- exactTest(dge.f)\ntopTags(dge.f.et) # printing the genes where the p-value of differential expression if the lowest\n</code></pre> <pre><code>Comparison of groups:  TAM-EtOH \n                       logFC   logCPM       PValue          FDR\nENSMUSG00000050272 -8.522762 4.988067 2.554513e-28 3.851950e-24\nENSMUSG00000075014  3.890079 5.175181 2.036909e-25 1.535728e-21\nENSMUSG00000009185  3.837786 6.742422 1.553964e-22 7.810743e-19\nENSMUSG00000075015  3.778523 3.274463 2.106799e-22 7.942107e-19\nENSMUSG00000028339 -5.692069 6.372980 4.593720e-16 1.385374e-12\nENSMUSG00000040111 -2.141221 6.771538 4.954522e-15 1.245154e-11\nENSMUSG00000041695  4.123972 1.668247 6.057909e-15 1.304960e-11\nENSMUSG00000072941  3.609170 7.080257 1.807618e-14 3.407135e-11\nENSMUSG00000000120 -6.340146 6.351489 2.507019e-14 4.200371e-11\nENSMUSG00000034981  3.727969 5.244841 3.934957e-14 5.933521e-11\n</code></pre> <pre><code># see how many genes are DE\nsummary(decideTests(dge.f.et , p.value = 0.01)) # let's use 0.01 as a threshold\n</code></pre> <pre><code>         TAM-EtOH \nDown     109\nNotSig 15393\nUp       210\n</code></pre> <p>The comparison is TAM-EtOH, so \u201cUp\u201d, corresponds to a higher in group TAM compared to group EtOH.</p> edgeR looking at differentially-expressed genes <pre><code>## plot all the logFCs versus average count size. Significantly DE genes are  colored\npar(mfrow=c(1,1))\nplotMD(dge.f.et)\n# lines at a log2FC of 1/-1, corresponding to a shift in expression of x2 \nabline(h=c(-1,1), col=\"blue\") </code></pre> <p></p> <p><pre><code>## Volcano plot\nallGenes = topTags(dge.f.et , n = nrow(dge.f.et$table) )$table\n\nFDRthreshold = 0.01\nlogFCthreshold = 1.0\n# add a column of NAs\nallGenes$diffexpressed &lt;- \"NO\"\n# if log2Foldchange &gt; 1 and pvalue &lt; 0.01, set as \"UP\" \nallGenes$diffexpressed[allGenes$logFC &gt; logFCthreshold &amp; allGenes$FDR &lt; FDRthreshold] &lt;- \"UP\"\n# if log2Foldchange &lt; 1 and pvalue &lt; 0.01, set as \"DOWN\"\nallGenes$diffexpressed[allGenes$logFC &lt; -logFCthreshold &amp; allGenes$FDR &lt; FDRthreshold] &lt;- \"DOWN\"\n\nggplot( data = allGenes , aes( x=logFC , y = -log10(FDR) , col =diffexpressed ) ) + geom_point() + geom_vline(xintercept=c(-logFCthreshold, logFCthreshold), col=\"red\") +\ngeom_hline(yintercept=-log10(FDRthreshold), col=\"red\") +\nscale_color_manual(values=c(\"blue\", \"grey\", \"red\"))\n</code></pre> </p> <pre><code>## writing the table of results\nwrite.csv( allGenes , 'Ruhland2016.edgeR.results.csv')\n</code></pre> edgeR extra stuff <pre><code># how to extract log CPM\nlogcpm &lt;- cpm(dge.f, prior.count=2, log=TRUE)\n</code></pre> <pre><code># there is another fitting method reliying on quasi-likelihood, which is useful when the model is more complex (ie. more than 1 factor with 2 levels)\ndge.f.QLfit &lt;- glmQLFit(dge.f, dge.f.design)\ndge.f.qlt &lt;- glmQLFTest(dge.f.QLfit, coef=2)\n\n# you can see the results are relatively different. The order of genes changes a bit, and the p-values are more profoundly affected\ntopTags(dge.f.et)\ntopTags(dge.f.qlt)\n\n## let's see how much the two methods agree:\npar(mfrow=c(1,2))\nplot( dge.f.et$table$logFC , dge.f.qlt$table$logFC,\nxlab = 'exact test logFC',\nylab = 'quasi-likelihood test logFC')\n\nprint( paste('logFC pearson correlation coefficient :' , cor(dge.f.et$table$logFC ,dge.f.qlt$table$logFC) ) )\n\nplot( log10(dge.f.et$table$PValue ), log10(dge.f.qlt$table$PValue) ,\nxlab = 'exact test p-values (log10)',\nylab = 'quasi-likelihood test p-values (log10)')\n\nprint( paste( \"P-values spearman correlation coefficient\",\ncor( log10(dge.f.et$table$PValue ), log10(dge.f.qlt$table$PValue) , method = 'spearman' )))\n</code></pre> <pre><code>\"logFC pearson correlation coefficient : 0.999997655536736\"\n\"P-values spearman correlation coefficient 0.993238670517236\"\n</code></pre> <p></p> <p>The logFC are highly correlated. FDRs show less correlation but their rank are higly correlated : they come in a very similar order.</p>"},{"location":"days/DE/#downstream-analysis-over-representation-analysis","title":"Downstream analysis : over-representation analysis","text":"<p>Having lists of differentially-expressed genes is quite interesting in itself, however when there are many DE genes, it can be interesting to map these results  onto curated sets of genes associated with known biological functions.</p> <p>Here, we propose to use clusterProfiler, which regroups several enrichment detection algorithms onto several databases.</p> <p>We recommend you get inspiration from their very nice vignette/e-book to perform your own analyses.</p> <p>The proposed correction will concern the results obtained with DESeq2 on the Ruhland2016 dataset.</p> analysis with clusterProfiler <p>We begin by reading the results of the DE analysis. Adapt this to your own analysis. Beware that edgeR and DESeq2 use different column names in their result tables (log2FoldChange/logFC , padj/FDR).</p> <pre><code>library(AnnotationHub)\nlibrary(AnnotationDbi)\nlibrary(clusterProfiler)\nlibrary(ReactomePA)\n\nlibrary(org.Mm.eg.db)\n\n\nres = read.csv( 'Ruhland2016.DESeq2.results.csv'  , row.names=1)\n#let's define significance as padj &lt;0.01 &amp; abs(lfc) &gt; 1\nres$sig = abs(res$log2FoldChange)&gt;1 &amp; res$padj&lt;0.01\n\ntable( res$sig )\n</code></pre> <p>Number of non-significant/significant genes </p> <pre><code> FALSE  TRUE \n 18569   401 \n</code></pre> <p>Translating gene ENSEMBL names to their entrezID (this is what clusterProfiler uses), as well as Symbol (named used by most biologist). <pre><code>genes_universe &lt;- bitr(rownames(res), fromType = \"ENSEMBL\",\ntoType = c(\"ENTREZID\", \"SYMBOL\"),\nOrgDb = \"org.Mm.eg.db\")\n\nhead( genes_universe )\n#ENSEMBL ENTREZID  SYMBOL\n#2 ENSMUSG00000033845    27395  Mrpl15\n#4 ENSMUSG00000025903    18777  Lypla1\n#5 ENSMUSG00000033813    21399   Tcea1\n#7 ENSMUSG00000002459    58175   Rgs20\n#8 ENSMUSG00000033793   108664 Atp6v1h\n#9 ENSMUSG00000025907    12421  Rb1cc1\n\ndim(genes_universe)\n# 15878     3\n\nlength(rownames(res))\n# 19378\n</code></pre></p> <pre><code>genes_DE &lt;- bitr(rownames(res)[which( res$sig==T )], fromType = \"ENSEMBL\",\ntoType = c(\"ENTREZID\", \"SYMBOL\"),\nOrgDb = \"org.Mm.eg.db\")\ndim(genes_DE)\n# 387   3\n</code></pre> <p><pre><code># GO \"biological process (BP)\" enrichment\nego_bp &lt;- enrichGO(gene          = as.character(unique(genes_DE$ENTREZID)),\nuniverse      = as.character(unique(genes_universe$ENTREZID)),\nOrgDb         = org.Mm.eg.db,\nont           = \"BP\",\npAdjustMethod = \"BH\",\npvalueCutoff  = 0.01,\nqvalueCutoff  = 0.05,\nreadable      = TRUE)\nhead(ego_bp)\ndotplot(ego_bp, showCategory = 20)\n# sample plot, but with adjusted p-value as x-axis\n#dotplot(ego_bp, x = \"p.adjust\", showCategory = 20)\n</code></pre> </p> <p><pre><code># Reactome pathways enrichment\nreactome.enrich &lt;- enrichPathway(gene=as.character(unique(genes_DE$ENTREZID)),\norganism = \"mouse\",\npAdjustMethod = \"BH\",\nqvalueCutoff = 0.01,\nreadable=T,\nuniverse = genes_universe$ENTREZID)\n\n\ndotplot(reactome.enrich, x = \"p.adjust\")\n</code></pre> </p>"},{"location":"days/DE/#additional-importing-counts-from-salmon-with-tximport","title":"Additional : importing counts from salmon with <code>tximport</code>","text":"<p>The <code>tximport</code> R packages offers a fairly simple set of functions to get transcript-level expression quantification from salmon or kallisto into a differential gene expression analysis.</p> <p>Task : import salmon transcript-level quantification in R in order to perform a DE analysis on it using either edgeR or DESeq2. Additional: compare the results with the ones obtained from STAR-aligned reads.</p> <ul> <li>The tximport vignette is a very good guide for this task.</li> <li>If you have not computed them, you can find files with expression quantifications in : <code>/shared/data/Solutions/Liu2015/</code> and <code>/shared/data/Solutions/Ruhland2016/</code></li> </ul>"},{"location":"days/counting/","title":"Read counting","text":"<p>Read counting refers to the quantification of an \u201cexpression level\u201d, or abundance, from reads mapped onto a reference genome/transcriptome. This expression level can take several forms, such as a count, or a fraction (RPKM/TPM), and concern different entities (exon, transcript, genes) depending on your biological application.</p> <p>During this lesson, you will learn to:</p> <ul> <li>differentiate between different levels of counting and their relevance for different questions.</li> <li>perform read counting at the gene level for Differential Gene expression.</li> </ul>"},{"location":"days/counting/#material","title":"Material","text":"<p> Download the presentation</p> <p>featureCounts website</p>"},{"location":"days/counting/#read-counting-with-featurecounts","title":"Read counting with featureCounts","text":"<p>The featureCount website provides several useful command-line examples to get started. For more details on the algorithm behavior (with multi/overlapping reads for instance), you can refer to the package\u2019s User\u2019s guide (go to the read summarization chapter).</p> <p>Task : </p> <ul> <li>Decide which parameters are appropriate for counting reads from the Ruhland dataset. Assume you are interested in determining which genes are differentially expressed.</li> <li>Count the reads from one of your BAM files using featureCount.</li> <li> <p>How do the featureCount-derived counts compare to those from STAR ?</p> </li> <li> <p>You can find bam files at <code>/shared/data/Solutions/Liu2015/STAR_Liu2015</code> and <code>/shared/data/Solutions/Ruhland2016/STAR_Ruhland2016</code></p> </li> <li>featureCount requirements : 400M RAM / BAM file</li> <li>featureCount requirements : 2 min CPU time / BAM file</li> </ul> featureCounts script <pre><code>#!/usr/bin/bash\n#SBATCH --job-name=featurecount\n#SBATCH --time=00:30:00\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=4G\n#SBATCH -o count.o\n#SBATCH -e count.e\n\n\nG_GTF=/shared/data/DATA/Mus_musculus.GRCm39.105.gtf\n\ninFOLDER=/shared/data/Solutions/Ruhland2016/STAR_Ruhland2016\noutFOLDER=featureCOUNT_Ruhland2016\n\nml subread\n\nmkdir -p $outFOLDER\n\nfeatureCounts -T 8 -a $G_GTF -t exon -g gene_id -o featureCounts_Ruhland2016.counts.txt \\\n                                    $inFOLDER/SRR3180535_EtOH1_1.fastq.gzAligned.sortedByCoord.out.bam \\\n                                    $inFOLDER/SRR3180536_EtOH2_1.fastq.gzAligned.sortedByCoord.out.bam \\\n                                    $inFOLDER/SRR3180537_EtOH3_1.fastq.gzAligned.sortedByCoord.out.bam \\\n                                    $inFOLDER/SRR3180538_TAM1_1.fastq.gzAligned.sortedByCoord.out.bam \\\n                                    $inFOLDER/SRR3180539_TAM2_1.fastq.gzAligned.sortedByCoord.out.bam \\\n                                    $inFOLDER/SRR3180540_TAM3_1.fastq.gzAligned.sortedByCoord.out.bam\n</code></pre> comparison with STAR counts <p>You can use this little R script to check they are the same :</p> <pre><code>fc = read.table( \"featureCounts_SRR3180535_EtOH1_1.counts.txt\" , header =T)\nrownames( fc ) = fc$Geneid\nhead( fc )\n\nstar = read.table( \"SRR3180535_EtOH1_1.fastq.gzReadsPerGene.out.tab\")\nrownames( star ) = star$V1\nhead( star )\n\n\nstar_count = star[ rownames( fc ) , 'V2' ]\nfC_count = fc$STAR_Ruhland2016.SRR3180535_EtOH1_1.fastq.gzAligned.sortedByCoord.out.bam\nplot(log10( star_count + 1),\n    log10(fC_count+1) )\n\nquantile( star_count  - fC_count)\n</code></pre>"},{"location":"days/design/","title":"RNAseq - technologies and design","text":"<p>Designing your experiment is the first step.  Design is crucial as it conditions the sort of questions that you can ask from your data, as well as the confidence you may have in the answers.</p> <p>Knowing about the sequencing technologies, their strengths and limitations, as well as the RNA-seq analysis pipeline, are the keys to design a successful RNA-seq experiment.</p> <p>After having completed this chapter you will be able to:</p> <ul> <li>describe different sequencing technologies and their application in RNA-seq.</li> <li>differentiate between technical and biological replicates.</li> <li>choose an appropriate sequencing depth and number of replicates depending on your scientific question.</li> </ul>"},{"location":"days/design/#material","title":"Material","text":"<p> Download the presentation</p>"},{"location":"days/mapping/","title":"Reads mapping","text":"<p>Once you are happy with your read sequences in your FASTQ files, you can use a mapper software to align the reads to the genome and thereby find where they originated from.</p> <p>At the end of this lesson, you will be able to :</p> <ul> <li>identify the differences between a local aligner and a pseudo aligner.</li> <li>perform genome indexing appropriate to your data.</li> <li>map your RNA-seq data onto the genome.</li> </ul>"},{"location":"days/mapping/#material","title":"Material","text":"<p> Download the presentation</p> <p>STAR website</p>"},{"location":"days/mapping/#building-a-reference-genome-index","title":"Building a reference genome index","text":"<p>Before any mapping can be achieved, you must first index the genome want to map to. </p> <p>To do this with STAR, you need two files:</p> <ul> <li>a fasta file containing the sequences of the chromosome (or genome contigs)</li> <li>a gtf file containing annotations (ie. where the genes and exons are)</li> </ul> <p>We will be using the Ensembl references, with their accompanying GTF annotations.</p> <p>Note</p> <p>While the data are already on the server here, in practice or if you are following this course without a teacher, you can grab the reference genome data from the Ensembl ftp website.</p> <p>In particular, you will want a mouse DNA fasta file and gtf file [release-104 at the time we are linking this. Checking for more recent release is recommended, but may slightly alter the results].</p> <p>Task : Using STAR, build a genome index for chromosome 19 of Mus musculus using the associated GTF</p> <p>Important notes :</p> <ul> <li>the module name for this aligner is <code>star</code>.</li> <li>.fasta and .gtf files are in : <code>/shared/data/DATA/Mouse_chr19/</code>.</li> <li>refer to the manual to determine which options to use.</li> <li>the <code>--genomeDir</code> parameter is the folder where the indexed genome will be output to.</li> <li>this job should require less than 4Gb and 30min to run.</li> </ul> <p>Note</p> <p>While your indexing job is running, you can read ahead in STAR\u2019s manual to prepare the next step : mapping your reads onto the indexed reference genome.</p> STAR indexing script <pre><code>#!/usr/bin/bash\n#SBATCH --job-name=star-build\n#SBATCH --time=00:30:00\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=4G\n#SBATCH -o star-build.o\n#SBATCH -e star-build.e\n\nG_FASTA=/shared/data/DATA/Mouse_chr19/Mus_musculus.GRCm38.dna.chromosome.19.fa\nG_GTF=/shared/data/DATA/Mouse_chr19/Mus_musculus.GRCm38.101.chr19.gtf\n\nml star\n\nmkdir -p STAR_references\n\nSTAR --runMode genomeGenerate \\\n--genomeDir STAR_references \\\n--genomeFastaFiles $G_FASTA \\\n--sjdbGTFfile $G_GTF \\\n--runThreadN 4 \\\n--genomeSAindexNbases 11 \\\n--sjdbOverhang 49 </code></pre> <p>Extra task : Determine how you would add an additional feature to your reference, for example for a novel transcript not described by the standard reference.</p> Answer <p>Edit the gtf file to add your additional feature(s), following the proper format.</p>"},{"location":"days/mapping/#mapping-reads-onto-the-reference","title":"Mapping reads onto the reference","text":"<p>Task : Using STAR, align ONE of the FASTQ files from the Ruhland2016 study against the mouse genome.</p> <ul> <li>Use the full indexed genome at <code>/shared/data/DATA/Mouse_STAR_index/</code>, rather than the one we just made.</li> <li>IMPORTANT: use the following option in your STAR command: <code>--outTmpDir /tmp/${SLURM_JOB_USER}_${SLURM_JOB_ID}/</code>. You can use the manual to look up what this option does. The slurm variables ensure a distinct directory is created in <code>/tmp/</code> for each user and for each job.</li> <li>Generate a BAM file sorted by coordinate.</li> <li>Generate a geneCounts file.</li> <li>Mapping reads and generating a sorted BAM from one of the Ruhland2016 et al. FASTQ files should take about 20 minutes.</li> </ul> <p>Note</p> <p>Take the time to read the parts of the STAR manual which concern you : a bit of planning ahead can save you a lot of time-consuming/headache-inducing trial and error on your script.</p> <p>Warning</p> <p>Remember : request a maximum of 30G and 8 CPUs for 1 hour.</p> STAR mapping script <pre><code>#!/usr/bin/bash\n#SBATCH --job-name=star-aln\n#SBATCH --time=01:00:00\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=30G\n#SBATCH -o star-aln.o\n#SBATCH -e star-aln.e\n\n\nml star\noutDIR=STAR_Ruhland2016\n\nmkdir -p $outDIR\n\ndataDIR=/shared/data/DATA/Ruhland2016\n\ngenomeDIR=/shared/data/DATA/Mouse_STAR_index\n\nSTAR --runThreadN 8 --genomeDir $genomeDIR \\\n              --outSAMtype BAM SortedByCoordinate --outReadsUnmapped Fastx \\\n              --outFileNamePrefix $outDIR/SRR3180535_EtOH1_1 \\\n              --quantMode GeneCounts \\\n              --readFilesIn $dataDIR/SRR3180535_EtOH1_1.fastq.gz --readFilesCommand zcat \\\n</code></pre> <p>The options of STAR are :</p> <ul> <li>\u2013runThreadN 8  : 8 threads to go faster.</li> <li>\u2013genomeDir $genomeDIR : path of the genome to map to.</li> <li>\u2013outSAMtype BAM SortedByCoordinate  : output a coordinate-sorted BAM file.</li> <li>\u2013outReadsUnmapped Fastx : output the non-mapping reads (in case we want to analyse them).</li> <li>\u2013outFileNamePrefix $outDIR/$fastqFILE : prefix of output files.</li> <li>\u2013quantMode GeneCounts : will create a file with counts of reads per gene.</li> <li>\u2013readFilesIn $dataDIR/$fastqFILE  : input read file.</li> <li>\u2013readFilesCommand zcat : command to unzip the input file.</li> </ul> advanced : STAR mapping script with array job <p>The following sets up an array of tasks to align all samples.</p> <p>Source file : <code>Ruhland2016.fastqFiles.txt</code> :</p> <pre><code>SRR3180535_EtOH1_1.fastq.gz\nSRR3180536_EtOH2_1.fastq.gz\nSRR3180537_EtOH3_1.fastq.gz\nSRR3180538_TAM1_1.fastq.gz\nSRR3180539_TAM2_1.fastq.gz\nSRR3180540_TAM3_1.fastq.gz\n</code></pre> <p>sbatch script :</p> <pre><code>#!/usr/bin/bash\n#SBATCH --job-name=star-aln\n#SBATCH --time=01:00:00\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=30G\n#SBATCH -o star-aln.%a.o\n#SBATCH -e star-aln.%a.e\n#SBATCH --array 1-1%1\n\n\nml star\noutDIR=STAR_Ruhland2016\n\nmkdir -p $outDIR\n\ndataDIR=/shared/data/DATA/Ruhland2016\n\nsourceFILE=Ruhland2016.fastqFiles.txt\n\nfastqFILE=`sed -n ${SLURM_ARRAY_TASK_ID}p $sourceFILE`\n\ngenomeDIR=/shared/data/DATA/Mouse_STAR_index\n\nSTAR --runThreadN 8 --genomeDir $genomeDIR \\\n              --outSAMtype BAM SortedByCoordinate --outReadsUnmapped Fastx \\\n              --outFileNamePrefix $outDIR/$fastqFILE \\\n              --quantMode GeneCounts \\\n              --readFilesIn $dataDIR/$fastqFILE --readFilesCommand zcat \\\n</code></pre> <p>The options of STAR are :</p> <ul> <li>\u2013runThreadN 8  : 8 threads to go faster.</li> <li>\u2013genomeDir $genomeDIR : path of the genome to map to.</li> <li>\u2013outSAMtype BAM SortedByCoordinate  : output a coordinate-sorted BAM file.</li> <li>\u2013outReadsUnmapped Fastx : output the non-mapping reads (in case we want to analyse them).</li> <li>\u2013outFileNamePrefix $outDIR/$fastqFILE : prefix of output files.</li> <li>\u2013quantMode GeneCounts : will create a file with counts of reads per gene.</li> <li>\u2013readFilesIn $dataDIR/$fastqFILE  : input read file.</li> <li>\u2013readFilesCommand zcat : command to unzip the input file.</li> </ul>"},{"location":"days/mapping/#qc-on-the-aligned-reads","title":"QC on the aligned reads","text":"<p>You can call MultiQC on the STAR output folder to gather a report on the individual alignments.</p> <p>Here we\u2019ve aligned a single sample, but usually this would cover all your samples.</p> <p>Task : use <code>multiqc</code> to generate a QC report on the results of your mapping.</p> <ul> <li>Evaluate the alignment statistics. Do you consider this to be a good alignment?</li> <li>How many unmapped reads are there? Where might this come from, and how would you determine this?</li> <li>What could you say about library strandedness ? </li> </ul> script and answers <pre><code>#!/usr/bin/bash\n#SBATCH --job-name=multiqc\n#SBATCH --time=00:30:00\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=1G\n#SBATCH -o multiqc_star_Ruhland2016.o\n#SBATCH -e multiqc_star_Ruhland2016.e\n\n\nmkdir -p STAR_MULTIQC_Ruhland2016/\n\nmultiqc -o STAR_MULTIQC_Ruhland2016/ STAR_Ruhland2016/\n</code></pre> <p>Result : </p> <p> Download the report </p>"},{"location":"days/mapping/#additionnal-star-2-pass","title":"ADDITIONNAL : STAR 2-Pass","text":"<p>Genome annotations are incomplete, particularly for complex eukaryotes : there are many as-of-yet unannotated splice junctions.</p> <p>The first pass of STAR can create a splice junction database, containing both known and novel junctions. This splice junction database can, in turn, be used to guide an improved second round of alignment, using a command like:</p> <pre><code>STAR &lt;1st round options&gt; --sjdbFileChrStartEnd sample_SJ.out.tab\n</code></pre> <p>Task : run STAR in this STAR-2pass mode on the same sample as before and evaluate the results.</p> script <pre><code>#!/usr/bin/bash\n#SBATCH --job-name=star-aln2\n#SBATCH --time=01:00:00\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=30G\n#SBATCH -o star-aln-2pass.%a.o\n#SBATCH -e star-aln-2pass.%a.e\n#SBATCH --array 1-1%1\n\nml star\n\noutDIR=STAR_Ruhland2016\n\nmkdir -p $outDIR\n\ndataDIR=/shared/data/DATA/Ruhland2016\n\nsourceFILE=Ruhland2016.fastqFiles.txt\n\nfastqFILE=`sed -n ${SLURM_ARRAY_TASK_ID}p $sourceFILE`\n\ngenomeDIR=/shared/data/DATA/Mouse_STAR_index\n\nSTAR --runThreadN 8 --genomeDir $genomeDIR \\\n                  --outSAMtype BAM SortedByCoordinate \\\n                  --outFileNamePrefix $outDIR/$fastqFILE.2Pass. \\\n                  --outReadsUnmapped Fastx --quantMode GeneCounts \\\n                  --sjdbFileChrStartEnd $outDIR/${fastqFILE}SJ.out.tab \\\n                  --readFilesIn $dataDIR/$fastqFILE --readFilesCommand zcat \\\n</code></pre>"},{"location":"days/mapping/#additional-pseudo-aligning-with-salmon","title":"ADDITIONAL : pseudo-aligning with salmon","text":"<p>salmon website</p> <p>salmon can allow you to quantify transcript expression without explicitly aligning the sequenced reads onto the reference genome with its gene and splice junction annotations, but to a simplification of the corresponding transcriptome, thus saving computational resources.</p> <p>We refer you to the tool\u2019s documentation in order to see how the reference index is computed.</p> <p>Task : run salmon to quantify the expression of either the Ruhland or Liu dataset. </p> <ul> <li>Use the tool documentation to craft your command line.</li> <li>precomputed indices can be found in <code>/shared/data/Mouse_salmon_index</code> and <code>/shared/data/Human_salmon_index</code>.</li> </ul> script <pre><code>#!/usr/bin/bash\n#SBATCH --job-name=salmonRuhland\n#SBATCH --time=01:00:00\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=30G\n#SBATCH -o salmon_ruhland2016.%a.o\n#SBATCH -e salmon_ruhland2016.%a.e\n#SBATCH --array 1-6%1\n\nml salmon\n\noutDIR=salmon_Ruhland2016\n\nmkdir -p $outDIR\n\ndataDIR=/shared/data/DATA/Ruhland2016\n\nsourceFILE=Ruhland2016.fastqFiles.txt\n\nfastqFILE=`sed -n ${SLURM_ARRAY_TASK_ID}p $sourceFILE`\n\ngenomeDIR=/shared/data/DATA/Mouse_salmon_index\n\nsalmon quant -i $genomeDIR -l A \\\n            -r $dataDIR/$fastqFILE \\\n            -p 8 --validateMappings --gcBias --seqBias \\\n            -o $outDIR\n</code></pre>"},{"location":"days/quality_control/","title":"Quality control","text":"<p>Quality Control is the essential first step to perform once you receive your data from your sequencing facility, typically as <code>.fastq</code> or <code>.fastq.gz</code> files.</p> <p>During this session, you will learn to :</p> <ul> <li>create QC report for a single file with fastqc</li> <li>aggregate multiple QC reports using multiqc</li> <li>interpret the QC reports for an entire RNA-seq experiment</li> </ul> <p>Note</p> <p>Although we aim to present tools as stable as possible, software evolve and their precise interface can change with time. We strongly recommend you consult each command\u2019s help page or manual before launching them. To this end, we provide links to each tool\u2019s website. </p> <p>This can also be useful to you if you are following this course without access to a compute cluster and have to install these tools on your machine.</p>"},{"location":"days/quality_control/#material","title":"Material","text":"<p> Download the presentation</p> <p>FastQC website</p> <p>MultiQC website</p>"},{"location":"days/quality_control/#meet-the-datasets","title":"Meet the datasets","text":"<p>We will be working with two datasets from the following studies:</p> <ul> <li> <p>Liu et al. (2015) \u201cRNA-Seq identifies novel myocardial gene expression signatures of heart failure\u201d Genomics 105(2):83-89 https://doi.org/10.1016/j.ygeno.2014.12.002</p> <ul> <li>GSE57345</li> <li>Samples of Homo sapiens heart left ventricles : 3 with heart failure, 3 without</li> <li>6 samples of paired-end reads</li> </ul> </li> <li> <p>Ruhland et al. (2016) \u201cStromal senescence establishes an immunosuppressive microenvironment that drives tumorigenesis\u201d Nature Communications 7:11762 https://dx.doi.org/10.1038/ncomms11762</p> <ul> <li>GSE78128</li> <li>Samples of Mus musculus skin fibroblasts : 3 non-senescent (EtOH), 3 senescent (TAM)</li> <li>6 samples of single-end reads</li> </ul> </li> </ul>"},{"location":"days/quality_control/#retrieving-published-datasets","title":"Retrieving published datasets","text":"<p>Note</p> <p>If you are following this course with a teacher, then the for the data is already on the server. There is no need to download it again.</p> <p>Most NGS data is deposited at the Short Read Archive (SRA) hosted by the NCBI, with links from the Gene Expression Omnibus (GEO)</p> <p>Several steps are required to retrieve data from a published study :</p> <ol> <li>find GEO or SRA identifier from publication.</li> <li>find the \u201crun\u201d identifiers for each sample (SRR).</li> <li>use SRA Toolkit to dump data from the SRR repository to FASTQ files.</li> </ol> <p>For example, on the Liu2015 dataset :</p> <ol> <li>locate GEO ID</li> <li>look it up on GEO website: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE57345 </li> <li>Click Run Selector down the bottom.</li> <li>Copy/paste the SRR IDs from the table, or use the download accession list button to get a file listing them. </li> <li>From the results of your search, select all relevant runs</li> <li>Click on \u201cAccession List\u201d in the Select table </li> </ol> <p></p> <ol> <li>use <code>fastq-dump</code> (part of the SRA Toolkit) on the downloaded accession list. For example:</li> </ol> <p><code>fastq-dump --gzip --skip-technical --readids --split-files --clip SRR1272191</code></p> <p>Note</p> <ul> <li>You\u2019ll need to know the nature of the dataset (library type, paired vs single end, etc.) before analysing it.</li> <li><code>fastq-dump</code> takes a very long time</li> <li>More information about fastq-dump</li> </ul>"},{"location":"days/quality_control/#fastqc-a-report-for-a-single-fastq-file","title":"FastQC : a report for a single fastq file","text":"<p>FastQC is a nice tool to get a variety of QC measures from files such as <code>.fastq</code>, <code>.bam</code> or <code>.sam</code> files. </p> <p>Although it has many options, the default parameters are often enough for our purpose :</p> <pre><code>fastqc -o &lt;output_directory&gt; file1.fastq file2.fastq ... fileN.fastq\n</code></pre> <p>FastQC is reasonably intelligent, and will try to recognise the file format and uncompress it if necessary (so no need to decompress manually).</p> <p>Task: </p> <ul> <li>Write one or more slurm-compatible sbatch scripts in your home directory that run FastQC analysis on each FASTQ file from the two datasets. These are accessible at : <code>/shared/data/DATA/Liu2015/</code> and <code>/shared/data/DATA/Ruhland2016</code>. </li> <li>Look at at least one of the QC report. What are your conclusions ? Would you want to perform some operations on the reads such as low-quality bases trimming, removal of adapters ?</li> </ul> <p>Warning</p> <p>Make sure your script writes the fastqc output to a folder within your own home directory.</p> <p>Important points:</p> <ul> <li>in your script, don\u2019t forget to load fastqc : <code>ml fastqc</code>.</li> <li>there is no need to copy the read files to your home directory (in fact, it is good practice not to: it would create data redundancy, and we won\u2019t have enough space left on the disk anyway\u2026).</li> <li>FastQC RAM requirements : 1Gb is more than enough.</li> <li>FastQC time requirements : ~ 5min / read file.</li> <li>try to make sure FastQC outputs all reports in the same directory, this will save time for the next step ;-).</li> </ul> <p>Note</p> <p>Reminder : to get the data from the distant server to your machine, you may use an SFTP client (filezilla, mobaXterm), or the command line tool from your machine : <code>scp login@xx.xx.xx:~/path/to/file.txt .</code></p> Liu2015 FastQC sbatch script <p>Here is an sbatch script for one sample:</p> <pre><code>#!/usr/bin/bash\n#SBATCH --job-name=fastqc\n#SBATCH --time=01:00:00\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=1G\n#SBATCH -o fastqc_Liu2015.o\n#SBATCH -e fastqc_Liu2015.e\n\ndataDir=/shared/data/DATA/Liu2015\n\nml fastqc\n\n# creating the output folder\nmkdir -p FASTQC_Liu2015/\n\nfastqc -o FASTQC_Liu2015/ $dataDir/SRR1272187_1.fastq.gz\n</code></pre> <p>You could either have:</p> <ul> <li>one sbatch script per sample (recommended),</li> <li>OR put the <code>fastqc</code> commands for all the samples in the same script (not recommended).</li> </ul> <p>The first is recommended because you can submit both scripts at once and they will then run in parallel, whereas with the second option the samples would be handled sequentially and the overall job would take much longer to finish.</p> Ruhland2016 FastQC sbatch script <p>Here is an sbatch script for one sample:</p> <pre><code>#!/usr/bin/bash\n#SBATCH --job-name=fastqc\n#SBATCH --time=01:00:00\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=1G\n#SBATCH -o fastqc_Ruhland2016.o\n#SBATCH -e fastqc_Ruhland2016.e\n\ndataDir=/shared/data/DATA/Ruhland2016\n\nml fastqc\n\n# creating the output folder\nmkdir -p FASTQC_Ruhland2016/\n\nfastqc -o FASTQC_Ruhland2016/ $dataDir/SRR3180540_TAM3_1.fastq.gz\n</code></pre> <p>You could either have:</p> <ul> <li>one sbatch script per sample (recommended),</li> <li>OR put the <code>fastqc</code> commands for all the samples in the same script (not recommended).</li> </ul> <p>The first is recommended because you can submit both scripts at once and they will then run in parallel, whereas with the second option the samples would be handled sequentially and the overall job would take much longer to finish.</p> Alternative sbatch script using array job <p>Here is a solution where all files from a same dataset can be processed in parallel (recommended) by using slurm array jobs.</p> <p>First, have a file named <code>Ruhland2016.fastqFiles.txt</code> containing the sample fastq file names :</p> <pre><code>SRR3180535_EtOH1_1.fastq.gz\nSRR3180536_EtOH2_1.fastq.gz\nSRR3180537_EtOH3_1.fastq.gz\nSRR3180538_TAM1_1.fastq.gz\nSRR3180539_TAM2_1.fastq.gz\nSRR3180540_TAM3_1.fastq.gz\n</code></pre> <p>Then, in the same folder, you can create this sbatch script :</p> <pre><code>#!/usr/bin/bash\n#SBATCH --job-name=fastqc\n#SBATCH --time=00:30:00\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=1G\n#SBATCH -o fastqc_Ruhland2016.%a.o\n#SBATCH -e fastqc_Ruhland2016.%a.e\n#SBATCH --array 1-6%6\n\nml fastqc\n\ndataDir=/shared/data/DATA/Ruhland2016\n\nsourceFILE=Ruhland2016.fastqFiles.txt\n\n## retrieving 1 filename from Ruhland2016.fastqFiles.txt\nfastqFILE=`sed -n ${SLURM_ARRAY_TASK_ID}p $sourceFILE`\n\nmkdir -p FASTQC_Ruhland2016/\nfastqc -o FASTQC_Ruhland2016/ $dataDir/$fastqFILE\n</code></pre> <p>When submitted with <code>sbatch</code>, this script will spawn 6 tasks in parallel, each with a different value of <code>${SLURM_ARRAY_TASK_ID}</code>.</p> <p>This is the recommended option : this allows you to launch all your job in parallel with a single script.</p> Interpretation of a report <p> Download an annotated report</p>"},{"location":"days/quality_control/#multiqc-grouping-multiple-reports","title":"MultiQC : grouping multiple reports","text":"<p>In practice, you  likely will have more than a couple of samples (maybe even more than 30 or 50\u2026) to handle: individually consulting and comparing the QC reports of each would be tedious.</p> <p>MultiQC is a tool that lets you combine multiple reports in a single, interactive document that let you explore your data easily.</p> <p>Here, we will be focusing on grouping FastQC reports, but MultiQC can also be applied to the output or logs of other bioinformatics tools, such as mappers, as we will see later.</p> <p>In its default usage, <code>multiqc</code> only needs to be provided a path where it will find all the individual reports, and it will scan them and write a report named <code>multiqc_report.html</code>.</p> <p>Although the default behaviour is quite appropriate, with a couple of options we get a slightly better control over the output:  * <code>--interactive</code> : forces the plot to be interactive even when there is a lot of samples (this option can lead to larger html files).  * <code>-f &lt;filename&gt;</code> : specify the name of the output file name.</p> <p>For instance, a possible command line could be : <pre><code>multiqc -f multiQCreports/Liu2015_multiqc.html --interactive Liu2015_fastqc/\n</code></pre></p> <p>There are many additional parameters which let you customize your report. Use <code>multiqc --help</code> or visit their documentation webpage to learn more.</p> <p>Task: </p> <ul> <li>Write an sbatch script to run MultiQC for each dataset. </li> <li>Look at the QC reports. What are your conclusions ? </li> </ul> <p>Important points:</p> <ul> <li>MultiQC RAM requirements : 1Gb should be more than enough.</li> <li>MultiQC time requirements : ~ 1min / read file.</li> <li>Exceptionally, there is no need to load multiqc as a module (it is not part of ComputeCanada and we installed it directly on the cluster, on other clusters it may not be the same).</li> <li>Use <code>multiqc --help</code> to check the different options</li> </ul> MultiQC sbatch script <p>This is the script for the Ruhland2016 dataset. It presumes that the fastqc reports can be found in <code>FASTQC_Ruhland2016/</code></p> <pre><code>#!/usr/bin/bash\n#SBATCH --job-name=multiqc\n#SBATCH --time=00:30:00\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=1G\n#SBATCH -o multiqc_Ruhland2016.o\n#SBATCH -e multiqc_Ruhland2016.e\n\n\nmkdir -p MULTIQC_Ruhland2016/\n\nmultiqc -o MULTIQC_Ruhland2016/ FASTQC_Ruhland2016/ </code></pre> Interpretation of a report <p>We will interpret the report for the Liu2015 data.</p> <p></p> <p>The PHRED quality of reads drop below 30 around base 75. All samples seem affected. One sample seems to have some quality drops at specific timepoints/positions.</p> <p></p> <p>Mean quality scores are on average fairly high, but some reads exhibit low values.</p> <p></p> <p>Most samples do not deviate too much from the expected curve. The two samples colored in orange and red have a mode for a very specific value. This may be indicative of contamination, retaining specific rRNA, or adapter sequence content.</p> <p></p> <p>Ns are present at specific positions in specific samples, in particular for one sample. This is reminiscent of the PHRED quality curves at the top of the report. It seems some flowcells had a problem at specific time-point/positions.</p> <p></p> <p>This is colored red because this would be a problem if the data was coming from genomic DNA sequencing. However here we are in the context of RNA-seq : some transcripts are present in a large number of copies in the samples, and consequently it is expected that some sequences are over-represented.</p> <p></p> <p>We see a clear trend of adapter contamination as we get closer to the reads\u2019 end. Note the y-scale though : we never go above a 6% content per sample.</p> <p>Overall, we can conclude that these samples all suffer from some adapter content and a lower quality toward the reads\u2019 second half. Furthermore, a few samples have a peculiar N pattern between bases 20 and 30.</p> <p>It is then strongly advised to either :</p> <ul> <li>perform some trimming : remove adapter sequences + cut reads when average quality becomes too low</li> <li>use a mapper that takes base quality in account AND is able to ignore adapter sequence (and even then, you could try mapping on both trimmed and untrimmed data to see which is the best)</li> </ul>"},{"location":"days/server_login/","title":"Server login + unix fresh up","text":"<p>To conduct the practicals of this course, we will be using a dedicated High Performance Computing cluster.  This matches the reality of most NGS workflows, which cannot be completed in a reasonable time on a single machine. </p> <p>To interact with this cluster, you will have to log in to a distant head node. From there you will be able to distribute your computational tasks to the cluster using a job scheduler called Slurm.</p> <p>This page will cover our first contact with the distant cluster. </p> <p>You will learn to :</p> <ul> <li>understand a typical computer cluster architecture.</li> <li>connect to the server.</li> <li>use the command line to perform basic operations on the head node.</li> <li>exchange files between the server and your own machine.</li> <li>submit a job to the cluster.</li> </ul> <p>Note</p> <p>If you are doing this course on your own, then the distant server provided within the course will not be available.  Feel free to ignore or adapt any of the following steps to your own situation.</p>"},{"location":"days/server_login/#the-computing-cluster","title":"The computing cluster","text":"<p>The computing cluster follows an architecture that enables several users to distribute computational tasks among several machines which share a number of resources, such as a common file system.</p> <p></p> <p>Users do not access each machine individually, but rather connect to a head node. From there, they can interact with the cluster using the job scheduler (here slurm). The job scheduler\u2019s role is to manage where and how to run the jobs of all users, such that waiting time is minimized and resource usage is optimized.</p> <p>Warning</p> <p>Everyone is connected to the same head node. Do not perform compute-intensive tasks on it or you will slow everyone down! </p>"},{"location":"days/server_login/#connect-to-the-server","title":"Connect to the server","text":"<p>Say you want to connect to cluster with address <code>xx.xx.xx.xx</code> and your login is <code>login</code>.</p> <p>Warning</p> <p>If you are doing this course with a teacher, use the link, login and password provided before or during the course. </p> <p>The first step will be to open a terminal</p> Mac <p>Open a terminal, for instance with the application Xterm, or Xquartz. </p> Linux <p>Open a new terminal.</p> Windows <p>Open the application mobaXterm (or any ssh-enabling terminal aplpication you prefer).</p> <p>On mobaXterm, click on \u201cStart a local Terminal\u201d.</p> <p>In the terminal type the following command:</p> <pre><code>ssh login@xx.xx.xx.xx\n</code></pre> <p>When prompted for your password, type it and press Enter. </p> <p>Note</p> <p>There is no cursor or \u2018\u25cf\u2019 character appearing while you type your password. This is normal.</p> <p>After a few seconds, you should be logged into the head node and ready to begin.</p>"},{"location":"days/server_login/#using-command-line-on-the-cluster","title":"Using command line on the cluster","text":"<p>Now that you are in the head node, it is time to get acquainted with your environment and to prepare the upcoming practicals.  We will also use this as a short reminder about the UNIX command line.</p> <p>You can also refer to this nice Linux Command Line Cheat Sheet.</p> <p>At any time, you can get the location (folder) your terminal is in at by typing the \u201cprint working directory\u201d command:</p> <pre><code>pwd\n</code></pre> <p>When you start a session on a distant computer, you are placed in your <code>home</code> directory. So the cluster should return something like:</p> <pre><code>/shared/home/&lt;login&gt;\n</code></pre>"},{"location":"days/server_login/#creating-a-directory","title":"Creating a directory","text":"<p>Use the command line to create a repository called <code>day1</code> where you will put all materials relating to this first day.</p> Answer <pre><code>mkdir day1\n</code></pre> <p>Move to that directory.</p> Answer <pre><code>cd day1\n</code></pre> <p>The directory <code>/shared/data/</code> contains data and solutions for most practicals. Check the content of that directory.</p> Answer <pre><code>ls /shared/data/\n</code></pre> <p>Note</p> <p>You don\u2019t need to move to that directory to list its contents!</p> <p>Copy the script <code>fastqc_Liu2015_SRR1272187_1.sh</code> from  <code>/shared/data/Solutions/Liu2015</code> into your current directory.</p> Answer <pre><code>cp /shared/data/Solutions/Liu2015/fastqc_Liu2015_SRR1272187_1.sh .\n</code></pre> <p>Print the content of this script to the screen.</p> Answer <p><pre><code>more fastqc_Liu2015_SRR1272187_1.sh\n</code></pre> output: <pre><code>#!/usr/bin/bash\n#SBATCH --job-name=fastqc\n#SBATCH --time=00:30:00\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=1G\n#SBATCH -o fastqc_Liu2015.o\n#SBATCH -e fastqc_Liu2015.e\n\nml fastqc\n\ndataDir=/shared/data/DATA/Liu2015\n\nmkdir -p FASTQC_Liu2015/\nfastqc -o FASTQC_Liu2015/ $dataDir/SRR1272187_1.fastq.gz\n</code></pre></p> <p>We\u2019ll see what all this means soon.</p>"},{"location":"days/server_login/#creating-and-editing-a-file","title":"Creating and editing a file","text":"<p>To edit files on the distant server, we will use the command line editor <code>nano</code>. It is far from the most complete or efficient one, but it can be found on most servers and is arguably among the easiest to start with.</p> <p>Note</p> <p>Alternatively, feel free to use any other CLI editor you prefer, such as <code>vi</code>.</p> <p>To start editing a file named <code>test.txt</code>, type :</p> <pre><code>    nano test.txt\n</code></pre> <p>You will be taken to the <code>nano</code> interface :</p> <p></p> <p>Type in your favorite movie quote, and then exit by pressing <code>Ctrl+x</code> (<code>control+x</code> on a Mac keyboard), and then <code>y</code> and <code>Enter</code> when prompted to save the modifications you just made.</p> <p>You can check that your modifications were saved by typing</p> <pre><code>more test.txt\n</code></pre>"},{"location":"days/server_login/#exchanging-files-with-the-server","title":"Exchanging files with the server","text":"<p>Whether you want to transfer some data to the cluster or retrieve the results of your latest computation, it is important to be able to exchange files with the distant server.</p> <p>There exists several alternatives, depending on your platform and preferences.</p> command line <p>We will use <code>scp</code>.</p> <p>To copy a file from the server to your machine, use this syntax on a terminal in your local machine (open a new terminal if necessary).</p> <pre><code>scp &lt;login&gt;@&lt;server-adress&gt;:/path/to/file/on/server/file.txt /local/destination/\n</code></pre> <p>For example, to copy the file <code>test.txt</code> you just created in the folder <code>day1/</code>, to your current (local) working directory :</p> <pre><code>scp login@xx.xx.xx.xx:~/day1/file.txt .\n</code></pre> <p>To copy a file from your machine to the server (NB: here <code>~</code> will be interpreted as your home directory, this is a useful and time-saving shorthand):</p> <pre><code>scp /path/to/file/local/file.txt &lt;login&gt;@&lt;server-adress&gt;:/destination/on/server/\n</code></pre> graphical alternative <p>There are nice and free software with graphical user interfaces, such as filezilla, to help you manage exchanges with the distant server. Feel free to install and experiment with it during the break. </p> mobaXterm <p>If you are using mobaXterm, the left panel should provide a graphical SFTP browser in the left sidebar which allows you to browse and drag and drop files directly from/to the remote server.</p> <p></p>"},{"location":"days/server_login/#submitting-jobs","title":"Submitting jobs","text":"<p>Jobs can be submitted to the compute cluster using sbatch scripts, which contain 2 parts :</p> <ul> <li> <p>information for the job scheduler:</p> <ul> <li>how much RAM / CPUs do I need ?</li> <li>where to write the logs of my job ?</li> </ul> </li> <li> <p>bash commands corresponding to your task</p> </li> </ul> <p>But an example is worth a thousand words :</p> <pre><code>#!/usr/bin/bash\n#SBATCH --job-name=test\n#SBATCH --time=00:30:00\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=1G\n#SBATCH -o test_log.o\n\n\necho \"looking at the size of the elements of /shared/data/\"\nsleep 10 # making the script wait for 10 seconds - this is just so we can see it later on. \n# `du` is \"disk usage\", a command that returns the size of a folder structure.\ndu -h -d 2 /shared/data/\n</code></pre> <p>The lines beginning by <code>#SBATCH</code> specify options to the job scheduler:</p> <ul> <li>The first line indicates what shell should be used to interpret the commands.</li> <li><code>#SBATCH --job-name=test</code> : the job name</li> <li><code>#SBATCH --time=00:30:00</code> : time reserved for the job : 30min. </li> <li><code>#SBATCH --cpus-per-task=1</code> : CPUs for the job </li> <li><code>#SBATCH --mem=1G</code> : memory for the job</li> <li><code>#SBATCH -o test_log.o</code> : file to write output or error messages</li> </ul> <p>Warning</p> <p>Your job will fail as soon as it takes more time or RAM than requested. You might need to test it to find the appropriate values.</p> <p>Create a new file named <code>mySbatchScript.sh</code>, copy the code above into it, save, then submit this file to the job scheduler using the following command :</p> <pre><code>    sbatch mySbatchScript.sh\n</code></pre> <p>Afterward, use the command <code>squeue</code> to monitor the jobs submitted to the cluster. Locate your job and wait for it to be accepted (<code>RUNNING</code> status), and then to complete (the job disappears from the output of <code>squeue</code>). </p> <p>Check the output of your job in the output file.</p> <p>Note</p> <p>When there are a lot of jobs, <code>squeue -u &lt;username&gt;</code> will limit the list to those of the specified user.</p>"},{"location":"days/server_login/#advanced-cluster-usage-loading-modules","title":"Advanced cluster usage : loading modules","text":"<p>During our various analysis, we will call upon numerous software.</p> <p>Fortunately, in most cases we do not have to install each of these ourselves onto the cluster : they have already been packaged, prepared and made available to you or your code. </p> <p>However, by default, these are not loaded, and you have to explicitly load the module containing the software you want in your script (or in the interactive shell session).</p> <p>Question: Why aren\u2019t all the modules already pre-loaded ?</p> Answer <p>Many toolsets have dependencies toward different, sometimes incompatible libraries. Packaging each tool independently and loading them separately circumvents this as you only load what you need, and you can always unload a toolset if you need to load another, incompatible, toolset.</p> <p>Modules are managed with the <code>module</code> command.</p> <p>Basic commands are :</p> <ul> <li><code>module list</code> : lists currently loaded modules</li> <li><code>module load &lt;modulename&gt;</code> alias <code>ml &lt;modulename&gt;</code> : loads module <code>&lt;modulename&gt;</code></li> <li><code>module unload &lt;modulename&gt;</code>  : unloads module <code>&lt;modulename&gt;</code></li> <li><code>module purge</code> : unloads all loaded modules</li> <li><code>module avail</code> : lists all modules available for loading</li> <li><code>module keyword &lt;KW&gt;</code> : lists all modules available for loading which contains <code>&lt;KW&gt;</code></li> </ul> <p>Try it for yourself: soon, we will need the fastqc software.  If we type in the terminal: <pre><code>fastqc\n</code></pre> this should give you an error, telling you there is no such command.</p> <p>Now, if we load it first <pre><code>ml fastqc # shortcut for \"module load fastqc\"\nfastqc\n</code></pre> Now you should not have any error.</p> <p>Note: our module provider is ComputeCanada, which has a lot of available software. To avoid storing all these on our cluster, each time a new module is loaded, it is fetched first on the Compute Canada servers, so sometimes it can take a bit of time to load a module for the first time.</p>"},{"location":"days/server_login/#advanced-cluster-usage-job-array","title":"Advanced cluster usage : job array","text":"<p>Often, we have to repeat a similar analysis on a number of files, or for a number of different parameters. Rather than writing each sbatch script individually, we can rely on job arrays to facilitate our task.</p> <p>Say you want to execute a command, on 10 files (for example, map the reads of 10 samples).</p> <p>You first create a file containing the name of your files (one per line); let\u2019s call it <code>readFiles.txt</code>.</p> <p>Then, you write an sbatch array job script:</p> <pre><code>#!/usr/bin/bash\n#SBATCH --job-name=test_array\n#SBATCH --time=00:30:00\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=1G\n#SBATCH -o test_array_log.%a.o\n#SBATCH --array 1-10%5\n\n\necho \"job array id\" $SLURM_ARRAY_TASK_ID\n\n# sed -n &lt;X&gt;p &lt;file&gt; : retrieve line &lt;X&gt; of file\n# so the next line grabs the file name corresponding to our job array task id and stores it in the variable ReadFileName \nReadFileName=`sed -n ${SLURM_ARRAY_TASK_ID}p readFiles.txt`\n\n# here we would put the mapping command or whatever\necho $ReadFileName\n</code></pre> <p>Some things have changed compared to the previous sbatch script :</p> <ul> <li><code>#SBATCH --array 1-10%5</code> : will spawn independent tasks with ids from 1 to 10, and will manage them so that at most 5 run at the same time.</li> <li><code>#SBATCH -o test_array_log.%a.o</code> : the <code>%a</code> will take the value of the array task id. So we will have 1 log file per task (so 10 files).</li> <li><code>$SLURM_ARRAY_TASK_ID</code> : changes value between the different tasks. This is what we use to execute the same script on different files (using <code>sed -n ${SLURM_ARRAY_TASK_ID}p</code>)</li> </ul>"},{"location":"days/trimming/","title":"Sequence trimming","text":"<p>Following a QC analysis on sequencing results, one may detect stretches of low quality bases along reads, or a contamination by adapter sequence. Depending on your research question and the software you use for mapping, you may have to remove these bad quality / spurious sequences out of your data.</p> <p>During this block, you will learn to :</p> <ul> <li>trim your data with trimmomatic</li> </ul>"},{"location":"days/trimming/#material","title":"Material","text":"<p> Download the presentation</p> <p>Trimmomatic website</p>"},{"location":"days/trimming/#to-trim-or-not-to-trim","title":"to trim or not to trim ?","text":"<p>There are several ways to deal with poor quality bases or adapter contamination in reads, and several terms are used in the field, sometimes very loosely. We can talk about:</p> <ul> <li>Trimming : to remove a part of, or the entirety of, a read (for quality reasons).</li> <li>Hard trimming : trim with a high standard of quality (eg. remove everything with QUAL&lt;30).</li> <li>Soft trimming: trim with a low standard of quality (eg. remove everything with QUAL&lt;10).</li> <li>Clipping : to remove the end part of a read (typically because of adapter content).</li> <li>Hard clipping: actually removing the end of the read from the file (ie. with trimmomatic).</li> <li>Soft clipping: ignoring the end of the read at mapping time (ie. what STAR does).</li> </ul> <p>If the data will be used to perform transcriptome assembly, or variant analysis, then it must be trimmed.</p> <p>In contrast, for applications based on counting reads, such as Differential Expression analysis, most aligners, such as STAR, HISAT2, salmon, and kallisto, can handle bad quality sequences and adapter content by soft-clipping, and consequently they usually do not need trimming. In fact, trimming can be detrimental to the number of successfully quantified reads [William et al. 2016].</p> <p>Nevertheless, it is usually recommended to perform some amount of soft clipping (eg. kallisto, salmon ).</p> <p>If possible, we recommend to perform the mapping for both the raw data and the trimmed one, in order to compare the results for both, and choose the best.</p> <p>Question: what could be a good metric to choose the best between the trimmed and untrimmed ?</p> Answer <p>The number of uniquely mapped reads is generally what would matter in differential expression analysis. Of course, this means that you can only choose after you have mapped both the trimmed and the untrimmed reads.</p>"},{"location":"days/trimming/#trimming-with-trimmomatic","title":"trimming with Trimmomatic","text":"<p>The trimmomatic website gives very good examples of their software usage for both paired-end (<code>PE</code>) and single-end (<code>SE</code>) reads. We recommend you read their quick-start section attentively.</p> <p>Task : </p> <p>Conduct a soft trimming on the Liu2015 data.</p> <p>Extra (if you have the time) : run a QC analysis on your trimmmed reads and compare with the raw ones.</p> <p>Important notes :</p> <ul> <li>when you do <code>ml trimmomatic</code>, you will receive a little message which tells you how to launch the software.</li> <li>Adapter sequences can be found in <code>/shared/data/DATA/adapters/</code>.</li> <li>Trimmomatic RAM requirements : ~0.5G / CPU.</li> <li>Trimmomatic time requirements : ~ 10 min/ read file .</li> </ul> Trimmomatic script <p>The Liu2015 dataset has paired-end reads and we have to take that into account during trimming. For a soft-trimming, we chose the following options :</p> <ul> <li>SLIDINGWINDOW:4:20 Perform a sliding window trimming, cutting once the average quality within the window falls below a threshold.<ul> <li>4  : windowSize: specifies the number of bases to average across</li> <li>20 : requiredQuality: specifies the average quality required.</li> </ul> </li> <li>ILLUMINACLIP:/shared/home/SHARED/DATA/adapters/TruSeq3-PE.fa:2:30:10 Cut adapter and other Illumina-specific sequences from the read.<ul> <li>Cut adapter and other illumina-specific sequences from the read.</li> <li>2  : seedMismatches: specifies the maximum mismatch count which will still allow a full match to be performed</li> <li>30 : palindromeClipThreshold: specifies how accurate the match between the two \u2018adapter ligated\u2019 reads must be for PE palindrome read alignment.</li> <li>10 : simpleClipThreshold: specifies how accurate the match between any adapter etc. sequence must be against a read.</li> </ul> </li> </ul> <p>Here is a script for a single sample : </p> <pre><code>#!/bin/bash\n#SBATCH --job-name=trim\n#SBATCH --time=01:00:00\n#SBATCH --cpus-per-task=4\n#SBATCH --mem-per-cpu=4G\n#SBATCH -o trim.o\n#SBATCH -e trim.e\n\nml  trimmomatic\n\ndataDIR=/shared/data/DATA/Liu2015\n\ntrimmomatic=\"java -jar $EBROOTTRIMMOMATIC/trimmomatic-0.39.jar\"\n\noutDIR=Liu2015_trimmed_reads\n\nmkdir -p $outDIR\n\n$trimmomatic PE -threads 4 -phred33 \\\n$dataDIR/SRR1272187_1.fastq.gz \\\n$dataDIR/SRR1272187_2.fastq.gz \\\n$outDIR/SRR1272187_NFLV_trimmed_paired_1.fastq $outDIR/SRR1272187_NFLV_trimmed_unpaired_1.fastq \\\n$outDIR/SRR1272187_NFLV_trimmed_paired_2.fastq $outDIR/SRR1272187_NFLV_trimmed_unpaired_2.fastq \\\nSLIDINGWINDOW:4:20 ILLUMINACLIP:/shared/data/DATA/adapters/TruSeq3-PE.fa:2:30:10 ## compressing the resulting fastq files to save some space.\ngzip $outDIR/SRR1272187_NFLV_trimmed_paired_1.fastq\ngzip $outDIR/SRR1272187_NFLV_trimmed_unpaired_1.fastq\ngzip $outDIR/SRR1272187_NFLV_trimmed_paired_2.fastq\ngzip $outDIR/SRR1272187_NFLV_trimmed_unpaired_2.fastq\n</code></pre>"}]}
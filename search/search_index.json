{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Introduction to RNA-Seq: From quality control to pathway analysis Learning outcomes General learning outcomes After this course, you will be able to: Describe advantages and pitfalls of RNA sequencing Design their own experiment Perform the downstream analysis using command line software (QC, mapping, counting, differential expression analysis, pathway analysis, etc) Critically assess the quality of their results at each step of the downstream analysis Detect significantly differentially expressed genes between conditions Learning experiences To reach the learning outcomes we will use lectures and exercises. During lectures, do not hesitate to ask any questions as we progress through the slides. During exercises, you are free to discuss with other participants. Exercises are provided with solutions. How you use them to your advantage is up to you.","title":"Home"},{"location":"index.html#introduction-to-rna-seq-from-quality-control-to-pathway-analysis","text":"","title":"Introduction to RNA-Seq: From quality control to pathway analysis"},{"location":"index.html#learning-outcomes","text":"","title":"Learning outcomes"},{"location":"index.html#general-learning-outcomes","text":"After this course, you will be able to: Describe advantages and pitfalls of RNA sequencing Design their own experiment Perform the downstream analysis using command line software (QC, mapping, counting, differential expression analysis, pathway analysis, etc) Critically assess the quality of their results at each step of the downstream analysis Detect significantly differentially expressed genes between conditions","title":"General learning outcomes"},{"location":"index.html#learning-experiences","text":"To reach the learning outcomes we will use lectures and exercises. During lectures, do not hesitate to ask any questions as we progress through the slides. During exercises, you are free to discuss with other participants. Exercises are provided with solutions. How you use them to your advantage is up to you.","title":"Learning experiences"},{"location":"course_schedule.html","text":"Day 1 start end subject 9:00 9:15 Welcome 9:15 10:45 RNAseq - technologies and design 10:45 11:00 BREAK 11:00 11:30 Server login + unix fresh up 11:30 12:30 Quality control 12:30 13:30 LUNCH BREAK 13:30 14:30 Quality control - practical 13:30 14:30 Sequence Trimming and adapter removal 14:30 15:30 Reads mapping : indexing genome 15:30 15:45 BREAK 15:45 17:00 Reads mapping : mapping Day 2 start end subject 9:00 9:15 Recap of yesterday 9:15 10:30 Read counting 10:30 10:45 BREAK 10:30 12:30 Differential Expression Inference - theory 12:30 13:30 LUNCH BREAK 13:30 17:00 Differential Expression Inference - practice","title":"Course schedule"},{"location":"course_schedule.html#day-1","text":"start end subject 9:00 9:15 Welcome 9:15 10:45 RNAseq - technologies and design 10:45 11:00 BREAK 11:00 11:30 Server login + unix fresh up 11:30 12:30 Quality control 12:30 13:30 LUNCH BREAK 13:30 14:30 Quality control - practical 13:30 14:30 Sequence Trimming and adapter removal 14:30 15:30 Reads mapping : indexing genome 15:30 15:45 BREAK 15:45 17:00 Reads mapping : mapping","title":"Day 1"},{"location":"course_schedule.html#day-2","text":"start end subject 9:00 9:15 Recap of yesterday 9:15 10:30 Read counting 10:30 10:45 BREAK 10:30 12:30 Differential Expression Inference - theory 12:30 13:30 LUNCH BREAK 13:30 17:00 Differential Expression Inference - practice","title":"Day 2"},{"location":"precourse.html","text":"Precourse preparations As announced in the course registration webpage , we expect participants to already have a basic knowledge in Next Generation Sequencing (NGS) techniques. UNIX A correct command of UNIX command line is also required to be able to follow this course, given that that the tools used to process sequenced reads use this interface. If you are unsure about your capabilities or feel a bit rusty, we strongly recommend you spend some time practising before the course : in our experience the more comfortable you are with UNIX, the more you will be able to focus on the RNAseq during the course and the you will profit from it. You may refer to the SIB\u2019s UNIX e-learning module R A basic knowledge of the R language is required to perform most analytical steps after reads have been mapped and quantified : differential gene expression, gene set enrichment, over-representation analysis. Software To replicate the technical condition of today\u2019s real-life data analysis, we will perform our computations on a distant HPC cluster. To access it: macOS / Linux : you can already use your terminal Windows : you should install a terminal which lets you do ssh (for instance mobaXterm ).","title":"Precourse preparations"},{"location":"precourse.html#precourse-preparations","text":"As announced in the course registration webpage , we expect participants to already have a basic knowledge in Next Generation Sequencing (NGS) techniques.","title":"Precourse preparations"},{"location":"precourse.html#unix","text":"A correct command of UNIX command line is also required to be able to follow this course, given that that the tools used to process sequenced reads use this interface. If you are unsure about your capabilities or feel a bit rusty, we strongly recommend you spend some time practising before the course : in our experience the more comfortable you are with UNIX, the more you will be able to focus on the RNAseq during the course and the you will profit from it. You may refer to the SIB\u2019s UNIX e-learning module","title":"UNIX"},{"location":"precourse.html#r","text":"A basic knowledge of the R language is required to perform most analytical steps after reads have been mapped and quantified : differential gene expression, gene set enrichment, over-representation analysis.","title":"R"},{"location":"precourse.html#software","text":"To replicate the technical condition of today\u2019s real-life data analysis, we will perform our computations on a distant HPC cluster. To access it: macOS / Linux : you can already use your terminal Windows : you should install a terminal which lets you do ssh (for instance mobaXterm ).","title":"Software"},{"location":"day1/design.html","text":"Designing your experiment is the first step. It is crucial as it conditions the sort of questions that you can ask from your data as well as the confidence you may have in the answers. Knowing about the sequencing technologies, their strengths and limitations, as well as the RNAseq analysis pipeline, are the keys to design a successful RNAseq experiment. After having completed this chapter you will be able to: describe different sequencing technologies and their application in RNAseq differentiate between technical and biological replicates choose an appropriate sequencing depth and number of replicates depending on your scientific question Material Download the presentation","title":"RNAseq - technologies and design"},{"location":"day1/design.html#material","text":"Download the presentation","title":"Material"},{"location":"day1/mapping.html","text":"At the end of this lesson, you will be able to : identify the differences between a local aligner and a pseudo aligner perform genome indexing appropriate to your data map your RNAseq data onto the genome Material Download the presentation STAR website Building a reference genome index Before any mapping can be achieved, you must first index the genome want to map to. We will be using the Ensembl versions of iGenome references, with their accompanying GTF annotations. Note While the data are already on the server here, in practice, and/or if you are following this course without a teacher you can grab the reference genome data from Ensembl ftp website . In particular, you will want a mouse DNA fasta file and gtf file [release-104 at the time we are linking this. Checking for more recent release is recommended]. Task : Using STAR, build a genome index for chromosome 19 of Mus musculus using the associated GTF Important notes : .fasta and .gtf files are in : /shared/home/SHARED/DATA/Mouse_chr19/ refer to the manual to determine which options to use. the --genomeDir parameter is the output folder this job should require less than 4Gb and 30min to run. Note While your indexing job is running, you can read ahead in STAR\u2019s manual to prepare the next step : mapping your reads onto the reference. STAR indexing script #!/usr/bin/bash #SBATCH --job-name=star-build #SBATCH --time=00:30:00 #SBATCH --cpus-per-task=4 #SBATCH --mem=4G #SBATCH -o star-build.o #SBATCH -e star-build.e G_FASTA = /home/SHARED/DATA/Mouse_chr19/Mus_musculus.GRCm38.dna.chromosome.19.fa G_GTF = /home/SHARED/DATA/Mouse_chr19/Mus_musculus.GRCm38.101.chr19.gtf ml STAR mkdir -p STAR_references $singularity_exec STAR --runMode genomeGenerate \\ --genomeDir STAR_references \\ --genomeFastaFiles $G_FASTA \\ --sjdbGTFfile $G_GTF \\ --outTmpDir /tmp/ ${ SLURM_JOB_USER } _ ${ SLURM_JOB_ID } \\ --runThreadN 4 Extra task : Determine how you would add an additional feature to your reference, for example for a novel transcript not described by the standard reference. Answer Edit the gtf file to add your additionnal feature(s), following the proper format Warning Remember : request a maximum of 30G and 8 cpus. Mapping reads onto the reference Task : Using STAR, align one of the FASTQ files from the Ruhland2016 study against the mouse genome. Use the full indexed genome at /shared/home/SHARED/DATA/Mouse_STAR_index/ IMPORTANT : on the server use the following option in your STAR commands: --outTmpDir /tmp/${SLURM_JOB_USER}_${SLURM_JOB_ID}/ Generate a BAM file sorted by coordinate Generate a geneCounts file Mapping reads and generating a sorted BAM from one of the Ruhland2016 et al. FASTQ files should take about 20 minutes Note Take the time to read the parts of the STAR manual which concern you : a bit of planning ahead can save you a lot of time-consuming/headache-inducing trial and error on your script. Warning Remember : request a maximum of 30G and 8 cpus. STAR mapping script The following sets-up an array of tasks to align all samples. Source file : Ruhland2016.fastqFiles.txt : SRR3180535_EtOH1_1.fastq.gz SRR3180536_EtOH2_1.fastq.gz SRR3180537_EtOH3_1.fastq.gz SRR3180538_TAM1_1.fastq.gz SRR3180539_TAM2_1.fastq.gz SRR3180540_TAM3_1.fastq.gz sbatch script : #!/usr/bin/bash #SBATCH --job-name=star-aln #SBATCH --time=01:00:00 #SBATCH --cpus-per-task=8 #SBATCH --mem=30G #SBATCH -o star-aln.%a.o #SBATCH -e star-aln.%a.e #SBATCH --array 1-6%6 ml STAR outDIR=STAR_Ruhland2016 mkdir -p $outDIR dataDIR=/home/SHARED/DATA/Ruhland2016 sourceFILE=Ruhland2016.fastqFiles.txt fastqFILE=`sed -n ${SLURM_ARRAY_TASK_ID}p $sourceFILE` genomeDIR=/home/SHARED/DATA/Mouse_STAR_index STAR --runThreadN 8 --genomeDir $genomeDIR \\ --outSAMtype BAM SortedByCoordinate --outReadsUnmapped Fastx \\ --outFileNamePrefix $outDIR/$fastqFILE \\ --quantMode GeneCounts \\ --readFilesIn $dataDIR/$fastqFILE --readFilesCommand zcat \\ --outTmpDir /tmp/${SLURM_JOB_USER}_${SLURM_JOB_ID} The options of STAR are : * \u2013runThreadN 8 : 8 threads to go faster * \u2013genomeDir $genomeDIR : path of the genome to map to * \u2013outSAMtype BAM SortedByCoordinate : output a sorted BAM file * \u2013outReadsUnmapped Fastx : output the non-mapping reads (in case we want to analyse them) * \u2013outFileNamePrefix $outDIR/$fastqFILE : prefix of output files * \u2013quantMode GeneCounts : will create a file with counts of reads per gene * \u2013readFilesIn $dataDIR/$fastqFILE : input read file * \u2013readFilesCommand zcat : command to unzip the input file * \u2013outTmpDir /tmp/${SLURM_JOB_USER}_${SLURM_JOB_ID} : temporary file folder, for STAR temp files QC on the aligned reads You can call MultiQC on the STAR output folder to gather a report on the alignment. Here this concern a single sample but usually this would cover all your samples. Task : use multiqc to generate a QC report on the result of your mapping. Evaluate the alignment statistics. Do you consider this to be a good alignment? How many unmapped reads are there? Where might this come from, and how would you determine this? what could you say about library strandedness ? script and answers #!/usr/bin/bash #SBATCH --job-name=multiqc #SBATCH --time=00:30:00 #SBATCH --cpus-per-task=1 #SBATCH --mem=1G #SBATCH -o multiqc_star_Ruhland2016.o #SBATCH -e multiqc_star_Ruhland2016.e ml multiqc mkdir -p STAR_MULTIQC_Ruhland2016/ multiqc -o STAR_MULTIQC_Ruhland2016/ STAR_Ruhland2016/ answers : TBD\u2026 ADDITIONNAL : STAR 2-Pass Genome annotations are incomplete, particularly for complex eukaryotes : ther are many missing splice junctions. The first pass of STAR can create a splice junction database, known and novel. This splice junction database can, in turn, be used to guide an improved second round of alignment using a command like: STAR <1st round options> --sjdbFileChrStartEnd sample_SJ.out.tab Task : run STAR in this STAR-2pass mode on the same sample as before and evaluate the results. script #!/usr/bin/bash #SBATCH --job-name=star-aln2 #SBATCH --time=01:00:00 #SBATCH --cpus-per-task=8 #SBATCH --mem=30G #SBATCH -o star-aln-2pass.%a.o #SBATCH -e star-aln-2pass.%a.e #SBATCH --array 1-6%6 ml STAR outDIR=STAR_Ruhland2016 mkdir -p $outDIR dataDIR=/home/SHARED/DATA/Ruhland2016 sourceFILE=Ruhland2016.fastqFiles.txt fastqFILE=`sed -n ${SLURM_ARRAY_TASK_ID}p $sourceFILE` genomeDIR=/home/SHARED/DATA/Mouse_STAR_index $singularity_exec STAR --runThreadN 8 --genomeDir $genomeDIR \\ --outSAMtype BAM SortedByCoordinate \\ --outFileNamePrefix $outDIR/$fastqFILE.2Pass. \\ --outReadsUnmapped Fastx --quantMode GeneCounts \\ --sjdbFileChrStartEnd $outDIR/${fastqFILE}SJ.out.tab \\ --readFilesIn $dataDIR/$fastqFILE --readFilesCommand zcat \\ --outTmpDir /tmp/${SLURM_JOB_USER}_${SLURM_JOB_ID} ADDITIONNAL : Assessing read coverage for biases The RSeQC package includes a function for evaluating \u201cgene body coverage\u201d , which can be used to assess 5\u2019 or 3\u2019 bias, which might happen if your RNA is degraded or otherwise biased Requirements: Genome annotations in the 12-column BED format : you can grab one for mouse here , or at /data/GRCm38/Mus_musculus.GRCm38.89.bed12 on the server CHECK LINK. gtf2bed instead? Indexed and sorted BAM file, which can be generated from a sorted BAM using the SAMtools package: samtools index sample1_sorted.bam Example command : geneBody_coverage.py -r /data/GRCm38/Mus_musculus.GRCm38.89.bed12 \\ -i sample1_sorted.bam \\ -f pdf \\ -o output_prefix Task : Evaluate the gene body coverage for the sample you aligned. expected RAM : expected time : geneBody_coverage script TBD","title":"Reads mapping"},{"location":"day1/mapping.html#material","text":"Download the presentation STAR website","title":"Material"},{"location":"day1/mapping.html#building-a-reference-genome-index","text":"Before any mapping can be achieved, you must first index the genome want to map to. We will be using the Ensembl versions of iGenome references, with their accompanying GTF annotations. Note While the data are already on the server here, in practice, and/or if you are following this course without a teacher you can grab the reference genome data from Ensembl ftp website . In particular, you will want a mouse DNA fasta file and gtf file [release-104 at the time we are linking this. Checking for more recent release is recommended]. Task : Using STAR, build a genome index for chromosome 19 of Mus musculus using the associated GTF Important notes : .fasta and .gtf files are in : /shared/home/SHARED/DATA/Mouse_chr19/ refer to the manual to determine which options to use. the --genomeDir parameter is the output folder this job should require less than 4Gb and 30min to run. Note While your indexing job is running, you can read ahead in STAR\u2019s manual to prepare the next step : mapping your reads onto the reference. STAR indexing script #!/usr/bin/bash #SBATCH --job-name=star-build #SBATCH --time=00:30:00 #SBATCH --cpus-per-task=4 #SBATCH --mem=4G #SBATCH -o star-build.o #SBATCH -e star-build.e G_FASTA = /home/SHARED/DATA/Mouse_chr19/Mus_musculus.GRCm38.dna.chromosome.19.fa G_GTF = /home/SHARED/DATA/Mouse_chr19/Mus_musculus.GRCm38.101.chr19.gtf ml STAR mkdir -p STAR_references $singularity_exec STAR --runMode genomeGenerate \\ --genomeDir STAR_references \\ --genomeFastaFiles $G_FASTA \\ --sjdbGTFfile $G_GTF \\ --outTmpDir /tmp/ ${ SLURM_JOB_USER } _ ${ SLURM_JOB_ID } \\ --runThreadN 4 Extra task : Determine how you would add an additional feature to your reference, for example for a novel transcript not described by the standard reference. Answer Edit the gtf file to add your additionnal feature(s), following the proper format Warning Remember : request a maximum of 30G and 8 cpus.","title":"Building a reference genome index"},{"location":"day1/mapping.html#mapping-reads-onto-the-reference","text":"Task : Using STAR, align one of the FASTQ files from the Ruhland2016 study against the mouse genome. Use the full indexed genome at /shared/home/SHARED/DATA/Mouse_STAR_index/ IMPORTANT : on the server use the following option in your STAR commands: --outTmpDir /tmp/${SLURM_JOB_USER}_${SLURM_JOB_ID}/ Generate a BAM file sorted by coordinate Generate a geneCounts file Mapping reads and generating a sorted BAM from one of the Ruhland2016 et al. FASTQ files should take about 20 minutes Note Take the time to read the parts of the STAR manual which concern you : a bit of planning ahead can save you a lot of time-consuming/headache-inducing trial and error on your script. Warning Remember : request a maximum of 30G and 8 cpus. STAR mapping script The following sets-up an array of tasks to align all samples. Source file : Ruhland2016.fastqFiles.txt : SRR3180535_EtOH1_1.fastq.gz SRR3180536_EtOH2_1.fastq.gz SRR3180537_EtOH3_1.fastq.gz SRR3180538_TAM1_1.fastq.gz SRR3180539_TAM2_1.fastq.gz SRR3180540_TAM3_1.fastq.gz sbatch script : #!/usr/bin/bash #SBATCH --job-name=star-aln #SBATCH --time=01:00:00 #SBATCH --cpus-per-task=8 #SBATCH --mem=30G #SBATCH -o star-aln.%a.o #SBATCH -e star-aln.%a.e #SBATCH --array 1-6%6 ml STAR outDIR=STAR_Ruhland2016 mkdir -p $outDIR dataDIR=/home/SHARED/DATA/Ruhland2016 sourceFILE=Ruhland2016.fastqFiles.txt fastqFILE=`sed -n ${SLURM_ARRAY_TASK_ID}p $sourceFILE` genomeDIR=/home/SHARED/DATA/Mouse_STAR_index STAR --runThreadN 8 --genomeDir $genomeDIR \\ --outSAMtype BAM SortedByCoordinate --outReadsUnmapped Fastx \\ --outFileNamePrefix $outDIR/$fastqFILE \\ --quantMode GeneCounts \\ --readFilesIn $dataDIR/$fastqFILE --readFilesCommand zcat \\ --outTmpDir /tmp/${SLURM_JOB_USER}_${SLURM_JOB_ID} The options of STAR are : * \u2013runThreadN 8 : 8 threads to go faster * \u2013genomeDir $genomeDIR : path of the genome to map to * \u2013outSAMtype BAM SortedByCoordinate : output a sorted BAM file * \u2013outReadsUnmapped Fastx : output the non-mapping reads (in case we want to analyse them) * \u2013outFileNamePrefix $outDIR/$fastqFILE : prefix of output files * \u2013quantMode GeneCounts : will create a file with counts of reads per gene * \u2013readFilesIn $dataDIR/$fastqFILE : input read file * \u2013readFilesCommand zcat : command to unzip the input file * \u2013outTmpDir /tmp/${SLURM_JOB_USER}_${SLURM_JOB_ID} : temporary file folder, for STAR temp files","title":"Mapping reads onto the reference"},{"location":"day1/mapping.html#qc-on-the-aligned-reads","text":"You can call MultiQC on the STAR output folder to gather a report on the alignment. Here this concern a single sample but usually this would cover all your samples. Task : use multiqc to generate a QC report on the result of your mapping. Evaluate the alignment statistics. Do you consider this to be a good alignment? How many unmapped reads are there? Where might this come from, and how would you determine this? what could you say about library strandedness ? script and answers #!/usr/bin/bash #SBATCH --job-name=multiqc #SBATCH --time=00:30:00 #SBATCH --cpus-per-task=1 #SBATCH --mem=1G #SBATCH -o multiqc_star_Ruhland2016.o #SBATCH -e multiqc_star_Ruhland2016.e ml multiqc mkdir -p STAR_MULTIQC_Ruhland2016/ multiqc -o STAR_MULTIQC_Ruhland2016/ STAR_Ruhland2016/ answers : TBD\u2026","title":"QC on the aligned reads"},{"location":"day1/mapping.html#additionnal-star-2-pass","text":"Genome annotations are incomplete, particularly for complex eukaryotes : ther are many missing splice junctions. The first pass of STAR can create a splice junction database, known and novel. This splice junction database can, in turn, be used to guide an improved second round of alignment using a command like: STAR <1st round options> --sjdbFileChrStartEnd sample_SJ.out.tab Task : run STAR in this STAR-2pass mode on the same sample as before and evaluate the results. script #!/usr/bin/bash #SBATCH --job-name=star-aln2 #SBATCH --time=01:00:00 #SBATCH --cpus-per-task=8 #SBATCH --mem=30G #SBATCH -o star-aln-2pass.%a.o #SBATCH -e star-aln-2pass.%a.e #SBATCH --array 1-6%6 ml STAR outDIR=STAR_Ruhland2016 mkdir -p $outDIR dataDIR=/home/SHARED/DATA/Ruhland2016 sourceFILE=Ruhland2016.fastqFiles.txt fastqFILE=`sed -n ${SLURM_ARRAY_TASK_ID}p $sourceFILE` genomeDIR=/home/SHARED/DATA/Mouse_STAR_index $singularity_exec STAR --runThreadN 8 --genomeDir $genomeDIR \\ --outSAMtype BAM SortedByCoordinate \\ --outFileNamePrefix $outDIR/$fastqFILE.2Pass. \\ --outReadsUnmapped Fastx --quantMode GeneCounts \\ --sjdbFileChrStartEnd $outDIR/${fastqFILE}SJ.out.tab \\ --readFilesIn $dataDIR/$fastqFILE --readFilesCommand zcat \\ --outTmpDir /tmp/${SLURM_JOB_USER}_${SLURM_JOB_ID}","title":"ADDITIONNAL : STAR 2-Pass"},{"location":"day1/mapping.html#additionnal-assessing-read-coverage-for-biases","text":"The RSeQC package includes a function for evaluating \u201cgene body coverage\u201d , which can be used to assess 5\u2019 or 3\u2019 bias, which might happen if your RNA is degraded or otherwise biased Requirements: Genome annotations in the 12-column BED format : you can grab one for mouse here , or at /data/GRCm38/Mus_musculus.GRCm38.89.bed12 on the server CHECK LINK. gtf2bed instead? Indexed and sorted BAM file, which can be generated from a sorted BAM using the SAMtools package: samtools index sample1_sorted.bam Example command : geneBody_coverage.py -r /data/GRCm38/Mus_musculus.GRCm38.89.bed12 \\ -i sample1_sorted.bam \\ -f pdf \\ -o output_prefix Task : Evaluate the gene body coverage for the sample you aligned. expected RAM : expected time : geneBody_coverage script TBD","title":"ADDITIONNAL : Assessing read coverage for biases"},{"location":"day1/quality_control.html","text":"Quality Control is the essential first step to perform once you have your data as .fastq or .fastq.gz form. During this block, you will learn to : create QC report for a single file with fastqc aggregate multiple QC reports using multiqc interpret the QC reports for RNAseq experiement Note Although we aim to present tools as stable as possible, software evolve and their precise interface can change with time. We strongly recommend you to have a look at each command help page or manual before launching any command. To this end we provide links to each tool website. This can also be useful to you if you are following this course without access to a compute cluster and have to install these tools on your machine. Material Download the presentation FastQC website MultiQC website Meet the datasets We will be working with two datasets from the following studies: Liu et al (2015) \u201cRNA-Seq identifies novel myocardial gene expression signatures of heart failure\u201d Genomics 105(2):83-89 https://doi.org/10.1016/j.ygeno.2014.12.002 GSE57345 Homo sapiens heart left ventricle samples : 3 with heart failure, 3 without 6 samples of paired-end reads Ruhland et al (2016) \u201cStromal senescence establishes an immunosuppressive microenvironment that drives tumorigenesis\u201d Nature Communications 7:11762 https://dx.doi.org/10.1038/ncomms11762 GSE78128 Mus musculus skin fibroblast samples : 3 non-senescent (EtOH) , 3 senescent(TAM) 6 samples of single-end reads Retrieving published datasets Warning If you are following this course with a teacher, then the for the data is already on the server. There is no need to download it again. Most NGS data is deposited at the Short Read Archive (SRA) hosted by the NCBI, with links from the Gene Expression Omnibus (GEO) Several steps are required to retrieve data from a published study : find GEO or SRA identifier from publication find the \u201crun\u201d files for each sample (SRR) use SRA Toolkit to dump SRR to FASTQ For example, on the Liu2015 dataset : Locate in their publication the GEO accession: GSE57345 Use the NCBI search engine to find this accession : GSE57345 This project is made of several subproject. Scroll down, and in the table find the Bioproject id : PRJNA246308 Go to the SRA run selector , enter your Bioproject id From the results of your search , select all relevant runs click on \u201cAccession List\u201d in the Select table use fastq-dump (part of the SRA Toolkit ) on the downloaded accession list. For example: fastq-dump --gzip --skip-technical --readids --split-files --clip SRR1272191 Note You\u2019ll need to know the nature of the dataset before attempting to (library type, paired vs single end, etc) fastq-dump takes a very long time More information about fastq-dump FastQC : a report for a single fastq file FastQC is a nice tool to get a variety of QC measures from files such as .fastq , .bam or .sam files. Although it has many options, the default parameters are often enough for our purpose : fastqc -o <output_directory> file1.fastq file2.fastq ... fileN.fastq FastQC is reasonnably intelligent and will try to recognise the file format and uncompress it if necessary (so no need to decompress). Task: run FastQC analysis on the two datasets at: /shared/home/SHARED/DATA/Liu2015/ and /shared/home/SHARED/DATA/Ruhland2016 . Look at one of the QC report. What are your conclusions ? Would you want to perform some operations on the reads such as low-quality bases trimming, removal of adapters ? Important points: there is no need to copy the read files to your home directory (in fact, don\u2019t do that: we won\u2019t have enough space left on the disk\u2026) FastQC RAM requirements : 1Gb is more than enough FastQC time requirements : ~ 5min / read file try to make sure FastQC outputs all reports in the same directory, this will save time for the next step ;-) Note reminder : to get the data from the distant server to your machine, you may use an SFTP client (filezilla, mobaXterm), or the command line tool from your mahcine : scp login@xx.xx.xx:~/path/to/file.txt . Liu2015 FastQC sbatch script Here is an sbatch script for one sample: #!/usr/bin/bash #SBATCH --job-name=fastqc #SBATCH --time=01:00:00 #SBATCH --cpus-per-task=1 #SBATCH --mem=1G #SBATCH -o fastqc_Liu2015.o #SBATCH -e fastqc_Liu2015.e dataDir = /shared/home/SHARED/DATA/Liu2015 ml fastqc # creating the output folder mkdir -p FASTQC_Liu2015/ fastqc -o FASTQC_Liu2015/ $dataDir /SRR1272187_1.fastq.gz You could either have one sbatch script per sample (recommended), OR put the fastqc commands for all the samples in the same script (not recommended). The first is recommended because you can submit each scripts at once and they will then run in parallel, whereas with the second option the samples would be handled sequentially and the overall job would likely take >1hour. Ruhland2016 FastQC sbatch script Here is an sbatch script for one sample: #!/usr/bin/bash #SBATCH --job-name=fastqc #SBATCH --time=01:00:00 #SBATCH --cpus-per-task=1 #SBATCH --mem=1G #SBATCH -o fastqc_Ruhland2016.o #SBATCH -e fastqc_Ruhland2016.e dataDir = /shared/home/SHARED/DATA/Ruhland2016 ml fastqc # creating the output folder mkdir -p FASTQC_Ruhland2016/ fastqc -o FASTQC_Ruhland2016/ $dataDir /SRR3180540_TAM3_1.fastq.gz You could either have one sbatch script per sample (recommended), OR put the fastqc commands for all the samples in the same script (not recommended). The first is recommended because you can submit each scripts at once and they will then run in parallel, whereas with the second option the samples would be handled sequentially and the overall job would likely take >1hour. alternative scripts using array job First, have a file named Ruhland2016.fastqFiles.txt containing the sample fastq file names : SRR3180535_EtOH1_1.fastq.gz SRR3180536_EtOH2_1.fastq.gz SRR3180537_EtOH3_1.fastq.gz SRR3180538_TAM1_1.fastq.gz SRR3180539_TAM2_1.fastq.gz SRR3180540_TAM3_1.fastq.gz Then, in the same folder, you can create this sbatch script : #!/usr/bin/bash #SBATCH --job-name=fastqc #SBATCH --time=00:30:00 #SBATCH --cpus-per-task=1 #SBATCH --mem=1G #SBATCH -o fastqc_Ruhland2016.%a.o #SBATCH -e fastqc_Ruhland2016.%a.e #SBATCH --array 1-6%6 ml fastqc dataDir = /shared/home/SHARED/DATA/Ruhland2016 sourceFILE = Ruhland2016.fastqFiles.txt ## retrieving 1 filename from Ruhland2016.fastqFiles.txt fastqFILE = ` sed -n ${ SLURM_ARRAY_TASK_ID } p $sourceFILE ` mkdir -p FASTQC_Ruhland2016/ $singularity_exec fastqc -o FASTQC_Ruhland2016/ $dataDir / $fastqFILE When submitted with sbatch , this script will spawn 6 tasks in parallel, each with a different value of ${SLURM_ARRAY_TASK_ID} . This is the most recommended option : this allows you to launch all your job in parallel with a single script. Interpretation of a report Download an annotated report MultiQC : grouping multiple reports In practice, you will be likely to have more than a couple of samples (and often more than 30 or 50) to handle: consulting and comparing the QC reports of each individually would be tedious. MultiQC is a tool that lets you combine multiple reports in a single, interactive document that let you explore your data easily. We will here be focusing on grouping FastQC reports, but MultiQC can also be applied to the output or logs of other bioinformatics tools, such as mappers as we will see later. In its default usage, multiqc only needs to be provided a path where it will find all the individual reports, and it will scan them and write a report named multiqc_report.html . Although the default behaviour, with a couple of options we get a slightly better control over the output: * --interactive : forces the plot to be interactive even when there is a lot of samples (this option can lead to larger html files) * -f <filename> : specify the name of the output file name For instance, a possible command line could be : multiqc -f multiQCreports/Liu2015_multiqc.html --interactive Liu2015_fastqc/ Ther are many additonnal info which let you customize your report. Use multiqc --help or visit their documentation webpage to learn more. Task: run MultiQC for each dataset. Look at the QC report. What are your conclusions ? Important points: MultiQC RAM requirements : 1Gb should be more than enough MultiQC time requirements : ~ 1min / read file sbatch scripts This is the script for the Ruhland2016 dataset. It presumes that the fastqc reports can be found in FASTQC_Ruhland2016/ #!/usr/bin/bash #SBATCH --job-name=multiqc #SBATCH --time=00:30:00 #SBATCH --cpus-per-task=1 #SBATCH --mem=1G #SBATCH -o multiqc_Ruhland2016.o #SBATCH -e multiqc_Ruhland2016.e ml multiqc mkdir -p MULTIQC_Ruhland2016/ multiqc -o MULTIQC_Ruhland2016/ FASTQC_Ruhland2016/ Interpretation of a report We will interpret the report for the Liu2015 data. The quality of reads drop below 30 around base 75. All samples seem affected Mean quality scores are on average fairly high, but some read exhibit low values. Most samples do not deviate too much from the expected curve. The two sample colored in orange and red have a mode for a very specific value. That may be indicative of contamination, retaining specific rRNA, or adapter sequence content. Ns are present at specific positions in specific samples. This is reminiscent of the PHRED quality curves at the top of the report. It seems some flowcells had a problem at specific time-point/positions. This is colored red because this would be a problem if the data was coming from genomic DNA sequencing. However here we are in the context of RNAseq : some transcripts are present in a large number of copies in the samples and consequently it is expected that some sequences are overrepresented. We see a clear trend of adapter contamination as we get closer to the reads end. Note the y-scale though : we never go above a 6% content per sample. Overall, we can conclude that these sample all suffer from some adapter content and a lower quality toward the reads second half. Furthermore a few samples have a peculiar Ns pattern between bases 20 and 30. It is then strongly advised to either : perform some trimming : remove adapter sequences + cut reads when average quality becomes too low use a mapper that takes base quality in account AND is able to ignore adapter sequence (and even then, you could try mapping on both croppedand uncropped data to see which is the best)","title":"Quality control"},{"location":"day1/quality_control.html#material","text":"Download the presentation FastQC website MultiQC website","title":"Material"},{"location":"day1/quality_control.html#meet-the-datasets","text":"We will be working with two datasets from the following studies: Liu et al (2015) \u201cRNA-Seq identifies novel myocardial gene expression signatures of heart failure\u201d Genomics 105(2):83-89 https://doi.org/10.1016/j.ygeno.2014.12.002 GSE57345 Homo sapiens heart left ventricle samples : 3 with heart failure, 3 without 6 samples of paired-end reads Ruhland et al (2016) \u201cStromal senescence establishes an immunosuppressive microenvironment that drives tumorigenesis\u201d Nature Communications 7:11762 https://dx.doi.org/10.1038/ncomms11762 GSE78128 Mus musculus skin fibroblast samples : 3 non-senescent (EtOH) , 3 senescent(TAM) 6 samples of single-end reads","title":"Meet the datasets"},{"location":"day1/quality_control.html#retrieving-published-datasets","text":"Warning If you are following this course with a teacher, then the for the data is already on the server. There is no need to download it again. Most NGS data is deposited at the Short Read Archive (SRA) hosted by the NCBI, with links from the Gene Expression Omnibus (GEO) Several steps are required to retrieve data from a published study : find GEO or SRA identifier from publication find the \u201crun\u201d files for each sample (SRR) use SRA Toolkit to dump SRR to FASTQ For example, on the Liu2015 dataset : Locate in their publication the GEO accession: GSE57345 Use the NCBI search engine to find this accession : GSE57345 This project is made of several subproject. Scroll down, and in the table find the Bioproject id : PRJNA246308 Go to the SRA run selector , enter your Bioproject id From the results of your search , select all relevant runs click on \u201cAccession List\u201d in the Select table use fastq-dump (part of the SRA Toolkit ) on the downloaded accession list. For example: fastq-dump --gzip --skip-technical --readids --split-files --clip SRR1272191 Note You\u2019ll need to know the nature of the dataset before attempting to (library type, paired vs single end, etc) fastq-dump takes a very long time More information about fastq-dump","title":"Retrieving published datasets"},{"location":"day1/quality_control.html#fastqc-a-report-for-a-single-fastq-file","text":"FastQC is a nice tool to get a variety of QC measures from files such as .fastq , .bam or .sam files. Although it has many options, the default parameters are often enough for our purpose : fastqc -o <output_directory> file1.fastq file2.fastq ... fileN.fastq FastQC is reasonnably intelligent and will try to recognise the file format and uncompress it if necessary (so no need to decompress). Task: run FastQC analysis on the two datasets at: /shared/home/SHARED/DATA/Liu2015/ and /shared/home/SHARED/DATA/Ruhland2016 . Look at one of the QC report. What are your conclusions ? Would you want to perform some operations on the reads such as low-quality bases trimming, removal of adapters ? Important points: there is no need to copy the read files to your home directory (in fact, don\u2019t do that: we won\u2019t have enough space left on the disk\u2026) FastQC RAM requirements : 1Gb is more than enough FastQC time requirements : ~ 5min / read file try to make sure FastQC outputs all reports in the same directory, this will save time for the next step ;-) Note reminder : to get the data from the distant server to your machine, you may use an SFTP client (filezilla, mobaXterm), or the command line tool from your mahcine : scp login@xx.xx.xx:~/path/to/file.txt . Liu2015 FastQC sbatch script Here is an sbatch script for one sample: #!/usr/bin/bash #SBATCH --job-name=fastqc #SBATCH --time=01:00:00 #SBATCH --cpus-per-task=1 #SBATCH --mem=1G #SBATCH -o fastqc_Liu2015.o #SBATCH -e fastqc_Liu2015.e dataDir = /shared/home/SHARED/DATA/Liu2015 ml fastqc # creating the output folder mkdir -p FASTQC_Liu2015/ fastqc -o FASTQC_Liu2015/ $dataDir /SRR1272187_1.fastq.gz You could either have one sbatch script per sample (recommended), OR put the fastqc commands for all the samples in the same script (not recommended). The first is recommended because you can submit each scripts at once and they will then run in parallel, whereas with the second option the samples would be handled sequentially and the overall job would likely take >1hour. Ruhland2016 FastQC sbatch script Here is an sbatch script for one sample: #!/usr/bin/bash #SBATCH --job-name=fastqc #SBATCH --time=01:00:00 #SBATCH --cpus-per-task=1 #SBATCH --mem=1G #SBATCH -o fastqc_Ruhland2016.o #SBATCH -e fastqc_Ruhland2016.e dataDir = /shared/home/SHARED/DATA/Ruhland2016 ml fastqc # creating the output folder mkdir -p FASTQC_Ruhland2016/ fastqc -o FASTQC_Ruhland2016/ $dataDir /SRR3180540_TAM3_1.fastq.gz You could either have one sbatch script per sample (recommended), OR put the fastqc commands for all the samples in the same script (not recommended). The first is recommended because you can submit each scripts at once and they will then run in parallel, whereas with the second option the samples would be handled sequentially and the overall job would likely take >1hour. alternative scripts using array job First, have a file named Ruhland2016.fastqFiles.txt containing the sample fastq file names : SRR3180535_EtOH1_1.fastq.gz SRR3180536_EtOH2_1.fastq.gz SRR3180537_EtOH3_1.fastq.gz SRR3180538_TAM1_1.fastq.gz SRR3180539_TAM2_1.fastq.gz SRR3180540_TAM3_1.fastq.gz Then, in the same folder, you can create this sbatch script : #!/usr/bin/bash #SBATCH --job-name=fastqc #SBATCH --time=00:30:00 #SBATCH --cpus-per-task=1 #SBATCH --mem=1G #SBATCH -o fastqc_Ruhland2016.%a.o #SBATCH -e fastqc_Ruhland2016.%a.e #SBATCH --array 1-6%6 ml fastqc dataDir = /shared/home/SHARED/DATA/Ruhland2016 sourceFILE = Ruhland2016.fastqFiles.txt ## retrieving 1 filename from Ruhland2016.fastqFiles.txt fastqFILE = ` sed -n ${ SLURM_ARRAY_TASK_ID } p $sourceFILE ` mkdir -p FASTQC_Ruhland2016/ $singularity_exec fastqc -o FASTQC_Ruhland2016/ $dataDir / $fastqFILE When submitted with sbatch , this script will spawn 6 tasks in parallel, each with a different value of ${SLURM_ARRAY_TASK_ID} . This is the most recommended option : this allows you to launch all your job in parallel with a single script. Interpretation of a report Download an annotated report","title":"FastQC : a report for a single fastq file"},{"location":"day1/quality_control.html#multiqc-grouping-multiple-reports","text":"In practice, you will be likely to have more than a couple of samples (and often more than 30 or 50) to handle: consulting and comparing the QC reports of each individually would be tedious. MultiQC is a tool that lets you combine multiple reports in a single, interactive document that let you explore your data easily. We will here be focusing on grouping FastQC reports, but MultiQC can also be applied to the output or logs of other bioinformatics tools, such as mappers as we will see later. In its default usage, multiqc only needs to be provided a path where it will find all the individual reports, and it will scan them and write a report named multiqc_report.html . Although the default behaviour, with a couple of options we get a slightly better control over the output: * --interactive : forces the plot to be interactive even when there is a lot of samples (this option can lead to larger html files) * -f <filename> : specify the name of the output file name For instance, a possible command line could be : multiqc -f multiQCreports/Liu2015_multiqc.html --interactive Liu2015_fastqc/ Ther are many additonnal info which let you customize your report. Use multiqc --help or visit their documentation webpage to learn more. Task: run MultiQC for each dataset. Look at the QC report. What are your conclusions ? Important points: MultiQC RAM requirements : 1Gb should be more than enough MultiQC time requirements : ~ 1min / read file sbatch scripts This is the script for the Ruhland2016 dataset. It presumes that the fastqc reports can be found in FASTQC_Ruhland2016/ #!/usr/bin/bash #SBATCH --job-name=multiqc #SBATCH --time=00:30:00 #SBATCH --cpus-per-task=1 #SBATCH --mem=1G #SBATCH -o multiqc_Ruhland2016.o #SBATCH -e multiqc_Ruhland2016.e ml multiqc mkdir -p MULTIQC_Ruhland2016/ multiqc -o MULTIQC_Ruhland2016/ FASTQC_Ruhland2016/ Interpretation of a report We will interpret the report for the Liu2015 data. The quality of reads drop below 30 around base 75. All samples seem affected Mean quality scores are on average fairly high, but some read exhibit low values. Most samples do not deviate too much from the expected curve. The two sample colored in orange and red have a mode for a very specific value. That may be indicative of contamination, retaining specific rRNA, or adapter sequence content. Ns are present at specific positions in specific samples. This is reminiscent of the PHRED quality curves at the top of the report. It seems some flowcells had a problem at specific time-point/positions. This is colored red because this would be a problem if the data was coming from genomic DNA sequencing. However here we are in the context of RNAseq : some transcripts are present in a large number of copies in the samples and consequently it is expected that some sequences are overrepresented. We see a clear trend of adapter contamination as we get closer to the reads end. Note the y-scale though : we never go above a 6% content per sample. Overall, we can conclude that these sample all suffer from some adapter content and a lower quality toward the reads second half. Furthermore a few samples have a peculiar Ns pattern between bases 20 and 30. It is then strongly advised to either : perform some trimming : remove adapter sequences + cut reads when average quality becomes too low use a mapper that takes base quality in account AND is able to ignore adapter sequence (and even then, you could try mapping on both croppedand uncropped data to see which is the best)","title":"MultiQC : grouping multiple reports"},{"location":"day1/server_login.html","text":"To conduct the practicals of this course we will be using a dedicated computer cluster. This matches the reality of most NGS workflows, which cannot be realized on a single machine. To interact with this cluster, you will have to login to a distant head node . From there you will be able to distribute your computational tasks to the cluster using a job scheduler called Slurm. This page will cover our first contact with the distant cluster. More precisely, you will learn how to : connect to the server use the command line to perform basic operations on the head node exchange files between the server and your own machine submit job to the cluster Note If you are doing this course on your own, then the distant server will not be available. Feel free to ignore or adapt any of the following steps to your own situation. Connect to the server Say you want to connect to cluster with adress xx.xx.xx.xx and your login is login . Warning If you are doing this course with a teacher, use the link, login and password provided before or during the course. The first step will be to open a terminal Mac Open the application Xterm Linux Open a new terminal Windows Open the application mobaXterm (or any ssh-enabling terminal aplpication you prefer) On mobaXterm, click on \u201cStart a local Terminal\u201d In the terminal type the following command: ssh login@xx.xx.xx.xx When prompted for your password, type it and press Enter. There is no cursor or \u2018\u25cf\u2019 character appearing while you type your password. This is normal. After a few seconds, you should be logged into the head node and ready to begin. Using command line on the cluster Now that you are in the head node, time to get acquainted with your environment and to prepare the upcoming practicals. we will also use this as a short introduction/reminder on UNIX command line. You can also refer to this nice Linux Command Line Cheat Sheet . At any time, you can get the location your terminal is currently at by typing: pwd When you start a session on a distant computer, you are placed in your home directory. So the cluster should return something like: /shared/home/<login> creating a directory Use the command line to create a repository called day1 where you will put all materials relating to this first day. Answer mkdir day1 Move to that directory Answer cd day1 The directory /shared/home/SHARED/ contains data and solutions for most practicals. Check the content of that directory. Answer ls /shared/home/SHARED/ Copy the script fastqc_Liu2015_SRR1272187_1.sh from /shared/home/SHARED/Solutions/Liu2015 into your current directory. Answer cp /shared/home/SHARED/Solutions/Liu2015/fastqc_Liu2015_SRR1272187_1.sh . Print the content of this script to the screen. Answer more fastqc_Liu2015_SRR1272187_1.sh output: PLACEHOLDER creating and editing a file To edit files on the distant server, we will use the command line editor nano . It is far from the most complete or efficient one, but it can be found on most servers and is arguably among the easiest to start with. Note Alternatively, feel free to use any other CLI editor you prefer, such as vi . To start editing a file named test.txt , type : nano test.txt You will be taken to the nano interface : Type in your favorite movie quote, and then exit by pressing Ctrl+x , and then y and Enter when prompted to save the modifications you just made. You can check that your modifications were saved by typing more test.txt Exchanging files with the server Whether you want to transfer some data to the cluster or retrieve the results of your latest computation, it is important to be able to exchange files with the distant server. There exists several alternatives, depending on your platform and preferences command line We will use scp . To copy a file from the server to your machine, use this syntax on a terminal in your local machine (open a new terminal if necessary). scp <login>@<server-adress>:/path/to/file/on/server/file.txt /local/destination/ For example, to copy the file test.txt you just created in the folder day1/ , to your current (local) working directory : scp login@xx.xx.xx.xx:~/day1/file.txt . here ~ will be interpreted as your home directory. This is useful and time-saving shorthand. To copy a file from your machine to the server: scp /path/to/file/local/file.txt <login>@<server-adress>:/destination/on/server/ graphical alternative There exist nice and free graphical software, such as filezilla to help you manage exchanges with the distant server. Feel free to install and experiment with it during the break. mobaXterm If you are using mobaXterm, the left panel should provide a graphical SFTP browser in the left sidebar which allows you to browse and drag and drop files directly from/to the remote server. The computing cluster The computing cluster follows an architecture that enables several users to distribute computational tasks among several machines which share a number of ressources, such as a common file system. Users do not access each machine individually, but rather connect to a head node . From there, they can interact with the cluster using the job scheduler (here slurm). The job scheduler role is to manage where and how to run the jobs of all users, such that waiting time is minimized and resource usage is optimized. Warning Everyone is connected to the same head node. Do not perform compute-intensive tasks on it or you will slow down everyone. Jobs can be submitted to the compute cluster using sbatch scripts , which contains 2 parts : informations for the job scheduler: how much RAM / CPUs do I need ? where to write the logs of my job ? bash commands corresponding to your task But an example is worth a thousand words : #!/usr/bin/bash #SBATCH --job-name=test #SBATCH --time=00:30:00 #SBATCH --cpus-per-task=1 #SBATCH --mem=1G #SBATCH -o test_log.o echo \"looking at the size of the elements of /shared/home/SHARED/\" sleep 10 # making the script wait for 10 seconds - this is just so we can see it later on. # `du` is a command that returns the size of a folder structure. du -h -d 2 /shared/home/SHARED/ The lines beginning by #SBATCH specify options to the job scheduler: #SBATCH --job-name=test : the job name #SBATCH --time=00:30:00 : time reserved for the job : 30min. #SBATCH --cpus-per-task=1 : cpus for the job #SBATCH --mem=1G : memory for the job #SBATCH -o test_log.o : file to write output or error messages Warning Your job will fail as soon as it takes more time or RAM than requested. Copy this script inside a new file named mySbatchScript.sh , then submit it to the job scheduler using : sbatch mySbatchScript.sh Afterward, use the command squeue to monitor the jobs submitted to the cluster. Locate your job and wait for it to be accepted ( RUNNING status), and then to complete (the job disappears from the output of squeue ). Check the output of your job in the output file. Note When there are a lot of jobs, squeue -u <username> will limit the list to your jobs only Advanced cluster usage : job array Oftentime we have to repeat a similar analysis on a number of files, or for a number of different parameters. Rather than writing each sbatch script individually, we can rely on job arrays to facilitate our task. Say you want to execute a command, on 10 files (for example, map the reads of 10 samples). You first create a file containing the name of your files (one per line); let\u2019s call it readFiles.txt . Then, you write an sbatch array job script: #!/usr/bin/bash #SBATCH --job-name=test_array #SBATCH --time=00:30:00 #SBATCH --cpus-per-task=1 #SBATCH --mem=1G #SBATCH -o test_array_log.%a.o #SBATCH --array 1-10%5 echo \"job array id\" $SLURM_ARRAY_TASK_ID # sed -n <X>p <file> : retrieve line <X> of file # so the next line grabs the file name corresponding to our job array task id and store it in the variable ReadFileName ReadFileName = ` sed -n ${ SLURM_ARRAY_TASK_ID } p readFiles.txt ` # here we would put the mapping command or whatever echo $ReadFileName Some things have changed compared to the previous sbatch script : #SBATCH --array 1-10%5 : will spawn independent tasks with ids from 1 to 10, and will manage them so that at most 5 run at the same time. #SBATCH -o test_array_log.%a.o : the %a will take the value of the array task id. So we will have 1 log file per task (so 10 files). $SLURM_ARRAY_TASK_ID : changes value between the different tasks. This is what we use to execute the same script on different files (using sed -n ${SLURM_ARRAY_TASK_ID}p )","title":"Server login + unix fresh up"},{"location":"day1/server_login.html#connect-to-the-server","text":"Say you want to connect to cluster with adress xx.xx.xx.xx and your login is login . Warning If you are doing this course with a teacher, use the link, login and password provided before or during the course. The first step will be to open a terminal Mac Open the application Xterm Linux Open a new terminal Windows Open the application mobaXterm (or any ssh-enabling terminal aplpication you prefer) On mobaXterm, click on \u201cStart a local Terminal\u201d In the terminal type the following command: ssh login@xx.xx.xx.xx When prompted for your password, type it and press Enter. There is no cursor or \u2018\u25cf\u2019 character appearing while you type your password. This is normal. After a few seconds, you should be logged into the head node and ready to begin.","title":"Connect to the server"},{"location":"day1/server_login.html#using-command-line-on-the-cluster","text":"Now that you are in the head node, time to get acquainted with your environment and to prepare the upcoming practicals. we will also use this as a short introduction/reminder on UNIX command line. You can also refer to this nice Linux Command Line Cheat Sheet . At any time, you can get the location your terminal is currently at by typing: pwd When you start a session on a distant computer, you are placed in your home directory. So the cluster should return something like: /shared/home/<login>","title":"Using command line on the cluster"},{"location":"day1/server_login.html#creating-a-directory","text":"Use the command line to create a repository called day1 where you will put all materials relating to this first day. Answer mkdir day1 Move to that directory Answer cd day1 The directory /shared/home/SHARED/ contains data and solutions for most practicals. Check the content of that directory. Answer ls /shared/home/SHARED/ Copy the script fastqc_Liu2015_SRR1272187_1.sh from /shared/home/SHARED/Solutions/Liu2015 into your current directory. Answer cp /shared/home/SHARED/Solutions/Liu2015/fastqc_Liu2015_SRR1272187_1.sh . Print the content of this script to the screen. Answer more fastqc_Liu2015_SRR1272187_1.sh output: PLACEHOLDER","title":"creating a directory"},{"location":"day1/server_login.html#creating-and-editing-a-file","text":"To edit files on the distant server, we will use the command line editor nano . It is far from the most complete or efficient one, but it can be found on most servers and is arguably among the easiest to start with. Note Alternatively, feel free to use any other CLI editor you prefer, such as vi . To start editing a file named test.txt , type : nano test.txt You will be taken to the nano interface : Type in your favorite movie quote, and then exit by pressing Ctrl+x , and then y and Enter when prompted to save the modifications you just made. You can check that your modifications were saved by typing more test.txt","title":"creating and editing a file"},{"location":"day1/server_login.html#exchanging-files-with-the-server","text":"Whether you want to transfer some data to the cluster or retrieve the results of your latest computation, it is important to be able to exchange files with the distant server. There exists several alternatives, depending on your platform and preferences command line We will use scp . To copy a file from the server to your machine, use this syntax on a terminal in your local machine (open a new terminal if necessary). scp <login>@<server-adress>:/path/to/file/on/server/file.txt /local/destination/ For example, to copy the file test.txt you just created in the folder day1/ , to your current (local) working directory : scp login@xx.xx.xx.xx:~/day1/file.txt . here ~ will be interpreted as your home directory. This is useful and time-saving shorthand. To copy a file from your machine to the server: scp /path/to/file/local/file.txt <login>@<server-adress>:/destination/on/server/ graphical alternative There exist nice and free graphical software, such as filezilla to help you manage exchanges with the distant server. Feel free to install and experiment with it during the break. mobaXterm If you are using mobaXterm, the left panel should provide a graphical SFTP browser in the left sidebar which allows you to browse and drag and drop files directly from/to the remote server.","title":"Exchanging files with the server"},{"location":"day1/server_login.html#the-computing-cluster","text":"The computing cluster follows an architecture that enables several users to distribute computational tasks among several machines which share a number of ressources, such as a common file system. Users do not access each machine individually, but rather connect to a head node . From there, they can interact with the cluster using the job scheduler (here slurm). The job scheduler role is to manage where and how to run the jobs of all users, such that waiting time is minimized and resource usage is optimized. Warning Everyone is connected to the same head node. Do not perform compute-intensive tasks on it or you will slow down everyone. Jobs can be submitted to the compute cluster using sbatch scripts , which contains 2 parts : informations for the job scheduler: how much RAM / CPUs do I need ? where to write the logs of my job ? bash commands corresponding to your task But an example is worth a thousand words : #!/usr/bin/bash #SBATCH --job-name=test #SBATCH --time=00:30:00 #SBATCH --cpus-per-task=1 #SBATCH --mem=1G #SBATCH -o test_log.o echo \"looking at the size of the elements of /shared/home/SHARED/\" sleep 10 # making the script wait for 10 seconds - this is just so we can see it later on. # `du` is a command that returns the size of a folder structure. du -h -d 2 /shared/home/SHARED/ The lines beginning by #SBATCH specify options to the job scheduler: #SBATCH --job-name=test : the job name #SBATCH --time=00:30:00 : time reserved for the job : 30min. #SBATCH --cpus-per-task=1 : cpus for the job #SBATCH --mem=1G : memory for the job #SBATCH -o test_log.o : file to write output or error messages Warning Your job will fail as soon as it takes more time or RAM than requested. Copy this script inside a new file named mySbatchScript.sh , then submit it to the job scheduler using : sbatch mySbatchScript.sh Afterward, use the command squeue to monitor the jobs submitted to the cluster. Locate your job and wait for it to be accepted ( RUNNING status), and then to complete (the job disappears from the output of squeue ). Check the output of your job in the output file. Note When there are a lot of jobs, squeue -u <username> will limit the list to your jobs only","title":"The computing cluster"},{"location":"day1/server_login.html#advanced-cluster-usage-job-array","text":"Oftentime we have to repeat a similar analysis on a number of files, or for a number of different parameters. Rather than writing each sbatch script individually, we can rely on job arrays to facilitate our task. Say you want to execute a command, on 10 files (for example, map the reads of 10 samples). You first create a file containing the name of your files (one per line); let\u2019s call it readFiles.txt . Then, you write an sbatch array job script: #!/usr/bin/bash #SBATCH --job-name=test_array #SBATCH --time=00:30:00 #SBATCH --cpus-per-task=1 #SBATCH --mem=1G #SBATCH -o test_array_log.%a.o #SBATCH --array 1-10%5 echo \"job array id\" $SLURM_ARRAY_TASK_ID # sed -n <X>p <file> : retrieve line <X> of file # so the next line grabs the file name corresponding to our job array task id and store it in the variable ReadFileName ReadFileName = ` sed -n ${ SLURM_ARRAY_TASK_ID } p readFiles.txt ` # here we would put the mapping command or whatever echo $ReadFileName Some things have changed compared to the previous sbatch script : #SBATCH --array 1-10%5 : will spawn independent tasks with ids from 1 to 10, and will manage them so that at most 5 run at the same time. #SBATCH -o test_array_log.%a.o : the %a will take the value of the array task id. So we will have 1 log file per task (so 10 files). $SLURM_ARRAY_TASK_ID : changes value between the different tasks. This is what we use to execute the same script on different files (using sed -n ${SLURM_ARRAY_TASK_ID}p )","title":"Advanced cluster usage : job array"},{"location":"day1/trimming.html","text":"Following a QC analysis on sequencing results, one could detect stretches of low quality bases along reads, or a contamination by adapter sequence. Depending on your application and the sofware you use for mapping, you may have to remove these bad quality / spurious sequences out of your data. During this block, you will learn to : trim your data with trimmomatic Material Download the presentation Trimmomatic website to trim or not to trim ? If the data will be used to perform transcriptome assembly, or variant analysis, then it must be trimmed . In contrast, for applications based on counting reads, such as Differential Expression analysis , most aligners, such as STAR , HISAT2 , salmon , kallisto can handle bad quality sequences and adapter content by performing \u201csoft-clipping\u201d on reads, and consequently they usually do not need trimming. In fact, hard trimming can be detrimental to the number of successfully quantified reads [ William et al. 2016 ]. Nevertheless, they usually recommend to perform some amount of soft trimming ( eg. kallisto , salmon ). If possible, we recommend to perform the mapping for both the raw data and the trimmed one, in order to compare the results for both and choose the best. trimming with Trimmomatic The trimmomatic website gives very good examples of their software usage for both paired-end ( PE ) and single-end ( SE ) reads. We recommend you read their quick-start section attentively. Task : Conduct a soft trimming on the Liu2015 data. Extra (if you have the time) : run a QC analysis on your trimmmed reads and compare with the raw ones. Important notes : Adapter sequences can be found in /home/SHARED/DATA/adapters/ Trimmomatic RAM requirements : Trimmomatic time requirements : ~ 10 min/ read file Trimmomatic script The Liu2015 dataset has paired-end reads and we have to take that into account during trimming. For a soft-trimming, we chose the following options : SLIDINGWINDOW:4:20 Perform a sliding window trimming, cutting once the average quality within the window falls below a threshold. 4 : windowSize: specifies the number of bases to average across 20 : requiredQuality: specifies the average quality required. ILLUMINACLIP:/shared/home/SHARED/DATA/adapters/TruSeq3-PE.fa:2:30:10 Cut adapter and other illumina-specific sequences from the read. Cut adapter and other illumina-specific sequences from the read. 2 : seedMismatches: specifies the maximum mismatch count which will still allow a full match to be performed 30 : palindromeClipThreshold: specifies how accurate the match between the two \u2018adapter ligated\u2019 reads must be for PE palindrome read alignment. 10 : simpleClipThreshold: specifies how accurate the match between any adapter etc. sequence must be against a read. Here is a script for a single sample : #!/bin/bash #SBATCH --job-name=trim #SBATCH --time=01:00:00 #SBATCH --cpus-per-task=4 #SBATCH --mem-per-cpu=4G #SBATCH -o trim.o #SBATCH -e trim.e ml trimmomatic dataDIR = /shared/home/SHARED/DATA/Liu2015 trimmomatic = \"/easybuild/apps/Trimmomatic/0.36-Java-1.7.0_80/trimmomatic-0.36.jar\" outDIR = Liu2015_trimmed_reads mkdir -p $outDIR $trimmomatic PE -threads 4 -phred33 \\ $dataDIR /SRR1272187_1.fastq.gz \\ $dataDIR /SRR1272187_2.fastq.gz \\ $outDIR /SRR1272187_NFLV_trimmed_paired_1.fastq $outDIR /SRR1272187_NFLV_trimmed_unpaired_1.fastq \\ $outDIR /SRR1272187_NFLV_trimmed_paired_2.fastq $outDIR /SRR1272187_NFLV_trimmed_unpaired_2.fastq \\ SLIDINGWINDOW:4:20 ILLUMINACLIP:/adapters/TruSeq3-PE.fa:2:30:10 ## compressing the resulting fastq files to save some space. gzip $outDIR /SRR1272187_NFLV_trimmed_paired_1.fastq gzip $outDIR /SRR1272187_NFLV_trimmed_unpaired_1.fastq gzip $outDIR /SRR1272187_NFLV_trimmed_paired_2.fastq gzip $outDIR /SRR1272187_NFLV_trimmed_unpaired_2.fastq","title":"Sequence trimming"},{"location":"day1/trimming.html#material","text":"Download the presentation Trimmomatic website","title":"Material"},{"location":"day1/trimming.html#to-trim-or-not-to-trim","text":"If the data will be used to perform transcriptome assembly, or variant analysis, then it must be trimmed . In contrast, for applications based on counting reads, such as Differential Expression analysis , most aligners, such as STAR , HISAT2 , salmon , kallisto can handle bad quality sequences and adapter content by performing \u201csoft-clipping\u201d on reads, and consequently they usually do not need trimming. In fact, hard trimming can be detrimental to the number of successfully quantified reads [ William et al. 2016 ]. Nevertheless, they usually recommend to perform some amount of soft trimming ( eg. kallisto , salmon ). If possible, we recommend to perform the mapping for both the raw data and the trimmed one, in order to compare the results for both and choose the best.","title":"to trim or not to trim ?"},{"location":"day1/trimming.html#trimming-with-trimmomatic","text":"The trimmomatic website gives very good examples of their software usage for both paired-end ( PE ) and single-end ( SE ) reads. We recommend you read their quick-start section attentively. Task : Conduct a soft trimming on the Liu2015 data. Extra (if you have the time) : run a QC analysis on your trimmmed reads and compare with the raw ones. Important notes : Adapter sequences can be found in /home/SHARED/DATA/adapters/ Trimmomatic RAM requirements : Trimmomatic time requirements : ~ 10 min/ read file Trimmomatic script The Liu2015 dataset has paired-end reads and we have to take that into account during trimming. For a soft-trimming, we chose the following options : SLIDINGWINDOW:4:20 Perform a sliding window trimming, cutting once the average quality within the window falls below a threshold. 4 : windowSize: specifies the number of bases to average across 20 : requiredQuality: specifies the average quality required. ILLUMINACLIP:/shared/home/SHARED/DATA/adapters/TruSeq3-PE.fa:2:30:10 Cut adapter and other illumina-specific sequences from the read. Cut adapter and other illumina-specific sequences from the read. 2 : seedMismatches: specifies the maximum mismatch count which will still allow a full match to be performed 30 : palindromeClipThreshold: specifies how accurate the match between the two \u2018adapter ligated\u2019 reads must be for PE palindrome read alignment. 10 : simpleClipThreshold: specifies how accurate the match between any adapter etc. sequence must be against a read. Here is a script for a single sample : #!/bin/bash #SBATCH --job-name=trim #SBATCH --time=01:00:00 #SBATCH --cpus-per-task=4 #SBATCH --mem-per-cpu=4G #SBATCH -o trim.o #SBATCH -e trim.e ml trimmomatic dataDIR = /shared/home/SHARED/DATA/Liu2015 trimmomatic = \"/easybuild/apps/Trimmomatic/0.36-Java-1.7.0_80/trimmomatic-0.36.jar\" outDIR = Liu2015_trimmed_reads mkdir -p $outDIR $trimmomatic PE -threads 4 -phred33 \\ $dataDIR /SRR1272187_1.fastq.gz \\ $dataDIR /SRR1272187_2.fastq.gz \\ $outDIR /SRR1272187_NFLV_trimmed_paired_1.fastq $outDIR /SRR1272187_NFLV_trimmed_unpaired_1.fastq \\ $outDIR /SRR1272187_NFLV_trimmed_paired_2.fastq $outDIR /SRR1272187_NFLV_trimmed_unpaired_2.fastq \\ SLIDINGWINDOW:4:20 ILLUMINACLIP:/adapters/TruSeq3-PE.fa:2:30:10 ## compressing the resulting fastq files to save some space. gzip $outDIR /SRR1272187_NFLV_trimmed_paired_1.fastq gzip $outDIR /SRR1272187_NFLV_trimmed_unpaired_1.fastq gzip $outDIR /SRR1272187_NFLV_trimmed_paired_2.fastq gzip $outDIR /SRR1272187_NFLV_trimmed_unpaired_2.fastq","title":"trimming with Trimmomatic"},{"location":"day1/welcome.html","text":"If working independently If you are doing this course independently, you can skip this part. Welcome to the course","title":"Welcome"},{"location":"day2/DE.html","text":"Once the reads have been mapped and counted, one can assess the differential expression of genes between different conditions. During this lesson, you will learn to: describe the different steps of data normalization and modelization commonly used for RNAseq data detect significantly differentially expressed genes using either edgeR or DESeq2 perform downstream analysis to over-representated gene sets (such as GO terms or reactome pathways) Material Download the presentation Rstudio website edgeR user\u2019s guide DESeq2 vignette Connexion to the Rstudio server Note This step is intended only for usrs who attend the course with a teacher. Otherwise you will have to rely on your own installation of Rstudio. The analysis of the data once reads have been counted will be done on a Rstudio instance, using the language R and some relevant Bioconductor libraries. As you start your session on the Rstudio server, please make sure that you know where your data is situated with respect to your working directory (use getwd() and setwd() to respectively : know what your working is, and change it). Differential Expression Inference Use either of these libraries to conduct a differential expression analysis on the Ruhland2016 and/or Liu2015 dataset. You can find the expression matrices on the server at: /shared/home/SHARED/Solutions/Ruhland2016/... and /shared/home/SHARED/Solutions/Liu2015/ Or you may download them : Liu2015 count matrix Ruhland2016 count matrix Note Generally, users find the syntax and workflow of DESeq2 easier for getting started If you have the time, conduct a differential expression analysis using both DESeq2 and edgeR Follow the vignettes/user\u2019s guide! They are the most up-to-date and generally contains everything a newcomer might need, including worked-out examples in the case of edgeR. DESeq2 DESeq2 vignette read in the data # setup library ( DESeq2 ) library ( ggplot2 ) #setwd(\"/shared/home/user01/Ruhland2016/\") # reading the counts files - adapt the file path to your situation raw_counts <- read.table ( '.../fC_all.counts' , skip = 1 , sep = \"\\t\" , header = T ) # setting up row names as ensembl gene ids row.names ( raw_counts ) = raw_counts $ Geneid # removing these first columns to keep only the sample counts raw_counts = raw_counts [ , -1 : -6 ] # changing colomn names names ( raw_counts ) = gsub ( '_.*' , '' , names ( raw_counts ) ) # some checking of what we just read head ( raw_counts ); tail ( raw_counts ); dim ( raw_counts ) colSums ( raw_counts ) # total number of counted reads per sample preprocessing # setting up the model treatment <- c ( rep ( \"EtOH\" , 3 ), rep ( \"TAM\" , 3 )) colData <- data.frame ( treatment , row.names = colnames ( raw_counts )) colData # creating the DESeq data object dds <- DESeqDataSetFromMatrix ( countData = raw_counts , colData = colData , design = ~ treatment ) dim ( dds ) ## filter low count genes . Here the filter is to have at least 2 samples where there is a least 5 reads idx <- rowSums ( counts ( dds , normalized = FALSE ) >= 5 ) >= 2 dds.f <- dds [ idx , ] dim ( dds.f ) # we go from 46078 to 18010 genes Around 18k genes pass our minimum expression threshold. estimate dipesersion / model fitting # we perform the estimation of dispersions dds.f <- DESeq ( dds.f ) # we plot the estimate of the dispersions # * black dot : raw # * red dot : local trend # * blue : corrected plotDispEsts ( dds.f ) # extracting results for the treatment versus control contrast res <- results ( dds.f ) This plot is not easy to interpret. It represents the amount of dispersion at different levels of expression. It is directly linked to our ability to detect differential expression. Here it Looks about normal compared to many other RNAseq experiment : the dispersion is comparatively larger for lowly expressed genes. looking at the results # adds estimate of the LFC the results table. # This shrunk logFC estimate is more robust than the raw value head ( coef ( dds.f )) # the second column corresponds to the difference between the 2 conditions res.lfc <- lfcShrink ( dds.f , coef = 2 , res = res ) #plotting to see the difference. par ( mfrow = c ( 2 , 1 )) DESeq2 :: plotMA ( res ) DESeq2 :: plotMA ( res.lfc ) # -> with shrinkage, the significativeness and logFC are more consistent par () Without the shrinkage, we can see that for low counts we can see a high log-fold change but non significant (ie. we see a large difference but with variance is also so high that this observation may be due to chance only). The shrinkage corrects this and the relationshipo between logFC and significance is smoother. # we apply the variance stabilising transformation to make the read counts comparable across libraries # (nb : this is not needed for DESeq DE analysis, but rather for the PCA on the data. This replace normal PCA scaling) vst.dds.f <- vst ( dds.f , blind = FALSE ) vst.dds.f.counts <- assay ( vst.dds.f ) plotPCA ( vst.dds.f , intgroup = c ( \"treatment\" )) The first axis (58% of the variance) seems linked to the grouping of interest. ## Volcano plot FDRthreshold = 0.01 logFCthreshold = 1.0 # add a column of NAs res.lfc $ diffexpressed <- \"NO\" # if log2Foldchange > 1 and pvalue < 0.01, set as \"UP\" res.lfc $ diffexpressed [ res.lfc $ log2FoldChange > logFCthreshold & res.lfc $ padj < FDRthreshold ] <- \"UP\" # if log2Foldchange < 1 and pvalue < 0.01, set as \"DOWN\" res.lfc $ diffexpressed [ res.lfc $ log2FoldChange < - logFCthreshold & res.lfc $ padj < FDRthreshold ] <- \"DOWN\" ggplot ( data = data.frame ( res.lfc ) , aes ( x = log2FoldChange , y = - log10 ( padj ) , col = diffexpressed ) ) + geom_point () + geom_vline ( xintercept = c ( - logFCthreshold , logFCthreshold ), col = \"red\" ) + geom_hline ( yintercept =- log10 ( FDRthreshold ), col = \"red\" ) + scale_color_manual ( values = c ( \"blue\" , \"grey\" , \"red\" )) table ( res.lfc $ diffexpressed ) DOWN NO UP 125 17647 240 library ( pheatmap ) topVarGenes <- head ( order ( rowVars ( vst.dds.f.counts ), decreasing = TRUE ), 20 ) mat <- vst.dds.f.counts [ topVarGenes , ] #scaled counts of the top genes mat <- mat - rowMeans ( mat ) # centering pheatmap ( mat ) # writing results write.csv ( res , 'Ruhland2016.DESeq2.results.csv' ) EdgeR edgeR user\u2019s guide read in the data library ( edgeR ) library ( ggplot2 ) # reading the counts files - adapt the file path to your situation raw_counts <- read.table ( '.../fC_all.counts' , skip = 1 , sep = \"\\t\" , header = T ) # setting up row names as ensembl gene ids row.names ( raw_counts ) = raw_counts $ Geneid # removing these first columns to keep only the sample counts raw_counts = raw_counts [ , -1 : -6 ] # some checking of what we just read head ( raw_counts ); tail ( raw_counts ); dim ( raw_counts ) colSums ( raw_counts ) # total number of counted reads per sample edgeR object preprocessing # setting up the model # -> the first 3 samples form an group, the 3 remaining are the other group treatment <- c ( rep ( 0 , 3 ), rep ( 1 , 3 )) dge.f.design <- model.matrix ( ~ treatment ) # creating the edgeR DGE object dge.all <- DGEList ( counts = raw_counts , group = treatment ) #filtering by expression level. See ?filterByExpr for details keep <- filterByExpr ( dge.all ) dge.f <- dge.all [ keep , keep.lib.sizes = FALSE ] table ( keep ) keep FALSE TRUE 30999 15079 Around 15k genes are sufficiently expressed to be retained. #normalization dge.f <- calcNormFactors ( dge.f ) dge.f $ samples Each sample has been associated with a normalization factor. edgeR model fitting # estimate of the dispersion dge.f <- estimateDisp ( dge.f , dge.f.design , robust = T ) plotBCV ( dge.f ) This plot is not easy to interpret. It represents the amount of biological variation at different levels of expression. It is directly linked to our ability to detect differential expression. Here it Looks about normal compared to many other RNAseq experiment : the variation is comparatively larger for lowly expressed genes. # testing for differential expression. #This method is recommended whne you only have 2 groups to compare dge.f.et <- exactTest ( dge.f ) topTags ( dge.f.et ) # printing the genes where the p-value of differential expression if the lowest Comparison of groups: 1-0 logFC logCPM PValue FDR ENSMUSG00000050272 -8.522762 4.988067 2.554513e-28 3.851950e-24 ENSMUSG00000075014 3.890079 5.175181 2.036909e-25 1.535728e-21 ENSMUSG00000009185 3.837786 6.742422 1.553964e-22 7.810743e-19 ENSMUSG00000075015 3.778523 3.274463 2.106799e-22 7.942107e-19 ENSMUSG00000028339 -5.692069 6.372980 4.593720e-16 1.385374e-12 ENSMUSG00000040111 -2.141221 6.771538 4.954522e-15 1.245154e-11 ENSMUSG00000041695 4.123972 1.668247 6.057909e-15 1.304960e-11 ENSMUSG00000072941 3.609170 7.080257 1.807618e-14 3.407135e-11 ENSMUSG00000000120 -6.340146 6.351489 2.507019e-14 4.200371e-11 ENSMUSG00000034981 3.727969 5.244841 3.934957e-14 5.933521e-11 # see how many genes are DE summary ( decideTests ( dge.f.et , p.value = 0.01 )) # let's use 0.01 as a threshold 1-0 Down 110 NotSig 14770 Up 199 The comparision is 1-0, so \u201cUp\u201d, corresponds to a higher in group 1 (EtOH for us) compared to group 0 (TAM). edgeR looking at differentially expressed genes ## plot all the logFCs versus average count size. Significantly DE genes are colored par ( mfrow = c ( 1 , 1 )) plotMD ( dge.f.et ) # lines at a log2FC of 1/-1, corresponding to a shift in expression of x2 abline ( h = c ( -1 , 1 ), col = \"blue\" ) ## Volcano plot allGenes = topTags ( dge.f.et , n = nrow ( dge.f.et $ table ) ) $ table FDRthreshold = 0.01 logFCthreshold = 1.0 # add a column of NAs allGenes $ diffexpressed <- \"NO\" # if log2Foldchange > 1 and pvalue < 0.01, set as \"UP\" allGenes $ diffexpressed [ allGenes $ logFC > logFCthreshold & allGenes $ FDR < FDRthreshold ] <- \"UP\" # if log2Foldchange < 1 and pvalue < 0.01, set as \"DOWN\" allGenes $ diffexpressed [ allGenes $ logFC < - logFCthreshold & allGenes $ FDR < FDRthreshold ] <- \"DOWN\" ggplot ( data = allGenes , aes ( x = logFC , y = - log10 ( FDR ) , col = diffexpressed ) ) + geom_point () + geom_vline ( xintercept = c ( - logFCthreshold , logFCthreshold ), col = \"red\" ) + geom_hline ( yintercept =- log10 ( FDRthreshold ), col = \"red\" ) + scale_color_manual ( values = c ( \"blue\" , \"grey\" , \"red\" )) ## writing the table of results write.csv ( allGenes , 'Ruhland2016.edgeR.results.csv' ) edgeR extra stuff # how to extract log CPM logcpm <- cpm ( dge.f , prior.count = 2 , log = TRUE ) # there is another fitting method reliying on quasi likelihood, which is useful when the model is more complex (ie. more than 1 factor with 2 levels) dge.f.QLfit <- glmQLFit ( dge.f , dge.f.design ) dge.f.qlt <- glmQLFTest ( dge.f.QLfit , coef = 2 ) # you can see the results relatively different. The order of genes changes a bit, and the p-values are more profoundly affected topTags ( dge.f.et ) topTags ( dge.f.qlt ) ## let's see how much the two methods agree: par ( mfrow = c ( 1 , 2 )) plot ( dge.f.et $ table $ logFC , dge.f.qlt $ table $ logFC , xlab = 'exact test logFC' , ylab = 'quasi-likelihood test logFC' ) print ( paste ( 'logFC pearson correlation coefficient :' , cor ( dge.f.et $ table $ logFC , dge.f.qlt $ table $ logFC ) ) ) plot ( log10 ( dge.f.et $ table $ PValue ), log10 ( dge.f.qlt $ table $ PValue ) , xlab = 'exact test p-values (log10)' , ylab = 'quasi-likelihood test p-values (log10)' ) print ( paste ( \"P-values spearman correlation coefficient\" , cor ( log10 ( dge.f.et $ table $ PValue ), log10 ( dge.f.qlt $ table $ PValue ) , method = 'spearman' ))) \"logFC pearson correlation coefficient : 0.999997655536736\" \"P-values spearman correlation coefficient 0.993238670517236\" The logFC are highly correlated. FDRs show less correlation but their rank are higly correlated : they come in a very similar order. Downstream analysis : over-representation analysis Having lists of differentially expressed genes is quite interesting in itself, however when there is a large number of DE genes it can be interesting to map these results onto curated sets of genes associated to biological functions. We propose here to use clusterProfiler , which regroups several enrichment detection algorithm onto several databases. We recommend you get inspiration from their very nice vignette/e-book to perform your own analysis. The proposed correction will concern the results obtained with DESeq2 on the Ruhland2016 dataset. analysis with clusterProfiler We being by reading the results of the DE analysis. Adapt this to your own analsysis. Beware that edgeR and DESeq2 use different column names in their result tables (log2FoldChange/logFC , padj/FDR). library ( AnnotationHub ) library ( AnnotationDbi ) library ( clusterProfiler ) library ( ReactomePA ) library ( org.Mm.eg.db ) res = read.csv ( 'Ruhland2016.DESeq2.results.csv' , row.names = 1 ) #let's define significance as padj <0.01 & abs(lfc) > 1 res $ sig = abs ( res $ log2FoldChange ) > 1 & res $ padj < 0.01 table ( res $ sig ) Number of non-significant/significant genes FALSE TRUE 17639 365 Translating gene ENSEMBL names to their entrezID (this is what clusterProfiler uses), as well as Symbol (named used by most biologist). genes_universe <- bitr ( rownames ( res ), fromType = \"ENSEMBL\" , toType = c ( \"ENTREZID\" , \"SYMBOL\" ), OrgDb = \"org.Mm.eg.db\" ) head ( genes_universe ) #ENSEMBL ENTREZID SYMBOL #2 ENSMUSG00000033845 27395 Mrpl15 #4 ENSMUSG00000025903 18777 Lypla1 #5 ENSMUSG00000033813 21399 Tcea1 #7 ENSMUSG00000002459 58175 Rgs20 #8 ENSMUSG00000033793 108664 Atp6v1h #9 ENSMUSG00000025907 12421 Rb1cc1 dim ( genes_universe ) # 14708 3 length ( rownames ( dds.f )) # 18012 genes_DE <- bitr ( rownames ( res [ res $ sig == T ,]), fromType = \"ENSEMBL\" , toType = c ( \"ENTREZID\" , \"SYMBOL\" ), OrgDb = \"org.Mm.eg.db\" ) dim ( genes_DE ) # 354 3 # GO \"biological process (BP)\" enrichment ego_bp <- enrichGO ( gene = as.character ( unique ( genes_DE $ ENTREZID )), universe = as.character ( unique ( genes_universe $ ENTREZID )), OrgDb = org.Mm.eg.db , ont = \"BP\" , pAdjustMethod = \"BH\" , pvalueCutoff = 0.01 , qvalueCutoff = 0.05 , readable = TRUE ) head ( ego_bp ) dotplot ( ego_bp , showCategory = 20 ) # sample plot, but with adjusted p-value as x-axis #dotplot(ego_bp, x = \"p.adjust\", showCategory = 20) # Reactome pathways enrichment reactome.enrich <- enrichPathway ( gene = as.character ( unique ( genes_DE $ ENTREZID )), organism = \"mouse\" , pAdjustMethod = \"BH\" , qvalueCutoff = 0.01 , readable = T , universe = genes_universe $ ENTREZID ) dotplot ( reactome.enrich , x = \"p.adjust\" )","title":"Differential Expression Inference"},{"location":"day2/DE.html#material","text":"Download the presentation Rstudio website edgeR user\u2019s guide DESeq2 vignette","title":"Material"},{"location":"day2/DE.html#connexion-to-the-rstudio-server","text":"Note This step is intended only for usrs who attend the course with a teacher. Otherwise you will have to rely on your own installation of Rstudio. The analysis of the data once reads have been counted will be done on a Rstudio instance, using the language R and some relevant Bioconductor libraries. As you start your session on the Rstudio server, please make sure that you know where your data is situated with respect to your working directory (use getwd() and setwd() to respectively : know what your working is, and change it).","title":"Connexion to the Rstudio server"},{"location":"day2/DE.html#differential-expression-inference","text":"Use either of these libraries to conduct a differential expression analysis on the Ruhland2016 and/or Liu2015 dataset. You can find the expression matrices on the server at: /shared/home/SHARED/Solutions/Ruhland2016/... and /shared/home/SHARED/Solutions/Liu2015/ Or you may download them : Liu2015 count matrix Ruhland2016 count matrix Note Generally, users find the syntax and workflow of DESeq2 easier for getting started If you have the time, conduct a differential expression analysis using both DESeq2 and edgeR Follow the vignettes/user\u2019s guide! They are the most up-to-date and generally contains everything a newcomer might need, including worked-out examples in the case of edgeR.","title":"Differential Expression Inference"},{"location":"day2/DE.html#deseq2","text":"DESeq2 vignette read in the data # setup library ( DESeq2 ) library ( ggplot2 ) #setwd(\"/shared/home/user01/Ruhland2016/\") # reading the counts files - adapt the file path to your situation raw_counts <- read.table ( '.../fC_all.counts' , skip = 1 , sep = \"\\t\" , header = T ) # setting up row names as ensembl gene ids row.names ( raw_counts ) = raw_counts $ Geneid # removing these first columns to keep only the sample counts raw_counts = raw_counts [ , -1 : -6 ] # changing colomn names names ( raw_counts ) = gsub ( '_.*' , '' , names ( raw_counts ) ) # some checking of what we just read head ( raw_counts ); tail ( raw_counts ); dim ( raw_counts ) colSums ( raw_counts ) # total number of counted reads per sample preprocessing # setting up the model treatment <- c ( rep ( \"EtOH\" , 3 ), rep ( \"TAM\" , 3 )) colData <- data.frame ( treatment , row.names = colnames ( raw_counts )) colData # creating the DESeq data object dds <- DESeqDataSetFromMatrix ( countData = raw_counts , colData = colData , design = ~ treatment ) dim ( dds ) ## filter low count genes . Here the filter is to have at least 2 samples where there is a least 5 reads idx <- rowSums ( counts ( dds , normalized = FALSE ) >= 5 ) >= 2 dds.f <- dds [ idx , ] dim ( dds.f ) # we go from 46078 to 18010 genes Around 18k genes pass our minimum expression threshold. estimate dipesersion / model fitting # we perform the estimation of dispersions dds.f <- DESeq ( dds.f ) # we plot the estimate of the dispersions # * black dot : raw # * red dot : local trend # * blue : corrected plotDispEsts ( dds.f ) # extracting results for the treatment versus control contrast res <- results ( dds.f ) This plot is not easy to interpret. It represents the amount of dispersion at different levels of expression. It is directly linked to our ability to detect differential expression. Here it Looks about normal compared to many other RNAseq experiment : the dispersion is comparatively larger for lowly expressed genes. looking at the results # adds estimate of the LFC the results table. # This shrunk logFC estimate is more robust than the raw value head ( coef ( dds.f )) # the second column corresponds to the difference between the 2 conditions res.lfc <- lfcShrink ( dds.f , coef = 2 , res = res ) #plotting to see the difference. par ( mfrow = c ( 2 , 1 )) DESeq2 :: plotMA ( res ) DESeq2 :: plotMA ( res.lfc ) # -> with shrinkage, the significativeness and logFC are more consistent par () Without the shrinkage, we can see that for low counts we can see a high log-fold change but non significant (ie. we see a large difference but with variance is also so high that this observation may be due to chance only). The shrinkage corrects this and the relationshipo between logFC and significance is smoother. # we apply the variance stabilising transformation to make the read counts comparable across libraries # (nb : this is not needed for DESeq DE analysis, but rather for the PCA on the data. This replace normal PCA scaling) vst.dds.f <- vst ( dds.f , blind = FALSE ) vst.dds.f.counts <- assay ( vst.dds.f ) plotPCA ( vst.dds.f , intgroup = c ( \"treatment\" )) The first axis (58% of the variance) seems linked to the grouping of interest. ## Volcano plot FDRthreshold = 0.01 logFCthreshold = 1.0 # add a column of NAs res.lfc $ diffexpressed <- \"NO\" # if log2Foldchange > 1 and pvalue < 0.01, set as \"UP\" res.lfc $ diffexpressed [ res.lfc $ log2FoldChange > logFCthreshold & res.lfc $ padj < FDRthreshold ] <- \"UP\" # if log2Foldchange < 1 and pvalue < 0.01, set as \"DOWN\" res.lfc $ diffexpressed [ res.lfc $ log2FoldChange < - logFCthreshold & res.lfc $ padj < FDRthreshold ] <- \"DOWN\" ggplot ( data = data.frame ( res.lfc ) , aes ( x = log2FoldChange , y = - log10 ( padj ) , col = diffexpressed ) ) + geom_point () + geom_vline ( xintercept = c ( - logFCthreshold , logFCthreshold ), col = \"red\" ) + geom_hline ( yintercept =- log10 ( FDRthreshold ), col = \"red\" ) + scale_color_manual ( values = c ( \"blue\" , \"grey\" , \"red\" )) table ( res.lfc $ diffexpressed ) DOWN NO UP 125 17647 240 library ( pheatmap ) topVarGenes <- head ( order ( rowVars ( vst.dds.f.counts ), decreasing = TRUE ), 20 ) mat <- vst.dds.f.counts [ topVarGenes , ] #scaled counts of the top genes mat <- mat - rowMeans ( mat ) # centering pheatmap ( mat ) # writing results write.csv ( res , 'Ruhland2016.DESeq2.results.csv' )","title":"DESeq2"},{"location":"day2/DE.html#edger","text":"edgeR user\u2019s guide read in the data library ( edgeR ) library ( ggplot2 ) # reading the counts files - adapt the file path to your situation raw_counts <- read.table ( '.../fC_all.counts' , skip = 1 , sep = \"\\t\" , header = T ) # setting up row names as ensembl gene ids row.names ( raw_counts ) = raw_counts $ Geneid # removing these first columns to keep only the sample counts raw_counts = raw_counts [ , -1 : -6 ] # some checking of what we just read head ( raw_counts ); tail ( raw_counts ); dim ( raw_counts ) colSums ( raw_counts ) # total number of counted reads per sample edgeR object preprocessing # setting up the model # -> the first 3 samples form an group, the 3 remaining are the other group treatment <- c ( rep ( 0 , 3 ), rep ( 1 , 3 )) dge.f.design <- model.matrix ( ~ treatment ) # creating the edgeR DGE object dge.all <- DGEList ( counts = raw_counts , group = treatment ) #filtering by expression level. See ?filterByExpr for details keep <- filterByExpr ( dge.all ) dge.f <- dge.all [ keep , keep.lib.sizes = FALSE ] table ( keep ) keep FALSE TRUE 30999 15079 Around 15k genes are sufficiently expressed to be retained. #normalization dge.f <- calcNormFactors ( dge.f ) dge.f $ samples Each sample has been associated with a normalization factor. edgeR model fitting # estimate of the dispersion dge.f <- estimateDisp ( dge.f , dge.f.design , robust = T ) plotBCV ( dge.f ) This plot is not easy to interpret. It represents the amount of biological variation at different levels of expression. It is directly linked to our ability to detect differential expression. Here it Looks about normal compared to many other RNAseq experiment : the variation is comparatively larger for lowly expressed genes. # testing for differential expression. #This method is recommended whne you only have 2 groups to compare dge.f.et <- exactTest ( dge.f ) topTags ( dge.f.et ) # printing the genes where the p-value of differential expression if the lowest Comparison of groups: 1-0 logFC logCPM PValue FDR ENSMUSG00000050272 -8.522762 4.988067 2.554513e-28 3.851950e-24 ENSMUSG00000075014 3.890079 5.175181 2.036909e-25 1.535728e-21 ENSMUSG00000009185 3.837786 6.742422 1.553964e-22 7.810743e-19 ENSMUSG00000075015 3.778523 3.274463 2.106799e-22 7.942107e-19 ENSMUSG00000028339 -5.692069 6.372980 4.593720e-16 1.385374e-12 ENSMUSG00000040111 -2.141221 6.771538 4.954522e-15 1.245154e-11 ENSMUSG00000041695 4.123972 1.668247 6.057909e-15 1.304960e-11 ENSMUSG00000072941 3.609170 7.080257 1.807618e-14 3.407135e-11 ENSMUSG00000000120 -6.340146 6.351489 2.507019e-14 4.200371e-11 ENSMUSG00000034981 3.727969 5.244841 3.934957e-14 5.933521e-11 # see how many genes are DE summary ( decideTests ( dge.f.et , p.value = 0.01 )) # let's use 0.01 as a threshold 1-0 Down 110 NotSig 14770 Up 199 The comparision is 1-0, so \u201cUp\u201d, corresponds to a higher in group 1 (EtOH for us) compared to group 0 (TAM). edgeR looking at differentially expressed genes ## plot all the logFCs versus average count size. Significantly DE genes are colored par ( mfrow = c ( 1 , 1 )) plotMD ( dge.f.et ) # lines at a log2FC of 1/-1, corresponding to a shift in expression of x2 abline ( h = c ( -1 , 1 ), col = \"blue\" ) ## Volcano plot allGenes = topTags ( dge.f.et , n = nrow ( dge.f.et $ table ) ) $ table FDRthreshold = 0.01 logFCthreshold = 1.0 # add a column of NAs allGenes $ diffexpressed <- \"NO\" # if log2Foldchange > 1 and pvalue < 0.01, set as \"UP\" allGenes $ diffexpressed [ allGenes $ logFC > logFCthreshold & allGenes $ FDR < FDRthreshold ] <- \"UP\" # if log2Foldchange < 1 and pvalue < 0.01, set as \"DOWN\" allGenes $ diffexpressed [ allGenes $ logFC < - logFCthreshold & allGenes $ FDR < FDRthreshold ] <- \"DOWN\" ggplot ( data = allGenes , aes ( x = logFC , y = - log10 ( FDR ) , col = diffexpressed ) ) + geom_point () + geom_vline ( xintercept = c ( - logFCthreshold , logFCthreshold ), col = \"red\" ) + geom_hline ( yintercept =- log10 ( FDRthreshold ), col = \"red\" ) + scale_color_manual ( values = c ( \"blue\" , \"grey\" , \"red\" )) ## writing the table of results write.csv ( allGenes , 'Ruhland2016.edgeR.results.csv' ) edgeR extra stuff # how to extract log CPM logcpm <- cpm ( dge.f , prior.count = 2 , log = TRUE ) # there is another fitting method reliying on quasi likelihood, which is useful when the model is more complex (ie. more than 1 factor with 2 levels) dge.f.QLfit <- glmQLFit ( dge.f , dge.f.design ) dge.f.qlt <- glmQLFTest ( dge.f.QLfit , coef = 2 ) # you can see the results relatively different. The order of genes changes a bit, and the p-values are more profoundly affected topTags ( dge.f.et ) topTags ( dge.f.qlt ) ## let's see how much the two methods agree: par ( mfrow = c ( 1 , 2 )) plot ( dge.f.et $ table $ logFC , dge.f.qlt $ table $ logFC , xlab = 'exact test logFC' , ylab = 'quasi-likelihood test logFC' ) print ( paste ( 'logFC pearson correlation coefficient :' , cor ( dge.f.et $ table $ logFC , dge.f.qlt $ table $ logFC ) ) ) plot ( log10 ( dge.f.et $ table $ PValue ), log10 ( dge.f.qlt $ table $ PValue ) , xlab = 'exact test p-values (log10)' , ylab = 'quasi-likelihood test p-values (log10)' ) print ( paste ( \"P-values spearman correlation coefficient\" , cor ( log10 ( dge.f.et $ table $ PValue ), log10 ( dge.f.qlt $ table $ PValue ) , method = 'spearman' ))) \"logFC pearson correlation coefficient : 0.999997655536736\" \"P-values spearman correlation coefficient 0.993238670517236\" The logFC are highly correlated. FDRs show less correlation but their rank are higly correlated : they come in a very similar order.","title":"EdgeR"},{"location":"day2/DE.html#downstream-analysis-over-representation-analysis","text":"Having lists of differentially expressed genes is quite interesting in itself, however when there is a large number of DE genes it can be interesting to map these results onto curated sets of genes associated to biological functions. We propose here to use clusterProfiler , which regroups several enrichment detection algorithm onto several databases. We recommend you get inspiration from their very nice vignette/e-book to perform your own analysis. The proposed correction will concern the results obtained with DESeq2 on the Ruhland2016 dataset. analysis with clusterProfiler We being by reading the results of the DE analysis. Adapt this to your own analsysis. Beware that edgeR and DESeq2 use different column names in their result tables (log2FoldChange/logFC , padj/FDR). library ( AnnotationHub ) library ( AnnotationDbi ) library ( clusterProfiler ) library ( ReactomePA ) library ( org.Mm.eg.db ) res = read.csv ( 'Ruhland2016.DESeq2.results.csv' , row.names = 1 ) #let's define significance as padj <0.01 & abs(lfc) > 1 res $ sig = abs ( res $ log2FoldChange ) > 1 & res $ padj < 0.01 table ( res $ sig ) Number of non-significant/significant genes FALSE TRUE 17639 365 Translating gene ENSEMBL names to their entrezID (this is what clusterProfiler uses), as well as Symbol (named used by most biologist). genes_universe <- bitr ( rownames ( res ), fromType = \"ENSEMBL\" , toType = c ( \"ENTREZID\" , \"SYMBOL\" ), OrgDb = \"org.Mm.eg.db\" ) head ( genes_universe ) #ENSEMBL ENTREZID SYMBOL #2 ENSMUSG00000033845 27395 Mrpl15 #4 ENSMUSG00000025903 18777 Lypla1 #5 ENSMUSG00000033813 21399 Tcea1 #7 ENSMUSG00000002459 58175 Rgs20 #8 ENSMUSG00000033793 108664 Atp6v1h #9 ENSMUSG00000025907 12421 Rb1cc1 dim ( genes_universe ) # 14708 3 length ( rownames ( dds.f )) # 18012 genes_DE <- bitr ( rownames ( res [ res $ sig == T ,]), fromType = \"ENSEMBL\" , toType = c ( \"ENTREZID\" , \"SYMBOL\" ), OrgDb = \"org.Mm.eg.db\" ) dim ( genes_DE ) # 354 3 # GO \"biological process (BP)\" enrichment ego_bp <- enrichGO ( gene = as.character ( unique ( genes_DE $ ENTREZID )), universe = as.character ( unique ( genes_universe $ ENTREZID )), OrgDb = org.Mm.eg.db , ont = \"BP\" , pAdjustMethod = \"BH\" , pvalueCutoff = 0.01 , qvalueCutoff = 0.05 , readable = TRUE ) head ( ego_bp ) dotplot ( ego_bp , showCategory = 20 ) # sample plot, but with adjusted p-value as x-axis #dotplot(ego_bp, x = \"p.adjust\", showCategory = 20) # Reactome pathways enrichment reactome.enrich <- enrichPathway ( gene = as.character ( unique ( genes_DE $ ENTREZID )), organism = \"mouse\" , pAdjustMethod = \"BH\" , qvalueCutoff = 0.01 , readable = T , universe = genes_universe $ ENTREZID ) dotplot ( reactome.enrich , x = \"p.adjust\" )","title":"Downstream analysis : over-representation analysis"},{"location":"day2/counting.html","text":"Read counting refers to the quantification of an \u201cexpression level\u201d , or abundance, from reads mapped onto a reference genome/transcriptome. This expression level can take several forms, such as a count, or a fraction (RPKM/TPM), and concern different entities (exon, transcript, genes) depending on your biological application. During this lesson, you will learn to: differentiate between different levels of counting and their relevance for different questions perform read counting at the gene level for Differential Gene expression Material Download the presentation featureCounts website Read counting with featureCounts The featureCount website provides several useful command-line examples to get started. For more details on the algorithm behavior (with multi/overlapping reads for instance), you can refer to the package\u2019s User\u2019s guide (go to the read summarization chapter). Task : Decide which parameters are appropriate for counting reads from the Ruhland dataset. Assume you are interested in determining which genes are differentially expressed. Count the reads from one of your BAM files using featureCount How do the count compare to the counts from STAR ? featureCount : xxx RAM / bam featureCount : xxx cpu time / bam featureCounts script #!/usr/bin/bash #SBATCH --job-name=htseq #SBATCH --time=00:30:00 #SBATCH --cpus-per-task=1 #SBATCH --mem=4G #SBATCH -o htseq-count.%a.o #SBATCH -e htseq-count.%a.e G_GTF=/home/SHARED/DATA/Mus_musculus.GRCm38.99.gtf inFOLDER=${HOME}/Ruhland2016/STAR_Ruhland2016 outFOLDER=${HOME}/Ruhland2016/HTSQCOUNT_Ruhland2016 ml featureCounts mkdir -p $outFOLDER featureCounts -T 8 -a /home/SHARED/DATA/Mus_musculus.GRCm38.99.gtf -t exon -g gene_id -o featureCounts_Ruhland2016.counts.txt \\ inFOLDER/EtOH1_Aligned.sortedByCoord.out.bam \\ inFOLDER/EtOH2_Aligned.sortedByCoord.out.bam \\ inFOLDER/EtOH3_Aligned.sortedByCoord.out.bam \\ inFOLDER/TAM1_Aligned.sortedByCoord.out.bam \\ inFOLDER/TAM2_Aligned.sortedByCoord.out.bam \\ inFOLDER/TAM3_Aligned.sortedByCoord.out.bam comparison with STAR counts \u2026","title":"Read counting"},{"location":"day2/counting.html#material","text":"Download the presentation featureCounts website","title":"Material"},{"location":"day2/counting.html#read-counting-with-featurecounts","text":"The featureCount website provides several useful command-line examples to get started. For more details on the algorithm behavior (with multi/overlapping reads for instance), you can refer to the package\u2019s User\u2019s guide (go to the read summarization chapter). Task : Decide which parameters are appropriate for counting reads from the Ruhland dataset. Assume you are interested in determining which genes are differentially expressed. Count the reads from one of your BAM files using featureCount How do the count compare to the counts from STAR ? featureCount : xxx RAM / bam featureCount : xxx cpu time / bam featureCounts script #!/usr/bin/bash #SBATCH --job-name=htseq #SBATCH --time=00:30:00 #SBATCH --cpus-per-task=1 #SBATCH --mem=4G #SBATCH -o htseq-count.%a.o #SBATCH -e htseq-count.%a.e G_GTF=/home/SHARED/DATA/Mus_musculus.GRCm38.99.gtf inFOLDER=${HOME}/Ruhland2016/STAR_Ruhland2016 outFOLDER=${HOME}/Ruhland2016/HTSQCOUNT_Ruhland2016 ml featureCounts mkdir -p $outFOLDER featureCounts -T 8 -a /home/SHARED/DATA/Mus_musculus.GRCm38.99.gtf -t exon -g gene_id -o featureCounts_Ruhland2016.counts.txt \\ inFOLDER/EtOH1_Aligned.sortedByCoord.out.bam \\ inFOLDER/EtOH2_Aligned.sortedByCoord.out.bam \\ inFOLDER/EtOH3_Aligned.sortedByCoord.out.bam \\ inFOLDER/TAM1_Aligned.sortedByCoord.out.bam \\ inFOLDER/TAM2_Aligned.sortedByCoord.out.bam \\ inFOLDER/TAM3_Aligned.sortedByCoord.out.bam comparison with STAR counts \u2026","title":"Read counting with featureCounts"}]}
{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction to RNA-Seq: From quality control to pathway analysis","text":""},{"location":"#learning-outcomes","title":"Learning outcomes","text":""},{"location":"#general-learning-outcomes","title":"General learning outcomes","text":"<p>After this course, you will be able to:</p> <ul> <li>Describe advantages and pitfalls of RNA sequencing.</li> <li>Design your own experiment.</li> <li>Perform the downstream analysis using command line software   (QC, mapping, counting, differential expression analysis, pathway analysis, etc).</li> <li>Critically assess the quality of your results at each step of the downstream analysis.</li> <li>Detect significantly differentially-expressed genes between conditions.</li> </ul>"},{"location":"#learning-experiences","title":"Learning experiences","text":"<p>To reach the learning outcomes, we will use lectures and exercises.  During lectures, do not hesitate to ask any questions as we progress through the slides. During exercises, you are free to discuss with other participants. </p> <p>Exercises are provided with solutions. How you use them to your advantage is up to you.</p>"},{"location":"cheatsheets/","title":"Cheatsheets","text":"<p> Linux Command Line Cheat Sheet</p> <p>SLURM cluster Cheat Sheet</p> <p> Keyboards with some special characters highlighted</p>"},{"location":"course_schedule/","title":"Course schedule","text":""},{"location":"course_schedule/#day-1","title":"Day 1","text":"start end subject 9:00 9:15 Welcome 9:15 10:45 RNAseq - technologies and design 10:45 11:00 BREAK 11:00 11:30 Server login + unix fresh up 11:30 12:30 Quality control 12:30 13:30 LUNCH BREAK 13:30 14:30 Sequence Trimming and adapter removal 14:30 15:30 Reads mapping : indexing genome 15:30 15:45 BREAK 15:45 17:00 Reads mapping : mapping"},{"location":"course_schedule/#day-2","title":"Day 2","text":"start end subject 9:00 9:15 Recap of yesterday 9:15 10:30 Read counting 10:30 10:45 BREAK 10:30 12:30 Differential Expression Inference - theory  12:30 13:30 LUNCH BREAK 13:30 17:00 Differential Expression Inference - practice"},{"location":"precourse/","title":"Precourse preparations","text":"<p>On top of a thirst for knowledge, and a working Internet connection, here is what you will need for the course : </p>"},{"location":"precourse/#ngs","title":"NGS","text":"<p>As announced in the course registration webpage, we expect participants to already have a basic knowledge in Next Generation Sequencing (NGS) techniques. </p>"},{"location":"precourse/#unix","title":"UNIX","text":"<p>Practical knowledge of the UNIX command line is also required to be able to follow this course, given that that the tools used to process sequenced reads use this interface.</p> <p>If you are unsure about your capabilities or feel a bit rusty, we strongly recommend you spend some time practicing before the course : in our experience, the more comfortable you are with UNIX, the more you will be able to focus on the RNA-seq during the course, and the more you will gain from it.</p> <p>You may refer to the SIB\u2019s UNIX e-learning module</p>"},{"location":"precourse/#r","title":"R","text":"<p>A basic knowledge of the R language is required to perform most analytical steps after reads have been mapped and quantified : differential gene expression, gene set enrichment, over-representation analysis.</p> <p>If you are not familiar with R, we recommend the SIB First Steps with R course, or you can pick one among this list</p>"},{"location":"precourse/#software","title":"Software","text":"<p>To replicate the technical condition of today\u2019s real-life data analysis, we will perform our computations on a distant HPC cluster. To access it: </p> <ul> <li>macOS / Linux : you can use your pre-installed terminal.</li> <li>Windows : you should install a terminal which lets you do ssh (for instance mobaXterm). </li> </ul> <p>Additionally, a graphical client for file transfer to and from the distant server can be useful. MobaXterm integrates this functionality, so if you use it there is no need for additional software.  Otherwise, we recommend FileZilla.</p>"},{"location":"slides_notes/","title":"Slides notes","text":"<p>This document contains notes about the course slides.</p> <p>This is a somewhat internal document, so expect a fairly draft-ish and concise style.</p>"},{"location":"slides_notes/#01-overview","title":"01 Overview","text":"<p>slides 3 - 7</p> <ul> <li>the central dogma of molecular biology is know to be not so simple</li> <li>RNA is not only a messenger but may have an effect</li> <li>most of these elements interacts and regulates one-another </li> <li>alternative splicing in eukaryots adds a layer of possibilities to all this</li> <li>main takeaway maybe : measuring RNA is a proxy for protein levels, which is a proxy for protein activity , which is a proxy for the physiological state of the cell</li> </ul> <p>slides 8 - 9</p> <ul> <li>non-exhaustive list of sequencing possibilities </li> <li>https://liorpachter.wordpress.com/seq/</li> </ul> <p>slides 10-12</p> <ul> <li>RNAseq, the challenges</li> <li>slide 10 : from human gff, includes ncRNAs</li> <li> <p>slide 11 : </p> <ul> <li>data source: https://gtexportal.org/home/datasets (V8 gene TPMs)</li> <li>left: 1 sample -&gt; from 1 to $10^5$ TPM </li> <li>right: 50 random samples -&gt; 10% of genes contribute 90% of the transcripts</li> <li>NB: mammalian cell 10-30pg RNA/cell , around 360 000 mRNA molecules (source: https://www.qiagen.com/us/resources/faq?id=06a192c2-e72d-42e8-9b40-3171e1eb4cb8&amp;lang=en )</li> </ul> </li> <li> <p>slide 12 : important considerations as well</p> </li> </ul> <p>slide 13</p> <ul> <li>Illumina : market leader 50-600bp (generally 50-100), 0.1 (nextseq) to 3 (Hiseq) billion reads</li> <li>Ion torrent : 600bp, 260M reads</li> <li>Pacbio : 10-30kb N50 , 4M CCS reads </li> <li>nanopore : theory single molecule, practice: variable (N50 &gt;100kb on ultra-long kit, up to 4.2Mb )</li> </ul> <p>slides 14-22 : describe different technologies</p> <p>Ion Torrent :</p> <pre><code>- cell sequentially flooded with A T G C\n</code></pre> <p>PacBio SMRT</p> <pre><code>- DNApol at bottom of Zero-Mode-Waveguide \n- fluorescent dye on dNTPs\n</code></pre> <p>Illumina seq :</p> <pre><code>- formation of clusters with the same sequence \n- SBS : labelled nucleotides have reversible terminators, so only 1 base is incorporated at a time.\n</code></pre> <p>slide 23: paired end sequencing https://www.france-genomique.org/technological-expertises/whole-genome/sequencage-a-courtes-lectures-par-clusterisation/?lang=en</p> <p>slide 24: stranded sequencing https://www.ecseq.com/support/ngs/how-do-strand-specific-sequencing-protocols-work</p> <p>slide 25-26: RIN , RNA purification</p> <p>slide 27-34: sequencing depth and replicates</p> <ul> <li>slide 33-34 : this pattern applies to low- mid- and high- expressors (see their supp doc)</li> </ul> <p>slide 35-42: schematic analysis</p> <ul> <li>slide 35 : before the sequencing</li> <li>slide 36 : basic analysis</li> <li>slide 37 : basic analysis with trimmed reads</li> <li>slide 38 : main QC steps</li> <li>slide 39 : QC steps provide feedback on previous steps</li> <li>slide 40 : analysis for variant calling / isoform descriptions / \u2026</li> <li>slide 41 : when no reference genome: de novo assembly</li> <li>slide 42 : the analysis which we\u2019ll do during this course</li> </ul>"},{"location":"slides_notes/#02-quality-control","title":"02 Quality control","text":"<p>slide 04 : https://support.illumina.com/help/BaseSpace_OLH_009008/Content/Source/Informatics/BS/FileFormat_FASTQ-files_swBS.htm</p> <ul> <li>control bit: 0 when none of the control bits are on, otherwise it is an even number. On HiSeq X and NextSeq systems, control specification is not performed and this number is always 0.</li> </ul> <p>slide 16-\u2026 : interpretation of fastQC report</p>"},{"location":"slides_notes/#03-trimming","title":"03 trimming","text":"<p>slide 02-05 : enumerating reasons we may want to trim</p> <ul> <li>slide 04 : adapter can be present if, for instance, insert size is shorter than sequenced length. https://www.ecseq.com/support/ngs/trimming-adapter-sequences-is-it-necessary</li> </ul> <p>slide 06-11 : different cases where we may trim</p>"},{"location":"slides_notes/#04-mapping","title":"04 mapping","text":"<p>slides 4-8:</p> <ul> <li>bowtie2 : https://bowtie-bio.sourceforge.net/bowtie2/manual.shtml</li> <li>STAR : https://github.com/alexdobin/STAR</li> <li>tophat2 : https://hcc.unl.edu/docs/applications/app_specific/bioinformatics_tools/alignment_tools/tophat_tophat2/</li> <li>RSEM : https://github.com/deweylab/RSEM</li> <li>cufflinks : http://cole-trapnell-lab.github.io/cufflinks/</li> <li>salmon : https://salmon.readthedocs.io/en/latest/salmon.html</li> <li>kallisto : https://pachterlab.github.io/kallisto/</li> <li>tximport : https://bioconductor.org/packages/release/bioc/html/tximport.html</li> <li>featurecount : https://subread.sourceforge.net/featureCounts.html</li> <li>stringtie : http://ccb.jhu.edu/software/stringtie/index.shtml?t=manual</li> <li>GATK variant calling pipeline https://gatk.broadinstitute.org/hc/en-us/articles/360035531192-RNAseq-short-variant-discovery-SNPs-Indels-</li> </ul>"},{"location":"slides_notes/#05-de","title":"05 DE","text":"<p>slide 3-9 : challenges for RNAseq </p> <ul> <li> <p>slide 4: sequencing depth varies accross libraries</p> <ul> <li>left : 100 samples from the Gtex V8 dataset: https://gtexportal.org/home/datasets</li> <li>right : samples from a random binomial with and without a library size factor applied</li> </ul> </li> <li> <p>slide 5: most of the expression is taken by very few genes + a lot of genes have 0 reads  (plots: data from 100 samples from the Gtex V8 dataset: https://gtexportal.org/home/datasets )</p> </li> <li> <p>slide 6: small number of samples. 10k simulation of negative binomial draws</p> </li> <li> <p>slide 7-9 : xkcd.com/882</p> </li> </ul> <p>slide 10-11 : input</p> <p>slide 12-14 : https://www.rna-seqblog.com/rpkm-fpkm-and-tpm-clearly-explained/</p> <p>slide 15: filtering:      * https://academic.oup.com/bioinformatics/article/29/17/2146/240530 max and mean refer to CPM thresholds ; CPM 1: genes with a CPM less than one in more than half the samples are filtered     * http://bioconductor.org/packages/devel/bioc/vignettes/DESeq2/inst/doc/DESeq2.html#indfilt     * section 2.7 of https://www.bioconductor.org/packages/release/bioc/vignettes/edgeR/inst/doc/edgeRUsersGuide.pdf</p> <p>slide 16: normalization</p> <p>https://www.biostars.org/p/284775/ EdgeR: Trimmed Mean of M-values (TMM): </p> <pre><code>    also based on the hypothesis that most genes are not DE.\n\n    The TMM factor is computed for each lane, with one lane being considered as a reference sample and the others as test samples.\n\n    For each test sample, TMM is computed as the weighted mean of log ratios between this test and the reference, after exclusion of the most expressed genes and the genes with the largest log ratios.\n\n    According to the hypothesis of low DE, this TMM should be close to 1. If it is not, its value provides an estimate of the correction factor that must be applied to the library sizes (and not the raw counts) in order to fulfill the hypothesis.\n\n    [source: https://www.ncbi.nlm.nih.gov/pubmed/22988256]\n</code></pre> <p>DESeq2     DESeq:  is based on the hypothesis that most genes are not DE. </p> <pre><code>    the median of the ratio, for each gene, of its read count over its geometric mean across all lanes.\n\n    The underlying idea is that non-DE genes should have similar read counts across samples, leading to a ratio of 1.\n\n    Assuming most genes are not DE, the median of this ratio for the lane provides an estimate of the correction factor that should be applied to all read counts of this lane to fulfill the hypothesis.\n\n        [source: https://www.ncbi.nlm.nih.gov/pubmed/22988256]\n</code></pre> <p>slide 17 : https://pubmed.ncbi.nlm.nih.gov/22988256/     * TC: Total count (CPM) - UQ: Upper Quartile - Med: median - Q: quantile     * top left: coef of variation in housekeeping genes in H. sapiens data     * top right: average false-positive rate over 10 independent datasets simulated with varying proportions of differentially expressed genes (from 0% to 30% for each normalization method).      * bottom:         * distribution: distribution inter samples look the same         * Intra-variance: intra group variance          * Housekeeping : coef of variation in 30 housekeeping genes, which are supposed similarly expressed         * clustering: similarity of DE genes with other methods         * false positive rate : see above</p> <p>slide 19: NB model     * https://doi.org/10.1186/gb-2010-11-10-r106     * orange line is the fit w(q)     * purple line show the variance implied by the Poisson distribution      * dashed orange line is the variance estimate used by edgeR. </p>"},{"location":"days/DE/","title":"Differential Expression Inference","text":"<p>Once the reads have been mapped and counted, one can assess the differential expression of genes between different conditions.</p> <p>During this lesson, you will learn to :</p> <ul> <li>describe the different steps of data normalization and modelling commonly used for RNA-seq data.</li> <li>detect significantly differentially-expressed genes using either edgeR or DESeq2.</li> </ul>"},{"location":"days/DE/#material","title":"Material","text":"<p> Download the presentation</p> <p>Rstudio website</p> <p>edgeR user\u2019s guide</p> <p>DESeq2 vignette</p>"},{"location":"days/DE/#connexion-to-the-rstudio-server","title":"Connexion to the Rstudio server","text":"<p>Note</p> <p>This step is intended only for users who attend the course with a teacher. Otherwise you will have to rely on your own installation of Rstudio.</p> <p>The analysis of the read count data will be done on an RStudio instance, using the R language and some relevant Bioconductor libraries.</p> <p>As you start your session on the RStudio server, please make sure that you know where your data is situated with respect to your working directory (use <code>getwd()</code> and <code>setwd()</code> to respectively : know what your working is, and change it as necessary).</p>"},{"location":"days/DE/#differential-expression-inference","title":"Differential Expression Inference","text":"<p>Let\u2019s analyze the <code>mouseMT</code> toy dataset.</p> DESeq2 analysis <pre><code>library(DESeq2)\nlibrary(ggplot2)\nlibrary(pheatmap)\n</code></pre> edgeR analysis <pre><code># setup\nlibrary(edgeR)\nlibrary(ggplot2)\n</code></pre> <p>From ensembl gene ids to gene names</p> <p>We can convert between different gene ids using the <code>bitr</code> function from <code>clusterProfiler</code> <pre><code>library(clusterProfiler)\nlibrary(org.Mm.eg.db)\n\ngenes_universe &lt;- bitr(rownames(allGenes), fromType = \"ENSEMBL\",\ntoType = c(\"ENTREZID\", \"SYMBOL\"),\nOrgDb = \"org.Mm.eg.db\")\ngenes_universe\n</code></pre> <pre><code>             ENSEMBL ENTREZID SYMBOL\n1 ENSMUSG00000064341    17716    ND1\n2 ENSMUSG00000064354    17709   COX2\n3 ENSMUSG00000064345    17717    ND2\n4 ENSMUSG00000064368    17722    ND6\n5 ENSMUSG00000064351    17708   COX1\n6 ENSMUSG00000064358    17710   COX3\n7 ENSMUSG00000065947    17720   ND4L\n8 ENSMUSG00000064363    17719    ND4\n9 ENSMUSG00000064357    17705   ATP6\n</code></pre></p> <p>Here is the list of orgDb packages. For non-model organisms it will be more complex.</p>"},{"location":"days/DE/#reading","title":"reading","text":"<p>We will use the trimmed reads</p> <pre><code>folder  = \"/shared/data/Solutions/mouseMT/044_STAR_map_trimmed/\"\n\n# we skip the 4 first lines, which contains \n# N_unmapped , N_multimapping , N_noFeature , N_ambiguous   \n\nsample_a1_table = read.table(paste0( folder , \"sample_a1\" , \".ReadsPerGene.out.tab\") , row.names = 1 , skip = 4 )\nhead( sample_a1_table )\n</code></pre> <pre><code>                    V2      V3      V4\nENSMUSG00000064336  0       0       0   \nENSMUSG00000064337  0       0       0   \nENSMUSG00000064338  0       0       0   \nENSMUSG00000064339  0       0       0   \nENSMUSG00000064340  0       0       0   \nENSMUSG00000064341  4046    1991    2055    \n</code></pre> <p>We are interested in the first columns, which contains counts for unstranded reads</p> <p>Let\u2019s use a loop to automatize the reading: <pre><code>raw_counts = data.frame( row.names =  row.names(sample_a1_table) )\n\nfor( sample in c('a1','a2','a3','a4','b1','b2','b3','b4') ){\nsample_table = read.table(paste0( folder , \"sample_\" , sample , \".ReadsPerGene.out.tab\") , row.names = 1 , skip = 4 )\n\nraw_counts[sample] = sample_table[ row.names(raw_counts) , \"V2\" ]\n\n}\n\nhead( raw_counts )\n</code></pre> <pre><code>                    a1      a2      a3      a4  b1  b2  b3  b4\nENSMUSG00000064336  0       0       0       0   0   0   0   0\nENSMUSG00000064337  0       0       0       0   0   0   0   0\nENSMUSG00000064338  0       0       0       0   0   0   0   0\nENSMUSG00000064339  0       0       0       2   0   0   0   0\nENSMUSG00000064340  0       0       0       0   0   0   0   0\nENSMUSG00000064341  4046    4098    4031    1   449 515 13  456\n</code></pre></p>"},{"location":"days/DE/#setting-up-the-experimental-design","title":"setting up the experimental design","text":"<pre><code>#note: levels let's us define the reference levels\ntreatment &lt;- factor( c(rep(\"a\",4), rep(\"b\",4)), levels=c(\"a\", \"b\") )\ncolData &lt;- data.frame(treatment, row.names = colnames(raw_counts))\ncolData\n</code></pre> <pre><code>    treatment\na1  a           \na2  a           \na3  a           \na4  a           \nb1  b           \nb2  b           \nb3  b           \nb4  b\n</code></pre>"},{"location":"days/DE/#creating-the-deseq-data-object-and-some-qc","title":"creating the DESeq data object and some QC","text":"<p><pre><code>dds &lt;- DESeqDataSetFromMatrix(\ncountData = raw_counts, colData = colData, design = ~ treatment)\ndim(dds)\n</code></pre> <pre><code>[1] 37  8\n</code></pre></p> <p>Filter low count genes. </p> <p>Here, we will apply a very soft filter and keep genes with at least 1 read in at least 4 samples (size of the smallest group).</p> <p><pre><code>idx &lt;- rowSums(counts(dds, normalized=FALSE) &gt;= 1) &gt;= 4\ndds.f &lt;- dds[idx, ]\ndim(dds.f)\n</code></pre> <pre><code>[1] 13  8\n</code></pre></p> <p>We go from 37 to 13 genes</p> <p>We perform the estimation of dispersions  <pre><code>dds.f &lt;- DESeq(dds.f)\n</code></pre> <pre><code>estimating size factors\nestimating dispersions\ngene-wise dispersion estimates\nmean-dispersion relationship\n-- note: fitType='parametric', but the dispersion trend was not well captured by the\n   function: y = a/x + b, and a local regression fit was automatically substituted.\n   specify fitType='local' or 'mean' to avoid this message next time.\nfinal dispersion estimates\nfitting model and testing\n</code></pre></p> <p>PCA plot of the samples: <pre><code># blind : whether to blind the transformation to the experimental design. \n#   - blind=TRUE : comparing samples in a manner unbiased by prior information on samples, \n#                  for example to perform sample QA (quality assurance).\n#   - blind=FALSE: should be used for transforming data for downstream analysis, \n#                  where the full use of the design information should be made.\n\nvsd &lt;- varianceStabilizingTransformation(dds.f, blind=TRUE )\npcaData &lt;- plotPCA(vsd, intgroup=c(\"treatment\"))\npcaData + geom_label(aes(x=PC1,y=PC2,label=name))\n</code></pre></p> <p></p> <p>OK, so a4 and b3 are quite different from the rest.</p> <ul> <li>a4 was expected from the QC</li> <li>b3 we did not expect until now</li> </ul> <p>If we did the analysis with them, here is what we get: <pre><code>res &lt;- results(dds.f)\nsummary( res )\n</code></pre> <pre><code>out of 13 with nonzero total read count\nadjusted p-value &lt; 0.1\nLFC &gt; 0 (up)       : 0, 0%\nLFC &lt; 0 (down)     : 0, 0%\noutliers [1]       : 7, 54%\nlow counts [2]     : 0, 0%\n(mean count &lt; 16)\n[1] see 'cooksCutoff' argument of ?results\n[2] see 'independentFiltering' argument of ?results\n</code></pre></p> <p>So, let\u2019s eliminate these two samples.</p>"},{"location":"days/DE/#analysis-without-the-outliers","title":"analysis without the outliers","text":"<pre><code>raw_counts_no_outliers = raw_counts[ , !( colnames(raw_counts) %in% c('a4','b3') ) ]\n\ntreatment &lt;- factor( c(rep(\"a\",3), rep(\"b\",3)), levels=c(\"a\", \"b\") )\ncolData &lt;- data.frame(treatment, row.names = colnames(raw_counts_no_outliers))\ncolData </code></pre> <pre><code>    treatment\na1  a           \na2  a           \na3  a           \nb1  b           \nb2  b           \nb4  b\n</code></pre> <p><pre><code>dds &lt;- DESeqDataSetFromMatrix(\ncountData = raw_counts_no_outliers, colData = colData, design = ~ treatment)\ndim(dds)\n</code></pre> <pre><code>[1] 37  6\n</code></pre></p> <p>Filter low count genes: now the smallest group is 3 <pre><code>idx &lt;- rowSums(counts(dds, normalized=FALSE) &gt;= 1) &gt;= 3\ndds.f &lt;- dds[idx, ]\ndim(dds.f)\n</code></pre> <pre><code>[1] 12  6\n</code></pre> 12 genes remaining</p> <p>We perform the estimation of dispersions  <pre><code>dds.f &lt;- DESeq(dds.f)\n</code></pre> <pre><code>estimating size factors\nestimating dispersions\ngene-wise dispersion estimates\nmean-dispersion relationship\nfinal dispersion estimates\nfitting model and testing\n</code></pre></p> <p>PCA plot of the samples: <pre><code>vsd &lt;- varianceStabilizingTransformation(dds.f, blind=TRUE )\npcaData &lt;- plotPCA(vsd, intgroup=c(\"treatment\"))\npcaData + geom_label(aes(x=PC1,y=PC2,label=name))\n</code></pre> </p> <p>It looks much better. Seems like PC1 captures the group effect</p> <p>We plot the estimate of the dispersions <pre><code># * black dot : raw\n# * red dot : local trend\n# * blue : corrected\nplotDispEsts(dds.f)\n</code></pre></p> <p></p> <p>There is so few genes that this does not look super nice here</p> <p>For the Ruhland2016 dataset it looks like:</p> <p></p> <p>This plot is not easy to interpret. It represents the amount of dispersion at different levels of expression. It is directly linked to our ability to detect differential expression.</p> <p>Here it looks about normal compared to typical bulk RNA-seq experiments : the dispersion is comparatively larger for lowly expressed genes.</p> <p><pre><code># extracting results for the treatment versus control contrast\nres &lt;- results(dds.f)\nsummary( res )\n</code></pre> <pre><code>out of 12 with nonzero total read count\nadjusted p-value &lt; 0.1\nLFC &gt; 0 (up)       : 1, 8.3%\nLFC &lt; 0 (down)     : 2, 17%\noutliers [1]       : 0, 0%\nlow counts [2]     : 0, 0%\n(mean count &lt; 1)\n[1] see 'cooksCutoff' argument of ?results\n[2] see 'independentFiltering' argument of ?results\n</code></pre></p> <p>We can have a look at the coefficients of this model <pre><code>head(coef(dds.f)) # the second column corresponds to the difference between the 2 conditions\n</code></pre> <pre><code>                   Intercept treatment_b_vs_a\nENSMUSG00000064341 12.084282      -3.33601412\nENSMUSG00000064345  6.112479      -0.99101369\nENSMUSG00000064351  3.757967       0.64546475\nENSMUSG00000064354 10.209339       1.44160442\nENSMUSG00000064357 11.763707      -0.06245774\nENSMUSG00000064358  6.026334      -0.26447858\n</code></pre></p> <p>Here, it contains an intercept and a coefficient for the difference between the two groups.</p> <p>MA plot: <pre><code>res.lfc &lt;- lfcShrink(dds.f, coef=2, res=res)\nDESeq2::plotMA(res.lfc)\n</code></pre> </p> <p>Volcano plot: <pre><code>FDRthreshold = 0.01\nlogFCthreshold = 1.0\n# add a column of NAs\nres.lfc$diffexpressed &lt;- \"NO\"\n# if log2Foldchange &gt; 1 and pvalue &lt; 0.01, set as \"UP\" \nres.lfc$diffexpressed[res.lfc$log2FoldChange &gt; logFCthreshold &amp; res.lfc$padj &lt; FDRthreshold] &lt;- \"UP\"\n# if log2Foldchange &lt; 1 and pvalue &lt; 0.01, set as \"DOWN\"\nres.lfc$diffexpressed[res.lfc$log2FoldChange &lt; -logFCthreshold &amp; res.lfc$padj &lt; FDRthreshold] &lt;- \"DOWN\"\n\nggplot( data = data.frame( res.lfc ) , aes( x=log2FoldChange , y = -log10(padj) , col =diffexpressed ) ) + geom_point() + geom_vline(xintercept=c(-logFCthreshold, logFCthreshold), col=\"red\") +\ngeom_hline(yintercept=-log10(FDRthreshold), col=\"red\") +\nscale_color_manual(values=c(\"blue\", \"grey\", \"red\"))\n\ntable(res.lfc$diffexpressed)\n</code></pre> <pre><code>DOWN   NO   UP \n   1   10    1 \n</code></pre> </p> <p>Heatmap: <pre><code>vsd.counts &lt;- assay(vsd)\n\ntopVarGenes &lt;- head(order(rowVars(vsd.counts), decreasing = TRUE), 20)\nmat  &lt;- vsd.counts[ topVarGenes, ] #scaled counts of the top genes\nmat  &lt;- mat - rowMeans(mat)  # centering\npheatmap(mat)\n</code></pre> </p>"},{"location":"days/DE/#saving-results-to-file","title":"saving results to file","text":"<p>note: a CSV file can be imported into Excel <pre><code>write.csv( res ,'mouseMT.DESeq2.results.csv' )\n</code></pre></p>"},{"location":"days/DE/#reading_1","title":"reading","text":"<p>We will use the trimmed reads</p> <p>First we try with the sample_a1:</p> <p><pre><code>folder  = \"/shared/data/Solutions/mouseMT/044_STAR_map_trimmed/\"\n\n# we skip the 4 first lines, which contains \n# N_unmapped , N_multimapping , N_noFeature , N_ambiguous   \n\nsample_a1_table = read.table(paste0( folder , \"sample_a1\" , \".ReadsPerGene.out.tab\") , row.names = 1 , skip = 4 )\nhead( sample_a1_table )\n</code></pre> <pre><code>                    V2      V3      V4\nENSMUSG00000064336  0       0       0   \nENSMUSG00000064337  0       0       0   \nENSMUSG00000064338  0       0       0   \nENSMUSG00000064339  0       0       0   \nENSMUSG00000064340  0       0       0   \nENSMUSG00000064341  4046    1991    2055    \n</code></pre></p> <p>We are interested in the first columns, which contains counts for unstranded reads</p> <p>Let\u2019s use a loop to automatize the reading:</p> <p><pre><code>raw_counts = data.frame( row.names =  row.names(sample_a1_table) )\n\nfor( sample in c('a1','a2','a3','a4','b1','b2','b3','b4') ){\nsample_table = read.table(paste0( folder , \"sample_\" , sample , \".ReadsPerGene.out.tab\") , row.names = 1 , skip = 4 )\n\nraw_counts[sample] = sample_table[ row.names(raw_counts) , \"V2\" ]\n\n}\n\nhead( raw_counts )\n</code></pre> <pre><code>                    a1      a2      a3      a4  b1  b2  b3  b4\nENSMUSG00000064336  0       0       0       0   0   0   0   0\nENSMUSG00000064337  0       0       0       0   0   0   0   0\nENSMUSG00000064338  0       0       0       0   0   0   0   0\nENSMUSG00000064339  0       0       0       2   0   0   0   0\nENSMUSG00000064340  0       0       0       0   0   0   0   0\nENSMUSG00000064341  4046    4098    4031    1   449 515 13  456\n</code></pre></p>"},{"location":"days/DE/#experimental-design","title":"experimental design","text":"<p>note: levels let\u2019s us define the reference levels</p> <p><pre><code>treatment &lt;- factor( c(rep(\"a\",4), rep(\"b\",4)), levels=c(\"a\", \"b\") )\nnames(treatment) = colnames(raw_counts)\n\ntreatment\n</code></pre> <pre><code>a1 a2 a3 a4 b1 b2 b3 b4 \n a  a  a  a  b  b  b  b \nLevels: a b\n</code></pre></p>"},{"location":"days/DE/#edger-object-preprocessing-and-qc","title":"edgeR  object preprocessing and QC","text":"<p>Creating the edgeR DGE object and filtering low-count genes.</p> <p><pre><code>dge.all &lt;- DGEList(counts = raw_counts , group = treatment)  dge.f.design &lt;- model.matrix(~ treatment)\n\n# filtering by expression level. See ?filterByExpr for details\nkeep &lt;- filterByExpr(dge.all)\ndge.f &lt;- dge.all[keep, keep.lib.sizes=FALSE]\ntable( keep )\n</code></pre> <pre><code>keep\nFALSE  TRUE \n   28     9 \n</code></pre></p> <p><pre><code>#normalization\ndge.f &lt;- calcNormFactors(dge.f)\ndge.f$samples\n</code></pre> <pre><code>    group   lib.size    norm.factors\na1  a       13799       1.2101311   \na2  a       13649       1.2130900   \na3  a       13938       1.2131513   \na4  a       6831        0.1058474   \nb1  b       13703       1.3614563   \nb2  b       13627       1.3728761   \nb3  b       162         2.0759281   \nb4  b       13687       1.3671996   \n</code></pre> We represent the distances between the samples using MDS:</p> <p><pre><code>plotMDS( dge.f , col = c('cornflowerblue','forestgreen')[treatment] )\n</code></pre> </p> <p>OK, so a4 and b3 are quite different from the rest.</p> <ul> <li>a4 was expected from the QC</li> <li>b3 we did not expect until now</li> </ul> <p>If we did the analysis with them, here is what we get: <pre><code># estimate of the dispersion\ndge.f &lt;- estimateDisp(dge.f,dge.f.design , robust = T)\n# testing for differential expression. \ndge.f.et &lt;- exactTest(dge.f)\ntopTags(dge.f.et)\n</code></pre> <pre><code>                    logFC       logCPM      PValue      FDR\nENSMUSG00000065947  -5.06512022 13.51727    0.006432974 0.05110489\nENSMUSG00000064351  -7.46603064 20.14951    0.011356643 0.05110489\nENSMUSG00000064345  3.35236454  15.28962    0.050902734 0.13212506\nENSMUSG00000064341  -2.67947447 16.69126    0.058722251 0.13212506\nENSMUSG00000064354  1.58127747  16.60293    0.263189690 0.42743645\nENSMUSG00000064368  1.75395362  12.51557    0.284957631 0.42743645\nENSMUSG00000064358  -0.10554311 11.49478    0.912516094 0.96610797\nENSMUSG00000064363  -0.08216781 17.90743    0.950808801 0.96610797\nENSMUSG00000064357  0.06455233  17.18842    0.966107973 0.96610797\n</code></pre></p> <p>no gene is significantly DE.</p> <p>So, let\u2019s eliminate these two samples.</p>"},{"location":"days/DE/#analysis-without-the-outliers_1","title":"analysis without the outliers","text":"<p><pre><code>raw_counts_no_outliers = raw_counts[ , !( colnames(raw_counts) %in% c('a4','b3') ) ]\nhead( raw_counts_no_outliers )\n</code></pre> <pre><code>                    a1      a2      a3      b1  b2  b4\nENSMUSG00000064336  0       0       0       0   0   0\nENSMUSG00000064337  0       0       0       0   0   0\nENSMUSG00000064338  0       0       0       0   0   0\nENSMUSG00000064339  0       0       0       0   0   0\nENSMUSG00000064340  0       0       0       0   0   0\nENSMUSG00000064341  4046    4098    4031    449 515 456\n</code></pre></p> <p><pre><code>treatment &lt;- factor( c(rep(\"a\",3), rep(\"b\",3)), levels=c(\"a\", \"b\") )\ncolData &lt;- data.frame(treatment, row.names = colnames(raw_counts_no_outliers))\ncolData </code></pre> <pre><code>    treatment\na1  a           \na2  a           \na3  a           \nb1  b           \nb2  b           \nb4  b\n</code></pre></p> <p><pre><code>dge.all &lt;- DGEList(counts = raw_counts_no_outliers , group = treatment)  dge.f.design &lt;- model.matrix(~ treatment)\n\n# filtering by expression level. See ?filterByExpr for details\nkeep &lt;- filterByExpr(dge.all)\ndge.f &lt;- dge.all[keep, keep.lib.sizes=FALSE]\ntable( keep )\n</code></pre> <pre><code>keep\nFALSE  TRUE \n   28     9 \n</code></pre></p> <p>We compute the normalization factor for each library: <pre><code>#normalization\ndge.f &lt;- calcNormFactors(dge.f)\ndge.f$samples\n</code></pre> <pre><code>    group   lib.size    norm.factors\na1  a       13799       0.9437444   \na2  a       13649       0.9277668   \na3  a       13938       0.9412032   \nb1  b       13703       1.0672149   \nb2  b       13627       1.0651230   \nb4  b       13687       1.0675095   \n</code></pre></p> <p>We represent the distances between the samples using MDS:</p> <p><pre><code>plotMDS( dge.f , col = c('cornflowerblue','forestgreen')[treatment] )\n</code></pre> </p> <p>It looks much better. Seems like PC1 captures the group effect.</p> <p>We now fit the model:</p> <p><pre><code># estimate of the dispersion\ndge.f &lt;- estimateDisp(dge.f,dge.f.design , robust = T)\nplotBCV(dge.f)\n</code></pre> </p> <p>There is so few genes that this does not look super nice here.</p> <p>Here is how it looks like on the Ruhland2016 data:</p> <p></p> <p>This plot is not easy to interpret. It represents the amount of biological variation at different levels of expression. It is directly linked to our ability to detect differential expression.</p> <p>Here it looks about normal compared to other bulk RNA-seq experiments : the variation is comparatively larger for lowly expressed genes.</p> <p><pre><code># testing for differential expression. \n# This method is recommended when you only have 2 groups to compare\ndge.f.et &lt;- exactTest(dge.f)\ntopTags(dge.f.et) # printing the genes where the p-value of differential expression if the lowest\n</code></pre> <pre><code>                    logFC           logCPM      PValue          FDR\nENSMUSG00000064341  -3.2728209590   17.34360    0.000000e+00    0.000000e+00\nENSMUSG00000064354  1.5046106466    17.35950    0.000000e+00    0.000000e+00\nENSMUSG00000064345  -0.9251244495   11.92467    4.542291e-08    1.362687e-07\nENSMUSG00000064368  0.7262191019    10.55226    1.337410e-02    3.009174e-02\nENSMUSG00000064351  0.7031516899    10.50496    2.006905e-02    3.612429e-02\nENSMUSG00000064358  -0.1964770294   12.14888    2.110506e-01    3.165759e-01\nENSMUSG00000065947  0.2623477474    10.00748    4.831813e-01    5.851717e-01\nENSMUSG00000064363  -0.0127783014   18.61543    5.201527e-01    5.851717e-01\nENSMUSG00000064357  0.0006819028    17.93963    9.833826e-01    9.833826e-01\n</code></pre></p> <p>We can see 3 genes with FDR &lt; 0.01 and 2 others with 0.01 &lt; FDR &lt; 0.05.</p> <p><pre><code>summary(decideTests(dge.f.et , p.value = 0.01)) # let's use 0.01 as a threshold\n</code></pre> <pre><code>        b-a\nDown     2\nNotSig   6\nUp       1\n</code></pre></p> <p>Let\u2019s plot these: <pre><code>## plot all the logFCs versus average count size. Significantly DE genes are  colored\nplotMD(dge.f.et)\n# lines at a log2FC of 1/-1, corresponding to a shift in expression of x2 \nabline(h=c(-1,1), col=\"blue\")\nabline(h=c(0), col=\"grey\")\n</code></pre> </p> <p>Volcano plot</p> <p><pre><code>allGenes = topTags(dge.f.et , n = nrow(dge.f.et$table) )$table\n\nFDRthreshold = 0.01\nlogFCthreshold = 1.0\n# add a column of NAs\nallGenes$diffexpressed &lt;- \"NO\"\n# if log2Foldchange &gt; 1 and pvalue &lt; 0.01, set as \"UP\" \nallGenes$diffexpressed[allGenes$logFC &gt; logFCthreshold &amp; allGenes$FDR &lt; FDRthreshold] &lt;- \"UP\"\n# if log2Foldchange &lt; 1 and pvalue &lt; 0.01, set as \"DOWN\"\nallGenes$diffexpressed[allGenes$logFC &lt; -logFCthreshold &amp; allGenes$FDR &lt; FDRthreshold] &lt;- \"DOWN\"\n\nggplot( data = allGenes , aes( x=logFC , y = -log10(FDR) , col =diffexpressed ) ) + geom_point() + geom_vline(xintercept=c(-logFCthreshold, logFCthreshold), col=\"red\") +\ngeom_hline(yintercept=-log10(FDRthreshold), col=\"red\") +\nscale_color_manual(values=c(\"blue\", \"grey\", \"red\"))\n</code></pre> </p>"},{"location":"days/DE/#writing-the-table-of-results","title":"writing the table of results","text":"<pre><code>write.csv( allGenes , 'mouseMT.edgeR.results.csv')\n</code></pre>"},{"location":"days/DE/#differential-expression-task","title":"Differential Expression - Task","text":"<p>Use either edgeR or DESeq2 to conduct a differential expression analysis.</p> <p>You may play with either of the following dataset:</p> <ul> <li>Ruhland2016<ul> <li>simple 1 factor design</li> <li><code>/shared/data/Solutions/Ruhland2016/countFiles/featureCounts_Ruhland2016.counts.txt</code></li> <li>  Ruhland2016 count matrix</li> </ul> </li> <li>the Liu2015 dataset:<ul> <li>simple 1 factor design</li> <li><code>/shared/data/Solutions/Liu2015/countFiles/featureCounts_Liu2015.counts.txt</code></li> <li>  Liu2015 count matrix</li> </ul> </li> <li>Tuch 2010 dataset<ul> <li>2 factors design : 3 patients (8, 33, and 51) had each 1 sample from tumor tissue (T) and normal tissue (N) sequenced. </li> <li>the goal is to find the difference between tumor and normal while taking the patient variable into account.</li> <li><code>/shared/data/Solutions/Tuch2010/Tuch_et_al_2010_counts.csv</code></li> <li>  Tuch 2010 count matrix</li> </ul> </li> </ul> <p>Note</p> <ul> <li>Generally, users find the syntax and workflow of DESeq2 easier for getting started.</li> <li>If you have the time, conduct a differential expression analysis using both DESeq2 and edgeR.</li> <li> <p>Follow the vignettes/user\u2019s guide! They are the most up-to-date and generally contain everything a newcomer might need, including worked-out examples.</p> </li> <li> <p>when dealing with more than one factor, you will need a model matrix to specify the experimental design to the library, and to craft your contrasts of interest. The ExploreModelMatrix package may help you a lot in that regard.</p> </li> </ul>"},{"location":"days/DE/#ruhland2016-deseq2-correction","title":"Ruhland2016 - DESeq2 correction","text":"<p>DESeq2 vignette</p> read in the data <pre><code># setup\nlibrary(DESeq2)\nlibrary(ggplot2)\n\n\n# reading the counts files - adapt the file path to your situation\nraw_counts &lt;-read.table('/shared/data/Solutions/Ruhland2016/countFiles/featureCounts_Ruhland2016.counts.txt' , skip=1 , sep=\"\\t\" , header=T)\n\n# setting up row names as ensembl gene ids\nrow.names(raw_counts) = raw_counts$Geneid\n\n## looking at the beginning of that table\nraw_counts[1:5,1:5]\n\n# removing these first columns to keep only the sample counts\nraw_counts = raw_counts[ ,  -1:-6  ] # changing column names\nnames( raw_counts) = gsub('_.*', '', gsub('.*.SRR[0-9]{7}_', '', names(raw_counts) ) )\n\n# some checking of what we just read\nhead(raw_counts); tail(raw_counts); dim(raw_counts)\ncolSums(raw_counts) # total number of counted reads per sample\n</code></pre> preprocessing <pre><code>## telling DESeq2 what the experimental design was\n# note: by default, the 1st level is considered to be the reference/control/WT/...\ntreatment &lt;- factor( c(rep(\"EtOH\",3), rep(\"TAM\",3)), levels=c(\"EtOH\", \"TAM\") )\ncolData &lt;- data.frame(treatment, row.names = colnames(raw_counts))\ncolData\n\n## creating the DESeq data object &amp; positing the model\ndds &lt;- DESeqDataSetFromMatrix(\ncountData = raw_counts, colData = colData, design = ~ treatment)\ndim(dds)\n\n## filter low count genes. Here, only keep genes with at least 2 samples where there are at least 5 reads.\nidx &lt;- rowSums(counts(dds, normalized=FALSE) &gt;= 5) &gt;= 2\ndds.f &lt;- dds[idx, ]\ndim(dds.f)\n\n# we go from 55414 to 19378 genes\n</code></pre> <p>Around 19k genes pass our minimum expression threshold, quite typical for a bulk Mouse RNA-seq experiment.</p> estimate dispersion / model fitting <pre><code># we perform the estimation of dispersions \ndds.f &lt;- DESeq(dds.f)\n\n# we plot the estimate of the dispersions\n# * black dot : raw\n# * red dot : local trend\n# * blue : corrected\nplotDispEsts(dds.f)\n\n# extracting results for the treatment versus control contrast\nres &lt;- results(dds.f)\n</code></pre> <p></p> <p>This plot is not easy to interpret. It represents the amount of dispersion at different levels of expression. It is directly linked to our ability to detect differential expression.</p> <p>Here it looks about normal compared to typical bulk RNA-seq experiments : the dispersion is comparatively larger for lowly expressed genes.</p> looking at the results <pre><code># adds estimate of the LFC the results table. \n# This shrunk logFC estimate is more robust than the raw value\n\nhead(coef(dds.f)) # the second column corresponds to the difference between the 2 conditions\nres.lfc &lt;- lfcShrink(dds.f, coef=2, res=res)\n\n#plotting to see the difference.  \npar(mfrow=c(2,1))\nDESeq2::plotMA(res)\nDESeq2::plotMA(res.lfc)\n# -&gt; with shrinkage, the significativeness and logFC are more consistent\npar(mfrow=c(1,1))\n</code></pre> <p></p> <p>Without the shrinkage, we can see that for low counts we can see a high log-fold change but non significant (ie. we see a large difference but with variance is also so high that this observation may be due to chance only).</p> <p>The shrinkage corrects this and the relationship between logFC and significance is smoother.</p> <p><pre><code># we apply the variance stabilising transformation to make the read counts comparable across libraries\n# (nb : this is not needed for DESeq DE analysis, but rather for visualisations that compare expression across samples, such as PCA. This replaces normal PCA scaling)\nvst.dds.f &lt;- vst(dds.f, blind = FALSE)\nvst.dds.f.counts &lt;- assay(vst.dds.f)\n\nplotPCA(vst.dds.f, intgroup = c(\"treatment\"))\n</code></pre> </p> <p>The first axis (58% of the variance) seems linked to the grouping of interest.</p> <pre><code>## ggplot2-based volcano plot\nlibrary(ggplot2)\n\nFDRthreshold = 0.01\nlogFCthreshold = 1.0\n# add a column of NAs\nres.lfc$diffexpressed &lt;- \"NO\"\n# if log2Foldchange &gt; 1 and pvalue &lt; 0.01, set as \"UP\" \nres.lfc$diffexpressed[res.lfc$log2FoldChange &gt; logFCthreshold &amp; res.lfc$padj &lt; FDRthreshold] &lt;- \"UP\"\n# if log2Foldchange &lt; 1 and pvalue &lt; 0.01, set as \"DOWN\"\nres.lfc$diffexpressed[res.lfc$log2FoldChange &lt; -logFCthreshold &amp; res.lfc$padj &lt; FDRthreshold] &lt;- \"DOWN\"\n\nggplot( data = data.frame( res.lfc ) , aes( x=log2FoldChange , y = -log10(padj) , col =diffexpressed ) ) + geom_point() + geom_vline(xintercept=c(-logFCthreshold, logFCthreshold), col=\"red\") +\ngeom_hline(yintercept=-log10(FDRthreshold), col=\"red\") +\nscale_color_manual(values=c(\"blue\", \"grey\", \"red\"))\n\ntable(res.lfc$diffexpressed)\n</code></pre> <pre><code> DOWN    NO    UP \n  131 19002   245 \n</code></pre> <p></p> <pre><code>library(pheatmap)\ntopVarGenes &lt;- head(order(rowVars(vst.dds.f.counts), decreasing = TRUE), 20)\nmat  &lt;- vst.dds.f.counts[ topVarGenes, ] #scaled counts of the top genes\nmat  &lt;- mat - rowMeans(mat)  # centering\npheatmap(mat)\n</code></pre> <p></p> <pre><code># saving results to file\n# note: a CSV file can be imported into Excel\nwrite.csv( res ,'Ruhland2016.DESeq2.results.csv' )\n</code></pre>"},{"location":"days/DE/#ruhland2016-edger-correction","title":"Ruhland2016 - EdgeR correction","text":"<p>edgeR user\u2019s guide</p> read in the data <pre><code>library(edgeR)\nlibrary(ggplot2)\n\n# reading the counts files - adapt the file path to your situation\nraw_counts &lt;- read.table('.../Ruhland2016_featureCount_result.counts' , skip=1 , sep=\"\\t\" , header=T)\n\n# setting up row names as ensembl gene ids\nrow.names(raw_counts) = raw_counts$Geneid\n\n# removing these first columns to keep only the sample counts\nraw_counts = raw_counts[ ,  -1:-6  ] # changing column names\nnames( raw_counts) = gsub('_.*', '', gsub('.*.SRR[0-9]{7}_', '', names(raw_counts) ) )\n\n# some checking of what we just read\nhead(raw_counts); tail(raw_counts); dim(raw_counts)\ncolSums(raw_counts) # total number of counted reads per sample\n</code></pre> edgeR object preprocessing <pre><code># setting up the experimental design AND the model\n#  -&gt; the first 3 samples form a group, the 3 remaining are the other group\ntreatment &lt;-  c(rep(\"EtOH\",3), rep(\"TAM\",3))\ndge.f.design &lt;- model.matrix(~ treatment)\n\n# creating the edgeR DGE object\ndge.all &lt;- DGEList(counts = raw_counts , group = treatment)  # filtering by expression level. See ?filterByExpr for details\nkeep &lt;- filterByExpr(dge.all)\ndge.f &lt;- dge.all[keep, keep.lib.sizes=FALSE]\ntable( keep )\n</code></pre> <pre><code>keep\nFALSE  TRUE \n39702 15712 \n</code></pre> <p>Around 16k genes are sufficiently expressed to be retained.</p> <pre><code>#normalization\ndge.f &lt;- calcNormFactors(dge.f)\ndge.f$samples\n</code></pre> <p>Each sample has been associated with a normalization factor.</p> edgeR model fitting <p><pre><code># estimate of the dispersion\ndge.f &lt;- estimateDisp(dge.f,dge.f.design , robust = T)\nplotBCV(dge.f)\n</code></pre> </p> <p>This plot is not easy to interpret. It represents the amount of biological variation at different levels of expression. It is directly linked to our ability to detect differential expression.</p> <p>Here it looks about normal compared to other bulk RNA-seq experiments : the variation is comparatively larger for lowly expressed genes.</p> <pre><code># testing for differential expression. \n# This method is recommended when you only have 2 groups to compare\ndge.f.et &lt;- exactTest(dge.f)\ntopTags(dge.f.et) # printing the genes where the p-value of differential expression if the lowest\n</code></pre> <pre><code>Comparison of groups:  TAM-EtOH \n                       logFC   logCPM       PValue          FDR\nENSMUSG00000050272 -8.522762 4.988067 2.554513e-28 3.851950e-24\nENSMUSG00000075014  3.890079 5.175181 2.036909e-25 1.535728e-21\nENSMUSG00000009185  3.837786 6.742422 1.553964e-22 7.810743e-19\nENSMUSG00000075015  3.778523 3.274463 2.106799e-22 7.942107e-19\nENSMUSG00000028339 -5.692069 6.372980 4.593720e-16 1.385374e-12\nENSMUSG00000040111 -2.141221 6.771538 4.954522e-15 1.245154e-11\nENSMUSG00000041695  4.123972 1.668247 6.057909e-15 1.304960e-11\nENSMUSG00000072941  3.609170 7.080257 1.807618e-14 3.407135e-11\nENSMUSG00000000120 -6.340146 6.351489 2.507019e-14 4.200371e-11\nENSMUSG00000034981  3.727969 5.244841 3.934957e-14 5.933521e-11\n</code></pre> <pre><code># see how many genes are DE\nsummary(decideTests(dge.f.et , p.value = 0.01)) # let's use 0.01 as a threshold\n</code></pre> <pre><code>         TAM-EtOH \nDown     109\nNotSig 15393\nUp       210\n</code></pre> <p>The comparison is TAM-EtOH, so \u201cUp\u201d, corresponds to a higher in group TAM compared to group EtOH.</p> edgeR looking at differentially-expressed genes <pre><code>## plot all the logFCs versus average count size. Significantly DE genes are  colored\npar(mfrow=c(1,1))\nplotMD(dge.f.et)\n# lines at a log2FC of 1/-1, corresponding to a shift in expression of x2 \nabline(h=c(-1,1), col=\"blue\") </code></pre> <p></p> <p><pre><code>## Volcano plot\nallGenes = topTags(dge.f.et , n = nrow(dge.f.et$table) )$table\n\nFDRthreshold = 0.01\nlogFCthreshold = 1.0\n# add a column of NAs\nallGenes$diffexpressed &lt;- \"NO\"\n# if log2Foldchange &gt; 1 and pvalue &lt; 0.01, set as \"UP\" \nallGenes$diffexpressed[allGenes$logFC &gt; logFCthreshold &amp; allGenes$FDR &lt; FDRthreshold] &lt;- \"UP\"\n# if log2Foldchange &lt; 1 and pvalue &lt; 0.01, set as \"DOWN\"\nallGenes$diffexpressed[allGenes$logFC &lt; -logFCthreshold &amp; allGenes$FDR &lt; FDRthreshold] &lt;- \"DOWN\"\n\nggplot( data = allGenes , aes( x=logFC , y = -log10(FDR) , col =diffexpressed ) ) + geom_point() + geom_vline(xintercept=c(-logFCthreshold, logFCthreshold), col=\"red\") +\ngeom_hline(yintercept=-log10(FDRthreshold), col=\"red\") +\nscale_color_manual(values=c(\"blue\", \"grey\", \"red\"))\n</code></pre> </p> <pre><code>## writing the table of results\nwrite.csv( allGenes , 'Ruhland2016.edgeR.results.csv')\n</code></pre> edgeR extra stuff <pre><code># how to extract log CPM\nlogcpm &lt;- cpm(dge.f, prior.count=2, log=TRUE)\n</code></pre> <pre><code># there is another fitting method reliying on quasi-likelihood, which is useful when the model is more complex (ie. more than 1 factor with 2 levels)\ndge.f.QLfit &lt;- glmQLFit(dge.f, dge.f.design)\ndge.f.qlt &lt;- glmQLFTest(dge.f.QLfit, coef=2)\n\n# you can see the results are relatively different. The order of genes changes a bit, and the p-values are more profoundly affected\ntopTags(dge.f.et)\ntopTags(dge.f.qlt)\n\n## let's see how much the two methods agree:\npar(mfrow=c(1,2))\nplot( dge.f.et$table$logFC , dge.f.qlt$table$logFC,\nxlab = 'exact test logFC',\nylab = 'quasi-likelihood test logFC')\n\nprint( paste('logFC pearson correlation coefficient :' , cor(dge.f.et$table$logFC ,dge.f.qlt$table$logFC) ) )\n\nplot( log10(dge.f.et$table$PValue ), log10(dge.f.qlt$table$PValue) ,\nxlab = 'exact test p-values (log10)',\nylab = 'quasi-likelihood test p-values (log10)')\n\nprint( paste( \"P-values spearman correlation coefficient\",\ncor( log10(dge.f.et$table$PValue ), log10(dge.f.qlt$table$PValue) , method = 'spearman' )))\n</code></pre> <pre><code>\"logFC pearson correlation coefficient : 0.999997655536736\"\n\"P-values spearman correlation coefficient 0.993238670517236\"\n</code></pre> <p></p> <p>The logFC are highly correlated. FDRs show less correlation but their rank are higly correlated : they come in a very similar order.</p>"},{"location":"days/DE/#tuch-2010-edger-correction","title":"Tuch 2010 - EdgeR correction","text":"<p>We refer you here to section 4.1 of edgeR\u2019s vignette.</p>"},{"location":"days/DE/#additional-importing-counts-from-salmon-with-tximport","title":"Additional : importing counts from salmon with <code>tximport</code>","text":"<p>The <code>tximport</code> R packages offers a fairly simple set of functions to get transcript-level expression quantification from salmon or kallisto into a differential gene expression analysis.</p> <p>Task : import salmon transcript-level quantification in R in order to perform a DE analysis on it using either edgeR or DESeq2. Additional: compare the results with the ones obtained from STAR-aligned reads.</p> <ul> <li>The tximport vignette is a very good guide for this task.</li> <li>If you have not computed them, you can find files with expression quantifications in : <code>/shared/data/Solutions/Liu2015/</code> and <code>/shared/data/Solutions/Ruhland2016/</code></li> </ul>"},{"location":"days/counting/","title":"Counting","text":"<p>Read counting refers to the quantification of an \u201cexpression level\u201d, or abundance, from reads mapped onto a reference genome/transcriptome. This expression level can take several forms, such as a count, or a fraction (RPKM/TPM), and concern different entities (exon, transcript, genes) depending on your biological application.</p> <p>During this lesson, you will learn to:</p> <ul> <li>differentiate between different levels of counting and their relevance for different questions.</li> <li>perform read counting at the gene level for Differential Gene expression.</li> </ul>"},{"location":"days/counting/#material","title":"Material","text":"<p> Download the presentation</p> <p>featureCounts website</p>"},{"location":"days/counting/#read-counting-with-featurecounts","title":"Read counting with featureCounts","text":"<p>The featureCount website provides several useful command-line examples to get started. For more details on the algorithm behavior (with multi/overlapping reads for instance), you can refer to the package\u2019s User\u2019s guide (go to the read summarization chapter).</p> <p>Task : </p> <ul> <li>Decide which parameters are appropriate for counting reads from the Ruhland dataset. Assume you are interested in determining which genes are differentially expressed.</li> <li>Count the reads from one of your BAM files using featureCount.</li> <li> <p>How do the featureCount-derived counts compare to those from STAR ?</p> </li> <li> <p>You can find bam files at <code>/shared/data/Solutions/Liu2015/STAR_Liu2015</code> and <code>/shared/data/Solutions/Ruhland2016/STAR_Ruhland2016</code></p> </li> <li>featureCount requirements : 400M RAM / BAM file</li> <li>featureCount requirements : 2 min CPU time / BAM file</li> </ul> featureCounts script <pre><code>#!/usr/bin/bash\n#SBATCH --job-name=featurecount\n#SBATCH --time=00:30:00\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=4G\n#SBATCH -o count.o\n#SBATCH -e count.e\n\n\nG_GTF=/shared/data/DATA/Mus_musculus.GRCm39.105.gtf\n\ninFOLDER=/shared/data/Solutions/Ruhland2016/STAR_Ruhland2016\noutFOLDER=featureCOUNT_Ruhland2016\n\nml subread\n\nmkdir -p $outFOLDER\n\nfeatureCounts -T 8 -a $G_GTF -t exon -g gene_id -o featureCounts_Ruhland2016.counts.txt \\\n                                    $inFOLDER/SRR3180535_EtOH1_1.fastq.gzAligned.sortedByCoord.out.bam \\\n                                    $inFOLDER/SRR3180536_EtOH2_1.fastq.gzAligned.sortedByCoord.out.bam \\\n                                    $inFOLDER/SRR3180537_EtOH3_1.fastq.gzAligned.sortedByCoord.out.bam \\\n                                    $inFOLDER/SRR3180538_TAM1_1.fastq.gzAligned.sortedByCoord.out.bam \\\n                                    $inFOLDER/SRR3180539_TAM2_1.fastq.gzAligned.sortedByCoord.out.bam \\\n                                    $inFOLDER/SRR3180540_TAM3_1.fastq.gzAligned.sortedByCoord.out.bam\n</code></pre> comparison with STAR counts <p>You can use this little R script to check they are the same :</p> <pre><code>fc = read.table( \"featureCounts_SRR3180535_EtOH1_1.counts.txt\" , header =T)\nrownames( fc ) = fc$Geneid\nhead( fc )\n\nstar = read.table( \"SRR3180535_EtOH1_1.fastq.gzReadsPerGene.out.tab\")\nrownames( star ) = star$V1\nhead( star )\n\n\nstar_count = star[ rownames( fc ) , 'V2' ]\nfC_count = fc$STAR_Ruhland2016.SRR3180535_EtOH1_1.fastq.gzAligned.sortedByCoord.out.bam\nplot(log10( star_count + 1),\n    log10(fC_count+1) )\n\nquantile( star_count  - fC_count)\n</code></pre>"},{"location":"days/design/","title":"RNAseq - technologies and design","text":"<p>Designing your experiment is the first step.  Design is crucial as it conditions the sort of questions that you can ask from your data, as well as the confidence you may have in the answers.</p> <p>Knowing about the sequencing technologies, their strengths and limitations, as well as the RNA-seq analysis pipeline, are the keys to design a successful RNA-seq experiment.</p> <p>After having completed this chapter you will be able to:</p> <ul> <li>describe different sequencing technologies and their application in RNA-seq.</li> <li>differentiate between technical and biological replicates.</li> <li>choose an appropriate sequencing depth and number of replicates depending on your scientific question.</li> </ul>"},{"location":"days/design/#material","title":"Material","text":"<p> Download the presentation</p>"},{"location":"days/enrichment/","title":"Enrichment analysis","text":"<p>Once the reads have been mapped and counted, one can assess the differential expression of genes between different conditions.</p> <p>During this lesson, you will learn to :</p> <ul> <li>perform downstream analysis on gene sets, such as annotation (e.g. GO terms or Reactome pathways) over-representation.</li> </ul>"},{"location":"days/enrichment/#material","title":"Material","text":"<p> Download the presentation</p> <p>Rstudio website</p> <p>clusterProfiler vignette/e-book</p>"},{"location":"days/enrichment/#downstream-analysis-over-representation-analysis","title":"Downstream analysis : over-representation analysis","text":"<p>Having lists of differentially-expressed genes is quite interesting in itself, however when there are many DE genes, it can be interesting to map these results  onto curated sets of genes associated with known biological functions.</p> <p>Here, we propose to use clusterProfiler, which regroups several enrichment detection algorithms onto several databases.</p> <p>We recommend you get inspiration from their very nice vignette/e-book to perform your own analyses.</p> <p>If you do not have a list of DE genes from your previous analysis, you may use the following table:</p> <p>  Ruhland2016 DESeq2 results</p> <p>The proposed correction will concern these.</p> Ruhland2016 analysis with clusterProfiler <p>We begin by reading the results of the DE analysis. Adapt this to your own analysis. Beware that edgeR and DESeq2 use different column names in their result tables (log2FoldChange/logFC , padj/FDR).</p> <pre><code>library(AnnotationHub)\nlibrary(AnnotationDbi)\nlibrary(clusterProfiler)\nlibrary(ReactomePA)\n\nlibrary(org.Mm.eg.db)\n\n\nres = read.csv( 'Ruhland2016.DESeq2.results.csv'  , row.names=1)\n#let's define significance as padj &lt;0.01 &amp; abs(lfc) &gt; 1\nres$sig = abs(res$log2FoldChange)&gt;1 &amp; res$padj&lt;0.01\n\ntable( res$sig )\n</code></pre> <p>Number of non-significant/significant genes </p> <pre><code> FALSE  TRUE \n 18569   401 \n</code></pre> <p>Translating gene ENSEMBL names to their entrezID (this is what clusterProfiler uses), as well as Symbol (named used by most biologist). <pre><code>genes_universe &lt;- bitr(rownames(res), fromType = \"ENSEMBL\",\ntoType = c(\"ENTREZID\", \"SYMBOL\"),\nOrgDb = \"org.Mm.eg.db\")\n\nhead( genes_universe )\n#ENSEMBL ENTREZID  SYMBOL\n#2 ENSMUSG00000033845    27395  Mrpl15\n#4 ENSMUSG00000025903    18777  Lypla1\n#5 ENSMUSG00000033813    21399   Tcea1\n#7 ENSMUSG00000002459    58175   Rgs20\n#8 ENSMUSG00000033793   108664 Atp6v1h\n#9 ENSMUSG00000025907    12421  Rb1cc1\n\ndim(genes_universe)\n# 15878     3\n\nlength(rownames(res))\n# 19378\n</code></pre></p> <pre><code>genes_DE &lt;- bitr(rownames(res)[which( res$sig==T )], fromType = \"ENSEMBL\",\ntoType = c(\"ENTREZID\", \"SYMBOL\"),\nOrgDb = \"org.Mm.eg.db\")\ndim(genes_DE)\n# 387   3\n</code></pre> <p><pre><code># GO \"biological process (BP)\" enrichment\nego_bp &lt;- enrichGO(gene          = as.character(unique(genes_DE$ENTREZID)),\nuniverse      = as.character(unique(genes_universe$ENTREZID)),\nOrgDb         = org.Mm.eg.db,\nont           = \"BP\",\npAdjustMethod = \"BH\",\npvalueCutoff  = 0.01,\nqvalueCutoff  = 0.05,\nreadable      = TRUE)\nhead(ego_bp)\ndotplot(ego_bp, showCategory = 20)\n# sample plot, but with adjusted p-value as x-axis\n#dotplot(ego_bp, x = \"p.adjust\", showCategory = 20)\n</code></pre> </p> <p><pre><code># Reactome pathways enrichment\nreactome.enrich &lt;- enrichPathway(gene=as.character(unique(genes_DE$ENTREZID)),\norganism = \"mouse\",\npAdjustMethod = \"BH\",\nqvalueCutoff = 0.01,\nreadable=T,\nuniverse = genes_universe$ENTREZID)\n\n\ndotplot(reactome.enrich, x = \"p.adjust\")\n</code></pre> </p>"},{"location":"days/mapping/","title":"Reads mapping","text":"<p>Once you are happy with your read sequences in your FASTQ files, you can use a mapper software to align the reads to the genome and thereby find where they originated from.</p> <p>At the end of this lesson, you will be able to :</p> <ul> <li>identify the differences between a local aligner and a pseudo aligner.</li> <li>perform genome indexing appropriate to your data.</li> <li>map your RNA-seq data onto the genome.</li> </ul>"},{"location":"days/mapping/#material","title":"Material","text":"<p> Download the presentation</p> <p>STAR website</p>"},{"location":"days/mapping/#building-a-reference-genome-index","title":"Building a reference genome index","text":"<p>Before any mapping can be achieved, you must first index the genome want to map to. </p> <p>To do this with STAR, you need two files:</p> <ul> <li>a fasta file containing the sequences of the chromosome (or genome contigs)</li> <li>a gtf file containing annotations (ie. where the genes and exons are)</li> </ul> <p>We will be using the Ensembl references, with their accompanying GTF annotations.</p> <p>Note</p> <p>While the data are already on the server here, in practice or if you are following this course without a teacher, you can grab the reference genome data from the Ensembl ftp website.</p> <p>In particular, you will want a mouse DNA fasta file and gtf file.</p> <p>Task : Using STAR, build a genome index for the mouse mitochondrial chromosome.</p> <ul> <li>.fasta and .gtf files are in : <code>/shared/data/DATA/Mouse_MT_genome/</code>.</li> <li>create the index in the folder <code>041_STAR_mouseMT_reference</code></li> <li>the module name for this aligner is <code>star</code>.</li> <li>this job should require less than 4Gb and 10min to run. </li> </ul> <p>STAR basic parameter for genome index generation</p> <p>From the manual. Refer to it for more details</p> <ul> <li><code>--runMode genomeGenerate</code> : running STAR in index generation mode</li> <li><code>--genomeDir &lt;/path/to/genomeDir&gt;</code> : output fodler for the index</li> <li><code>--genomeFastaFiles &lt;/path/to/genome/fasta1&gt;</code> : chromosomes sequences fasta file (can be several files)</li> <li><code>--sjdbGTFfile &lt;/path/to/annotations.gtf&gt;</code> : annotation gtf file</li> <li><code>--runThreadN &lt;NumberOfThreads&gt;</code> : number of threads to run on </li> <li><code>--sjdbOverhang &lt;ReadLength-1&gt;</code> : length of the genomic sequence around the annotated junctions to be used in constructing the splice junctions database. Ideally : read length - 1.</li> </ul> <p>Additionally, because the genome is so small here (we only use the muitochondrial chromosome afterall), you will need the following advanced option:</p> <ul> <li><code>--genomeSAindexNbases 5</code> : must be scalled to <code>min(14, log2(GenomeLength)/2 - 1)</code>, so 5 in our case</li> </ul> <p>Note</p> <p>While your indexing job is running, you can read ahead in STAR\u2019s manual to prepare the next step : mapping your reads onto the indexed reference genome.</p> STAR indexing script <pre><code>#!/usr/bin/bash\n#SBATCH --job-name=star-build\n#SBATCH --time=00:30:00\n#SBATCH --cpus-per-task=2\n#SBATCH --mem=3G\n#SBATCH -o star-build.o\n\n\nG_FASTA=/shared/data/DATA/Mouse_MT_genome/Mus_musculus.GRCm39.dna.chromosome.MT.fa\nG_GTF=/shared/data/DATA/Mouse_MT_genome/Mus_musculus.GRCm39.MT.gtf\n\nml star\n\nmkdir -p 041_STAR_mouseMT_reference\n\nSTAR --runMode genomeGenerate \\\n--genomeDir 041_STAR_mouseMT_reference \\\n--genomeFastaFiles $G_FASTA \\\n--sjdbGTFfile $G_GTF \\\n--runThreadN 4 \\\n--genomeSAindexNbases 5 \\\n--sjdbOverhang 99 </code></pre> <p>Extra task : Determine how you would add an additional feature to your reference, for example for a novel transcript not described by the standard reference.</p> Answer <p>You would edit the gtf file to add your additional feature(s), following the proper format.</p> <p>Note</p> <p>In case you\u2019ve got multiple FASTA files for your genome (eg, 1 per chromosome), you may just list them with the <code>genomeFastaFiles</code> option as follow:</p> <p><code>--genomeFastaFiles /path/to/genome/fasta1.fa /path/to/genome/fasta2.fa /path/to/genome/fasta3.fa ...</code></p>"},{"location":"days/mapping/#mapping-reads-onto-the-reference","title":"Mapping reads onto the reference","text":"<p>basic </p> <p>Task : Using STAR, align the raw FASTQ files of the mouseMT dataset against thed mouse mitochondrial reference you just created</p> <ul> <li>if were not able to complete the previous task you can use the index in <code>/shared/data/Solutions/mouseMT/041_STAR_mouseMT_reference</code></li> <li>search the STAR manual for the option to output a BAM file sorted by coordinate.</li> <li>search the STAR manual for the option to output a geneCounts file.</li> <li>put the results in folder <code>042_STAR_map_raw/</code></li> </ul> <p>STAR basic parameter for mapping</p> <p>Taken again from the manual:</p> <ul> <li><code>--genomeDir &lt;/path/to/genomeDir&gt;</code> : folder where you have put the genome index</li> <li><code>--readFilesIn &lt;/path/to/read1&gt;</code> : path to a fastq file. If the reads are paired, then also include the path to the second fastq file </li> <li><code>--runThreadN &lt;NumberOfThreads&gt;</code>: number of threads to run on.</li> <li><code>--outFileNamePrefix  &lt;prefix&gt;</code> : prefix of the output files, typically something like <code>output_directory/sampleName.</code> </li> </ul> <p>Note</p> <p>Take the time to read the parts of the STAR manual which concern you : a bit of planning ahead can save you a lot of time-consuming/headache-inducing trial and error on your script.</p> <p>Warning</p> <p>Mapping reads and generating a sorted BAM from one of the mouseMT FASTQ file will take less than a minute and very little RAM, but on a real dataset it should take from 15 minutes to an hour per sample and require at least 30G.</p> STAR mapping script <p>We will be using a job array to map each file in different job that will run at the same time.</p> <p>First create a file named <code>sampleNames.txt</code>, containing the sample names:</p> <p><pre><code>sample_a1\nsample_a2\nsample_a3\nsample_a4\nsample_b1\nsample_b2\nsample_b3\nsample_b4\n</code></pre> it can also be found in the cluster at <code>/shared/data/Solutions/mouseMT/sampleNames.txt</code></p> <p><pre><code>#!/usr/bin/bash\n#SBATCH --job-name=star-aln\n#SBATCH --time=00:10:00\n#SBATCH --cpus-per-task=2\n#SBATCH --mem=1G\n#SBATCH -o star-aln.raw.%a.o\n#SBATCH --array 1-8%8\n\nml star\n\nmkdir -p 042_STAR_map_raw\n\nSAMPLE=`sed -n ${SLURM_ARRAY_TASK_ID}p sampleNames.txt`\n\nFASTQ_NAME=/shared/data/DATA/mouseMT/${SAMPLE}.fastq\n\nSTAR --runThreadN 4 --genomeDir 041_STAR_mouseMT_reference \\\n--outSAMtype BAM SortedByCoordinate \\\n--outFileNamePrefix  042_STAR_map_raw/${SAMPLE}. \\\n--quantMode GeneCounts \\\n--readFilesIn $FASTQ_NAME\n</code></pre> it can also be found in the cluster at <code>/shared/data/Solutions/mouseMT/042_STAR_map_raw.sh</code></p> <p>and its results can be found at <code>/shared/data/Solutions/mouseMT/042_STAR_map_raw/</code></p> <p>The options of STAR are :</p> <ul> <li><code>--runThreadN 4</code> : 4 threads to go faster.</li> <li><code>--genomeDir 041_STAR_reference</code> : path of the genome to map to.</li> <li><code>--outSAMtype BAM SortedByCoordinate</code> : output a coordinate-sorted BAM file.</li> <li><code>--outFileNamePrefix 042_STAR_map_raw/${SAMPLE}.</code> : prefix of output files.</li> <li>`\u2013quantMode GeneCounts** : will create a file with counts of reads per gene.</li> <li>`\u2013readFilesIn $FASTQ_NAME ** : input read file.</li> </ul>"},{"location":"days/mapping/#qc-on-the-aligned-reads","title":"QC on the aligned reads","text":"<p>You can call MultiQC on the STAR output folder to gather a report on the individual alignments.</p> <p>Task : use <code>multiqc</code> to generate a QC report on the results of your mapping.</p> <ul> <li>Evaluate the alignment statistics. Do you consider this to be a good alignment?</li> <li>How many unmapped reads are there? Where might this come from, and how would you determine this?</li> <li>What could you say about library strandedness ? </li> </ul> script and answers <p><pre><code>#!/usr/bin/bash\n#SBATCH --job-name=map-multiqc\n#SBATCH --time=00:30:00\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=1G\n#SBATCH -o multiqc-map-raw.o\n\nmultiqc -n 043_multiqc_mouseMT_mapped_raw.html -f --title mapped_raw 042_STAR_map_raw/\n</code></pre> it can also be found in the cluster at <code>043_multiqc_map_raw.sh</code></p> <p> Download the report </p>"},{"location":"days/mapping/#comparison-of-mapping-the-trimmed-reads","title":"Comparison of mapping the trimmed reads","text":"<p>After having mapped the raw reads, we also map the trimmed reads and then compare the results to decide which one we want to use for the rest of our analysis.</p> <p>We will spare you the mapping of the trimmed read and let you directly download the mapping multiqc report:</p> <p> trimmed reads mapping  report </p> For the curious: scripts for the mapping of trimmed reads <pre><code>#!/usr/bin/bash\n#SBATCH --job-name=star-aln\n#SBATCH --time=00:10:00\n#SBATCH --cpus-per-task=2\n#SBATCH --mem=1G\n#SBATCH -o star-aln.trimmed.%a.o\n#SBATCH --array 1-8%8\n\nml star\n\nmkdir -p 044_STAR_map_trimmed\n\nSAMPLE=`sed -n ${SLURM_ARRAY_TASK_ID}p sampleNames.txt`\n\nFASTQ_NAME=030_trim/${SAMPLE}.trimmed.fastq\n\nSTAR --runThreadN 4 --genomeDir 041_STAR_mouseMT_reference \\\n--outSAMtype BAM SortedByCoordinate \\\n--outFileNamePrefix  044_STAR_map_trimmed/${SAMPLE}_trimmed. \\\n--quantMode GeneCounts \\\n--readFilesIn $FASTQ_NAME\n</code></pre> <pre><code>#!/usr/bin/bash\n#SBATCH --job-name=map-multiqc\n#SBATCH --time=00:30:00\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=1G\n#SBATCH -o multiqc-map-trim.o\n\nmultiqc -n 045_multiqc_mouseMT_mapped_trimmed.html -f --title mapped_trimmed 044_STAR_map_trimmed/\n</code></pre>"},{"location":"days/mapping/#additional-pseudo-aligning-with-salmon","title":"ADDITIONAL : pseudo-aligning with salmon","text":"<p>salmon website</p> <p>salmon can allow you to quantify transcript expression without explicitly aligning the sequenced reads onto the reference genome with its gene and splice junction annotations, but to a simplification of the corresponding transcriptome, thus saving computational resources.</p> <p>We refer you to the tool\u2019s documentation in order to see how the reference index is computed.</p> <p>Task : run salmon to quantify the expression of either the Ruhland or Liu dataset. </p> <ul> <li>Use the tool documentation to craft your command line.</li> <li>precomputed indices can be found in <code>/shared/data/DATA/Mouse_salmon_index</code> and <code>/shared/data/DATA/Human_salmon_index</code>.</li> </ul> script <pre><code>#!/usr/bin/bash\n#SBATCH --job-name=salmonRuhland\n#SBATCH --time=01:00:00\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=30G\n#SBATCH -o salmon_ruhland2016.%a.o\n#SBATCH -e salmon_ruhland2016.%a.e\n#SBATCH --array 1-6%1\n\nml salmon\n\noutDIR=salmon_Ruhland2016\n\nmkdir -p $outDIR\n\ndataDIR=/shared/data/DATA/Ruhland2016\n\nsourceFILE=Ruhland2016.fastqFiles.txt\n\nfastqFILE=`sed -n ${SLURM_ARRAY_TASK_ID}p $sourceFILE`\n\ngenomeDIR=/shared/data/DATA/Mouse_salmon_index\n\nsalmon quant -i $genomeDIR -l A \\\n            -r $dataDIR/$fastqFILE \\\n            -p 8 --validateMappings --gcBias --seqBias \\\n            -o $outDIR\n</code></pre>"},{"location":"days/mapping/#additional-mapping-reads-from-ruhland2016-on-the-reference","title":"ADDITIONAL Mapping reads from Ruhland2016 on the reference","text":"<p>Task : Using STAR, align the raw FASTQ files of the mouseMT dataset against thed mouse mitochondrial reference you just created</p> <ul> <li>Mapping reads and generating a sorted BAM from one of the Ruhland2016 et al. FASTQ files should take about 20 minutes.</li> <li>Use the full indexed genome at <code>/shared/data/DATA/Mouse_STAR_index/</code>, rather than the one we just made.</li> <li>IMPORTANT: use the following option in your STAR command: <code>--outTmpDir /tmp/${SLURM_JOB_USER}_${SLURM_JOB_ID}/</code>. You can use the manual to look up what this option does. The slurm variables ensure a distinct directory is created in <code>/tmp/</code> for each user and for each job.</li> </ul> STAR mapping script of the Ruhland2016 data <p>The following sets up an array of tasks to align all samples.</p> <p>Source file : <code>Ruhland2016.fastqFiles.txt</code> :</p> <pre><code>SRR3180535_EtOH1_1.fastq.gz\nSRR3180536_EtOH2_1.fastq.gz\nSRR3180537_EtOH3_1.fastq.gz\nSRR3180538_TAM1_1.fastq.gz\nSRR3180539_TAM2_1.fastq.gz\nSRR3180540_TAM3_1.fastq.gz\n</code></pre> <p>sbatch script :</p> <pre><code>#!/usr/bin/bash\n#SBATCH --job-name=star-aln\n#SBATCH --time=01:00:00\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=30G\n#SBATCH -o star-aln.%a.o\n#SBATCH -e star-aln.%a.e\n#SBATCH --array 1-1%1\n\n\nml star\noutDIR=STAR_Ruhland2016\n\nmkdir -p $outDIR\n\ndataDIR=/shared/data/DATA/Ruhland2016\n\nsourceFILE=Ruhland2016.fastqFiles.txt\n\nfastqFILE=`sed -n ${SLURM_ARRAY_TASK_ID}p $sourceFILE`\n\ngenomeDIR=/shared/data/DATA/Mouse_STAR_index\n\nSTAR --runThreadN 8 --genomeDir $genomeDIR \\\n              --outSAMtype BAM SortedByCoordinate --outReadsUnmapped Fastx \\\n              --outFileNamePrefix $outDIR/$fastqFILE \\\n              --quantMode GeneCounts \\\n              --readFilesIn $dataDIR/$fastqFILE --readFilesCommand zcat \\\n</code></pre> <p>The options of STAR are :</p> <ul> <li>\u2013runThreadN 8  : 8 threads to go faster.</li> <li>\u2013genomeDir $genomeDIR : path of the genome to map to.</li> <li>\u2013outSAMtype BAM SortedByCoordinate  : output a coordinate-sorted BAM file.</li> <li>\u2013outReadsUnmapped Fastx : output the non-mapping reads (in case we want to analyse them).</li> <li>\u2013outFileNamePrefix $outDIR/$fastqFILE : prefix of output files.</li> <li>\u2013quantMode GeneCounts : will create a file with counts of reads per gene.</li> <li>\u2013readFilesIn $dataDIR/$fastqFILE  : input read file.</li> <li>\u2013readFilesCommand zcat : command to unzip the input file.</li> </ul>"},{"location":"days/mapping/#additionnal-star-2-pass","title":"ADDITIONNAL : STAR 2-Pass","text":"<p>Genome annotations are incomplete, particularly for complex eukaryotes : there are many as-of-yet unannotated splice junctions.</p> <p>The first pass of STAR can create a splice junction database, containing both known and novel junctions. This splice junction database can, in turn, be used to guide an improved second round of alignment, using a command like:</p> <pre><code>STAR &lt;1st round options&gt; --sjdbFileChrStartEnd sample_SJ.out.tab\n</code></pre> <p>Task : run STAR in this STAR-2pass mode on the same sample as before and evaluate the results.</p> script <pre><code>#!/usr/bin/bash\n#SBATCH --job-name=star-aln2\n#SBATCH --time=01:00:00\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=30G\n#SBATCH -o star-aln-2pass.%a.o\n#SBATCH -e star-aln-2pass.%a.e\n#SBATCH --array 1-1%1\n\nml star\n\noutDIR=STAR_Ruhland2016\n\nmkdir -p $outDIR\n\ndataDIR=/shared/data/DATA/Ruhland2016\n\nsourceFILE=Ruhland2016.fastqFiles.txt\n\nfastqFILE=`sed -n ${SLURM_ARRAY_TASK_ID}p $sourceFILE`\n\ngenomeDIR=/shared/data/DATA/Mouse_STAR_index\n\nSTAR --runThreadN 8 --genomeDir $genomeDIR \\\n                  --outSAMtype BAM SortedByCoordinate \\\n                  --outFileNamePrefix $outDIR/$fastqFILE.2Pass. \\\n                  --outReadsUnmapped Fastx --quantMode GeneCounts \\\n                  --sjdbFileChrStartEnd $outDIR/${fastqFILE}SJ.out.tab \\\n                  --readFilesIn $dataDIR/$fastqFILE --readFilesCommand zcat \\\n</code></pre>"},{"location":"days/quality_control/","title":"Quality control","text":"<p>Quality Control is the essential first step to perform once you receive your data from your sequencing facility, typically as <code>.fastq</code> or <code>.fastq.gz</code> files.</p> <p>During this session, you will learn to :</p> <ul> <li>create QC report for a single file with fastqc</li> <li>aggregate multiple QC reports using multiqc</li> <li>interpret the QC reports for an entire RNA-seq experiment</li> </ul> <p>Note</p> <p>Although we aim to present tools as stable as possible, software evolve and their precise interface can change with time. We strongly recommend you consult each command\u2019s help page or manual before launching them. To this end, we provide links to each tool\u2019s website. </p> <p>This can also be useful to you if you are following this course without access to a compute cluster and have to install these tools on your machine.</p>"},{"location":"days/quality_control/#material","title":"Material","text":"<p> Download the presentation</p> <p>FastQC website</p> <p>MultiQC website</p>"},{"location":"days/quality_control/#meet-the-datasets","title":"Meet the datasets","text":"<p>We will be working with three datasets. The first is a small toy dataset for experimentation, the other two correspond to actual data:</p> <ul> <li> <p>toy dataset: RNAseq of mice mitochondrial mRNA</p> <ul> <li>8 samples : 4 in group A and 4 in group B</li> <li>single-end, 100bp reads</li> <li>on the cluster: <code>/shared/data/DATA/mouseMT/</code></li> <li> fastq-files</li> </ul> </li> <li> <p>Liu et al. (2015) \u201cRNA-Seq identifies novel myocardial gene expression signatures of heart failure\u201d Genomics 105(2):83-89 https://doi.org/10.1016/j.ygeno.2014.12.002</p> <ul> <li>Gene Expression Omnibus id: GSE57345</li> <li>Samples of Homo sapiens heart left ventricles : 3 with heart failure, 3 without</li> <li>paired-end, 100bp reads</li> <li>on the cluster: <code>/shared/data/DATA/Liu2015/</code></li> </ul> </li> <li> <p>Ruhland et al. (2016) \u201cStromal senescence establishes an immunosuppressive microenvironment that drives tumorigenesis\u201d Nature Communications 7:11762 https://dx.doi.org/10.1038/ncomms11762</p> <ul> <li>Gene Expression Omnibus id: GSE78128</li> <li>Samples of Mus musculus skin fibroblasts : 3 non-senescent (EtOH), 3 senescent (TAM)</li> <li>single-end, 50bp reads</li> <li>on the cluster: <code>/shared/data/DATA/Ruhland2016/</code></li> </ul> </li> </ul>"},{"location":"days/quality_control/#fastqc-a-report-for-a-single-fastq-file","title":"FastQC : a report for a single fastq file","text":"<p>FastQC is a nice tool to get a variety of QC measures from files such as <code>.fastq</code>, <code>.bam</code> or <code>.sam</code> files. </p> <p>Although it has many options, the default parameters are often enough for our purpose :</p> <pre><code>fastqc -o &lt;output_directory&gt; file1.fastq file2.fastq ... fileN.fastq\n</code></pre> <p>FastQC is reasonably intelligent, and will try to recognise the file format and uncompress it if necessary (so no need to decompress manually).</p> <p>So, let\u2019s apply fastQC to the toy dataset. </p> <p>Task: </p> <p>On the cluster, in the folder <code>day1</code>, create a new folder <code>mouseMT</code> and enter it.</p> <pre><code>mkdir mouseMT\ncd mouseMT\n</code></pre> <p>Then, create here a new text file name <code>010_fastqc.sh</code> (with <code>nano</code>, or on your local computer ), and paste the following content in it:</p> <pre><code>#!/usr/bin/bash\n#SBATCH --job-name=fastqc\n#SBATCH --time=01:00:00\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=1G\n#SBATCH -o fastqc_mouseMT.o\n\nml fastqc\n\n# creating the output folder\nmkdir -p 010_fastqc/\n\nfastqc -o 010_fastqc /shared/data/DATA/mouseMT/*.fastq\n</code></pre> <p>Save it, and then submit it to the cluster:</p> <pre><code>sbatch 010_fastqc.sh\n</code></pre> <p>Monitor it with <code>squeue</code>. It should take around 15 to 30 seconds in total.</p> <p>Once it has run, look at the content of the job output file, <code>fastqc_mouseMT.o</code>.</p> <p>Check that all analysis were complete and that there were no error.</p> <p>You can also check that the <code>010_fastqc/</code> folder contains several html files:</p> <pre><code>ls 010_fastqc/\n</code></pre> <p>output: <pre><code>sample_a1_fastqc.html  sample_a2_fastqc.html  sample_a3_fastqc.html  sample_a4_fastqc.html  sample_b1_fastqc.html  sample_b2_fastqc.html  sample_b3_fastqc.html  sample_b4_fastqc.html\nsample_a1_fastqc.zip   sample_a2_fastqc.zip   sample_a3_fastqc.zip   sample_a4_fastqc.zip   sample_b1_fastqc.zip   sample_b2_fastqc.zip   sample_b3_fastqc.zip   sample_b4_fastqc.zip\n</code></pre></p> <p>Unfortunately, we cannot consult the html files content directly on the cluster. </p> <p>We will look at one of these html report on the toy dataset, and one of the pre-computed report from one of our other datasets.</p> <ul> <li>repatriate one of the html report of the mouseMT dataset to your local computer, as well as the one you can find in: <code>/shared/data/Solution/Liu2015/010_fastqc/SRR3180538_TAM1_1_fastqc.html</code></li> <li>Look at these two QC report. What are your conclusions ? Would you want to perform some operations on the reads such as low-quality bases trimming, removal of adapters ?</li> </ul> <p>Reminder</p> <p>to get the data from the distant server to your machine, you may use an SFTP client (filezilla, mobaXterm), or the command line tool from your machine :   <code>scp login@xx.xx.xx:~/path/to/file.txt .</code></p> <p>Extra Task: </p> <ul> <li>Write one or more slurm-compatible sbatch scripts in your home directory that run FastQC analysis on each FASTQ file from the Liu2015 and Ruhland216 datasets. These are accessible at : <code>/shared/data/DATA/Liu2015/</code> and <code>/shared/data/DATA/Ruhland2016/</code>. </li> </ul> <p>Warning</p> <p>Make sure your script writes the fastqc output to a folder within your own home directory.</p> <p>Important points</p> <ul> <li>FastQC RAM requirements : 1Gb is more than enough.</li> <li>FastQC time requirements : ~ 5min / read file.</li> <li>try to make sure FastQC outputs all reports in the same directory, this will save time for the next step ;-).</li> <li>in your script, don\u2019t forget to load fastqc : <code>ml fastqc</code>.</li> <li>there is no need to copy the read files to your home directory (in fact, it is good practice not to: it would create data redundancy, and we won\u2019t have enough space left on the disk anyway\u2026).</li> </ul> Liu2015 FastQC sbatch script <pre><code>#!/usr/bin/bash\n#SBATCH --job-name=fastqc\n#SBATCH --time=01:00:00\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=1G\n#SBATCH -o fastqc_Liu2015.o\n\nml fastqc\n\n# creating the output folder\nmkdir -p 010_fastqc/\n\nfastqc -o 010_fastqc /shared/data/DATA/Liu2015/*.fastq.gz\n</code></pre> <p>on the cluster, this script is also in <code>/shared/data/Solutions/Liu2015/010_fastqc.sh</code></p> <p>Note that alternatively, you could have one sbatch script per sample. On a simple and relatively fast task such as this one, it is not to problematic.</p> <p>However on more important tasks such as mapping, treating each file in separate script is better because you can submit all scripts at once and they will then run in parallel, whereas if they are all treated in the same script, the samples would be handled sequentially and the overall job would take much longer to finish.</p> <p>NB: the actual recommended approach is to use SLURM arrays, which let you have a single script, but have different jobs execute in parallel. We show below how to do.</p> Ruhland2016 FastQC sbatch script <pre><code>#!/usr/bin/bash\n#SBATCH --job-name=fastqc\n#SBATCH --time=00:30:00\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=1G\n#SBATCH -o fastqc_Ruhland2016.o\n\nml fastqc\n\nmkdir -p 010_fastqc/\n\nfastqc -o 010_fastqc /shared/data/DATA/Ruhland2016/*.fastq.gz\n</code></pre> <p>on the cluster, this script is also in <code>/shared/data/Solutions/Ruhland2016/010_fastqc.sh</code></p> Alternative sbatch script using array job <p>Here is a solution where all files from a same dataset can be processed in parallel (recommended) by using slurm array jobs.</p> <p>First, have a file named <code>Ruhland2016.fastqFiles.txt</code> containing the sample fastq file names :</p> <pre><code>SRR3180535_EtOH1_1.fastq.gz\nSRR3180536_EtOH2_1.fastq.gz\nSRR3180537_EtOH3_1.fastq.gz\nSRR3180538_TAM1_1.fastq.gz\nSRR3180539_TAM2_1.fastq.gz\nSRR3180540_TAM3_1.fastq.gz\n</code></pre> <p>Then, in the same folder, you can create this sbatch script :</p> <pre><code>#!/usr/bin/bash\n#SBATCH --job-name=fastqc\n#SBATCH --time=00:30:00\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=1G\n#SBATCH -o fastqc_Ruhland2016.%a.o\n#SBATCH -e fastqc_Ruhland2016.%a.e\n#SBATCH --array 1-6%6\n\nml fastqc\n\ndataDir=/shared/data/DATA/Ruhland2016\n\nsourceFILE=Ruhland2016.fastqFiles.txt\n\n## retrieving 1 filename from Ruhland2016.fastqFiles.txt\nfastqFILE=`sed -n ${SLURM_ARRAY_TASK_ID}p $sourceFILE`\n\nmkdir -p 010_fastqc/\nfastqc -o 010_fastqc/ $dataDir/$fastqFILE\n</code></pre> <p>When submitted with <code>sbatch</code>, this script will spawn 6 tasks in parallel, each with a different value of <code>${SLURM_ARRAY_TASK_ID}</code>.</p> <p>This is the recommended option : this allows you to launch all your job in parallel with a single script.</p> Interpretation of a report <p> Download an annotated report</p> <p>We also refer you to this nice interpertation guide</p> <p>pre-computed reports can be found in :</p> <ul> <li><code>/shared/data/Solutions/Ruhland2016/010_fastqc/</code></li> <li><code>/shared/data/Solutions/Liu2015/010_fastqc/</code></li> <li><code>/shared/data/Solutions/mouseMT/010_fastqc/</code></li> </ul>"},{"location":"days/quality_control/#multiqc-grouping-multiple-reports","title":"MultiQC : grouping multiple reports","text":"<p>In practice, you  likely will have more than a couple of samples (maybe even more than 30 or 50\u2026) to handle: individually consulting and comparing the QC reports of each would be tedious.</p> <p>MultiQC is a tool that lets you combine multiple reports in a single, interactive document that let you explore your data easily.</p> <p>Here, we will be focusing on grouping FastQC reports, but MultiQC can also be applied to the output or logs of other bioinformatics tools, such as mappers, as we will see later.</p> <p>In its default usage, <code>multiqc</code> only needs to be provided a path where it will find all the individual reports, and it will scan them and write a report named <code>multiqc_report.html</code>.</p> <p>Although the default behaviour is quite appropriate, with a couple of options we get a slightly better control over the output:</p> <ul> <li><code>--interactive</code> : forces the plot to be interactive even when there is a lot of samples (this option can lead to larger html files).</li> <li><code>-n &lt;filename&gt;</code> : specify the name of the output file name.</li> </ul> <p>For instance, a possible command line could be : <pre><code>multiqc -n &lt;output_file.html&gt; --interactive &lt;fastqc reports folder&gt;/\n</code></pre></p> <p>There are many additional parameters which let you customize your report. Use <code>multiqc --help</code> or visit their documentation webpage to learn more.</p> <p>Task: </p> <ul> <li> <p>Write an sbatch script to run MultiQC for the toy dataset.</p> <p>to follow the naming convention with started with, you can use the following names:</p> <ul> <li>sbatch script: <code>020_multiqc.sh</code> </li> <li>output report: <code>020_multiqc_mouseMT.html</code></li> </ul> </li> <li> <p>Look at the generated html report. What are your conclusions ?</p> </li> </ul> <p>Info</p> <ul> <li>MultiQC RAM requirements : 1Gb should be more than enough.</li> <li>MultiQC time requirements : ~ 1min / read file.</li> <li>Exceptionally, there is no need to load multiqc as a module (it is not part of ComputeCanada and we installed it directly on the cluster, on other clusters it may not be the same).</li> <li>Use <code>multiqc --help</code> to check the different options</li> </ul> mouseMT MultiQC sbatch script <p><pre><code>#!/usr/bin/bash\n#SBATCH --job-name=multiqc\n#SBATCH --time=00:30:00\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=1G\n#SBATCH -o multiqc_mouseMT.o\n\nmultiqc -n 020_multiqc_mouseMT.html -f --title raw_fastq 010_fastqc/\n</code></pre> On the cluster, this script is also in <code>/shared/data/Solutions/mouseMT/020_multiqc.sh</code></p> <p> Download the results of this script</p> Interpretation of the report for the mouseMT data. <p></p> <p>The PHRED quality of reads drop below 30 around base 75. All samples seem affected. One sample seems to stand out a bit</p> <p></p> <p>Mean quality scores are on average fairly high.</p> <p></p> <p>Most samples do not deviate too much, Except for two samples which clearly contains more GC% rich content compared to the other samples. This may be indicative of contamination or just a difference in composition.</p> <p></p> <p>Here we see that some sequence are duplicated, but not many. Normally in the context of RNA-seq some transcripts are present in a large number of copies in the samples, so we would expect to see more over-represented sequences.  This is not the case here because this is a toy dataset with a very small number of reads.</p> <p></p> <p>We see a clear trend of adapter contamination for one sample as we get closer to the reads\u2019 end. Note the y-scale though : we never go above a 6% content per sample.</p> <p>Overall, we can conclude that one sample in particular stands out in particular. </p> <p>We should note its name and monitor it closely as we go through the rest of our analysis pipeline.</p> <p>At the moment, the analysis steps are independent from sample to sample, so keeping that potential outlier is not a problem. However, when we come to differential analysis we will have to decide if we keep this sample or exclude it.</p> Interpretation of the report for the Liu2015 data. <p>We will interpret the report for the Liu2015 data.</p> <p> Download the report</p> <p></p> <p>The PHRED quality of reads drop below 30 around base 75. All samples seem affected. One sample seems to have some quality drops at specific timepoints/positions.</p> <p></p> <p>Mean quality scores are on average fairly high, but some reads exhibit low values.</p> <p></p> <p>Most samples do not deviate too much from the expected curve. The two samples colored in orange and red have a mode for a very specific value. This may be indicative of contamination, retaining specific rRNA, or adapter sequence content.</p> <p></p> <p>Ns are present at specific positions in specific samples, in particular for one sample. This is reminiscent of the PHRED quality curves at the top of the report. It seems some flowcells had a problem at specific time-point/positions.</p> <p></p> <p>This is colored red because this would be a problem if the data was coming from genomic DNA sequencing. However here we are in the context of RNA-seq : some transcripts are present in a large number of copies in the samples, and consequently it is expected that some sequences are over-represented.</p> <p></p> <p>We see a clear trend of adapter contamination as we get closer to the reads\u2019 end. Note the y-scale though : we never go above a 6% content per sample.</p> <p>Overall, we can conclude that these samples all suffer from some adapter content and a lower quality toward the reads\u2019 second half. Furthermore, a few samples have a peculiar N pattern between bases 20 and 30.</p> <p>It is then strongly advised to either :</p> <ul> <li>perform some trimming : remove adapter sequences + cut reads when average quality becomes too low</li> <li>use a mapper that takes base quality in account AND is able to ignore adapter sequence (and even then, you could try mapping on both trimmed and untrimmed data to see which is the best)</li> </ul> <p>extra Task</p> <p>Write and execute sbatch scripts to run a MultiQC for the Liu2015 and the Ruhland2016 datasets.</p> MultiQC sbatch script for Ruhland2016 <p>This script fetches the report from the <code>Solutions/</code> folder.</p> <p>You may adapt it to to point to your own results if you want.</p> <pre><code>#!/usr/bin/bash\n#SBATCH --job-name=multiqc_Ruhland2016\n#SBATCH --time=00:30:00\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=1G\n#SBATCH -o multiqc_Ruhland2016.o\n\nmultiqc -f 020_multiqc_Ruhland2016.html /shared/data/Solutions/Ruhland2016/010_fastqc/\n</code></pre> <p>On the cluster, this script is also in <code>/shared/data/Solutions/Ruhland2016/020_multiqc.sh</code></p> MultiQC sbatch script for Liu2015 <p>This script fetches the report from the <code>Solutions/</code> folder.</p> <p>You may adapt it to to point to your own results if you want.</p> <pre><code>#!/usr/bin/bash\n#SBATCH --job-name=multiqc_Liu2015\n#SBATCH --time=00:30:00\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=1G\n#SBATCH -o multiqc_Liu2015.o\n\nmultiqc -f 020_multiqc_Liu2015.html /shared/data/Solutions/Liu2015/010_fastqc/\n</code></pre> <p>On the cluster, this script is also in <code>/shared/data/Solutions/Liu2015/020_multiqc.sh</code></p>"},{"location":"days/quality_control/#extra-retrieving-published-datasets","title":"EXTRA : retrieving published datasets","text":"<p>Note</p> <p>If you are following this course with a teacher, then the for the data is already on the server. There is no need to download it again.</p> <p>Most NGS data is deposited at the Short Read Archive (SRA) hosted by the NCBI, with links from the Gene Expression Omnibus (GEO)</p> <p>Several steps are required to retrieve data from a published study :</p> <ol> <li>find GEO or SRA identifier from publication.</li> <li>find the \u201crun\u201d identifiers for each sample (SRR).</li> <li>use SRA Toolkit to dump data from the SRR repository to FASTQ files.</li> </ol> <p>For example, on the Liu2015 dataset :</p> <ol> <li>Locate in their publication the GEO accession: GSE57345 </li> <li>Use the NCBI search engine to find this accession : GSE57345</li> <li>This project is made of several sub-projects. Scroll down, and in the table find the Bioproject id : PRJNA246308 </li> <li>Go to the SRA run selector, enter the Bioproject id \u2013&gt;</li> <li>From the results of your search, select all relevant runs</li> <li>Click on \u201cAccession List\u201d in the Select table </li> </ol> <p></p> <ol> <li>use <code>fastq-dump</code> (part of the SRA Toolkit) on the downloaded accession list. For example, with a very small dataset (45Mb only, but already, this can take about a minute):</li> </ol> <p><code>fastq-dump --gzip --skip-technical --readids --split-files --clip SRR306383</code></p> <p>Note</p> <ul> <li><code>fastq-dump</code> takes a very long time</li> <li>You\u2019ll need to know the nature of the dataset (library type, paired vs single end, etc.) before analysing it.</li> <li>More information about fastq-dump</li> </ul>"},{"location":"days/server_login/","title":"Server login + unix fresh up","text":"<p>To conduct the practicals of this course, we will be using a dedicated High Performance Computing cluster.  This matches the reality of most NGS workflows, which cannot be completed in a reasonable time on a single machine. </p> <p>To interact with this cluster, you will have to log in to a distant head node. From there you will be able to distribute your computational tasks to the cluster using a job scheduler called Slurm.</p> <p>This page will cover our first contact with the distant cluster. </p> <p>You will learn to :</p> <ul> <li>understand a typical computer cluster architecture.</li> <li>connect to the server.</li> <li>use the command line to perform basic operations on the head node.</li> <li>exchange files between the server and your own machine.</li> <li>submit a job to the cluster.</li> </ul> <p>Note</p> <p>If you are doing this course on your own, then the distant server provided within the course will not be available.  Feel free to ignore or adapt any of the following steps to your own situation.</p>"},{"location":"days/server_login/#the-computing-cluster","title":"The computing cluster","text":"<p>The computing cluster follows an architecture that enables several users to distribute computational tasks among several machines which share a number of resources, such as a common file system.</p> <p></p> <p>Users do not access each machine individually, but rather connect to a head node. From there, they can interact with the cluster using the job scheduler (here slurm). The job scheduler\u2019s role is to manage where and how to run the jobs of all users, such that waiting time is minimized and resource usage is optimized.</p> <p>Warning</p> <p>Everyone is connected to the same head node. Do not perform compute-intensive tasks on it or you will slow everyone down! </p>"},{"location":"days/server_login/#connect-to-the-server","title":"Connect to the server","text":"<p>Say you want to connect to cluster with address <code>xx.xx.xx.xx</code> and your login is <code>login</code>.</p> <p>Warning</p> <p>If you are doing this course with a teacher, use the link, login and password provided before or during the course. </p> <p>The first step will be to open a terminal</p> MacLinuxWindows <p>Open a terminal, for instance with the application Xterm, or Xquartz. </p> <p>Open a new terminal.</p> <p>Open the application mobaXterm (or any ssh-enabling terminal application you prefer).</p> <p>On mobaXterm, click on \u201cStart a local Terminal\u201d.</p> <p>In the terminal type the following command:</p> <pre><code>ssh login@xx.xx.xx.xx\n</code></pre> <p>When prompted for your password, type it and press Enter. </p> <p>Note</p> <p>There is no cursor or \u2018\u25cf\u2019 character appearing while you type your password. This is normal.</p> <p>After a few seconds, you should be logged into the head node and ready to begin.</p>"},{"location":"days/server_login/#using-command-line-on-the-cluster","title":"Using command line on the cluster","text":"<p>Now that you are in the head node, it is time to get acquainted with your environment and to prepare the upcoming practicals.  We will also use this as a short reminder about the UNIX command line.</p> <p>You can also refer to this nice Linux Command Line Cheat Sheet.</p> <p>At any time, you can get the location (folder) your terminal is in at by typing the \u201cprint working directory\u201d command:</p> <pre><code>pwd\n</code></pre> <p>When you start a session on a distant computer, you are placed in your <code>home</code> directory. So the cluster should return something like:</p> <pre><code>/shared/home/&lt;login&gt;\n</code></pre> <p>From then, we are going to do a little step-by-step practical to go through some of bash\u2019s most useful command for this course.</p>"},{"location":"days/server_login/#creating-a-directory","title":"Creating a directory","text":"<p>practical</p> <p>Use the command line to create a repository called <code>day1</code> where you will put all materials relating to this first day.</p> Answer <pre><code>mkdir day1\n</code></pre> <p>practical</p> <p>Move to that directory.</p> Answer <pre><code>cd day1\n</code></pre> <p>The directory <code>/shared/data/</code> contains data and solutions for most practicals. </p> <p>practical</p> <p>List the content of the <code>/shared/data/</code> directory.</p> Answer <pre><code>ls /shared/data/\n</code></pre> <p>Note</p> <p>You don\u2019t need to move to that directory to list its contents!</p> <p>practical</p> <p>Copy the script <code>010_fastqc.sh</code> from  <code>/shared/data/Solutions/mouseMT</code> into your current directory, and then print the content of this script to the screen.</p> Answer <p><pre><code>cp /shared/data/Solutions/mouseMT/010_fastqc.sh .\nmore 010_fastqc.sh\n</code></pre> output: <pre><code>#!/usr/bin/bash\n#SBATCH --job-name=fastqc\n#SBATCH --time=01:00:00\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=1G\n#SBATCH -o fastqc_mouseMT.o\n\nml fastqc\n\n# creating the output folder\nmkdir -p 010_fastqc/\n\nfastqc -o 010_fastqc /shared/data/DATA/mouseMT/*.fastq\n</code></pre></p> <p>We\u2019ll see what all this means soon.</p>"},{"location":"days/server_login/#creating-and-editing-a-file","title":"Creating and editing a file","text":"<p>To edit files on the distant server, we will use the command line editor <code>nano</code>. It is far from the most complete or efficient one, but it can be found on most servers and is arguably among the easiest to start with.</p> <p>Note</p> <p>Alternatively, feel free to use any other CLI editor you prefer, such as <code>vi</code>.</p> <p>To start editing a file named <code>test.txt</code>, type :</p> <pre><code>    nano test.txt\n</code></pre> <p>You will be taken to the <code>nano</code> interface :</p> <p></p> <p>Type in your favorite movie quote, and then exit by pressing <code>Ctrl+x</code> (<code>control+x</code> on a Mac keyboard), and then <code>y</code> and <code>Enter</code> when prompted to save the modifications you just made.</p> <p>You can check that your modifications were saved by typing</p> <pre><code>more test.txt\n</code></pre>"},{"location":"days/server_login/#exchanging-files-with-the-server","title":"Exchanging files with the server","text":"<p>Whether you want to transfer some data to the cluster or retrieve the results of your latest computation, it is important to be able to exchange files with the distant server.</p> <p>There exists several alternatives, depending on your platform and preferences.</p> filezillamobaXtermcommand line <p>There are nice and free software with graphical user interfaces, such as filezilla, to help you manage exchanges with the distant server. Feel free to install and experiment with it during the break. </p> <p>If you are using mobaXterm, the left panel should provide a graphical SFTP browser in the left sidebar which allows you to browse and drag and drop files directly from/to the remote server.</p> <p></p> <p>We will use <code>scp</code>.</p> <p>To copy a file from the server to your machine, use this syntax on a terminal in your local machine (open a new terminal if necessary).</p> <pre><code>scp &lt;login&gt;@&lt;server-adress&gt;:/path/to/file/on/server/file.txt /local/destination/\n</code></pre> <p>For example, to copy the file <code>test.txt</code> you just created in the folder <code>day1/</code>, to your current (local) working directory :</p> <pre><code>scp login@xx.xx.xx.xx:~/day1/test.txt .\n</code></pre> <p>To copy a file from your machine to the server (NB: here <code>~</code> will be interpreted as your home directory, this is a useful and time-saving shorthand):</p> <pre><code>scp /path/to/file/local/file.txt &lt;login&gt;@&lt;server-adress&gt;:/destination/on/server/\n</code></pre> <p>practical</p> <p>Retrieve the file <code>test.txt</code>, which you created in the previous practicals, from the distant server to your local machine. </p> <p>practical</p> <p>create a text file on your local computer (using wordpad on windows, Text Edit on Mac, or gedit on linux). Save that file, and then send it to the distant server.</p> <p>Bug</p> <p>Est-ce que j\u2019introduirai ici l\u2019instance Rstudio? Rstudio est present dans la majorite des serveurs et il permet de gerer plutot aisement l\u2019edition de texte, l\u2019exploration de fichier, et les terminaux depuis une seule interface.</p>"},{"location":"days/server_login/#bash-scripts","title":"bash scripts","text":"<p>So far we have been executing bash commands in the interactive shell. This is the most common way of dealing with day-to-day with our data on the server.</p> <p>However when you have to do some more serious job, such as what we will want to do on RNAseq data, you want to use scripts, which are just normal text files which contains bash commands.</p> <p>Scripts :</p> <ul> <li>keep a written trace of your analysis, so they enhance its reproducibility. </li> <li>make it easier to correct something in an analysis (you don\u2019t have to retype the whole command, just change the part that is wrong)</li> <li>often necessary when we want to submit the big computing jobs to the cluster</li> </ul> <p>Create a new text file name <code>myScript.sh</code> on the server (you can use nano or create it on your local machine and then transfer it to the server). </p> <p>Then, type this in the file:</p> <pre><code>#!/usr/bin/bash    \n\n## this is a comment, here to document this script\n## whatever is after # is not interpreted as code.\n\n# the echo command prints whatever text follows to the screen: \necho \"looking at the size of the elements of /shared/data/\"\n\nsleep 15 # making the script wait for 15 seconds - this is just so we can see it later on. \n\n# du : \"disk usage\", a command that returns the size of a folder structure.\ndu -h -d 2 /shared/data/\n</code></pre> <p>The first line is not 100% necessary at this step, but it will be in the next part, so we mght as well put it in now. It helps some software know that this file contains bash code. </p> <p>Then to execute the script, navigate in a terminal open on the server to the place where the script is, and execute the following command:</p> <pre><code>sh myScript.sh\n</code></pre> <p>Warning</p> <p>Be sure to execute the script from the folder that it is in. Otherwise you would have to specify in which folder to find the script using its path.</p> <p>This should have printed a number of information about the size of <code>/shared/data/</code> subfolders to the screen.</p> <p>Bug</p> <p>Est-ce que j\u2019inclue ici un layus sur les variables ? ca n\u2019est utile que pour ceux qui feront des job array, donc je pourrais le mettre la bas a la place</p>"},{"location":"days/server_login/#submitting-jobs","title":"Submitting jobs","text":""},{"location":"days/server_login/#submitting-a-simple-script","title":"Submitting a simple script","text":"<p>When connected to the server, when </p> <p>Jobs can be submitted to the compute cluster using bash scripts, with a facultative little preamble which tells the cluster about your computing resources needs and a few additional options.</p> <p>Each time a user submits a job to the cluster, SLURM checks how much ressource they asked for with respect to the amount available right now in the cluster and how much resources are allowed for that user. </p> <p>If there is enough resource available then it will launch the job on one or several of its worker node. If not, then it will wait for the required resources to become available and then launch the job.</p> <p>To start with, you can have a look at what is happening right now in the cluster with the command:</p> <pre><code>squeue\n</code></pre> <p>On the small course server, there should not be much (if anything), but on a normal cluster you would see something like:</p> <pre><code>JOBID     PARTITION           NAME                USER      STATE     TIME        TIME_LIMIT  QOS       NODELIST(REASON)\n48175517  a100                cryocare            user1     PENDING   0:00        6:00:00     gpu6hours (None)                                  48168955  a100                ecoli_pool2         user2     RUNNING   4:09:18     6:00:00     6hours    sgi77                                   47806032  a100                GAN_final           user3     RUNNING   4-04:10:20  13-20:00:00 projects  sgi64                                   47975434  a100                GAN_256_8192        user3     RUNNING   1-18:21:39  7-00:00:00  1week     sgi71                                   48174629  a100                x-35780_y20062      user4     RUNNING   13:36       7:00:00     gpu1day   sgi74                                   </code></pre> <p>The columns correspond to :</p> <ul> <li>JOBID : the job id, which is the number that SLURM uses to identify any job</li> <li>PARTITION : which part of the cluster is that job executing at</li> <li>NAME : the name of the job</li> <li>USER : which user submitted the job</li> <li>STATE : whether the job is currently <code>RUNNING</code>, <code>PENDING</code>, <code>COMPLETED</code>, <code>FAILED</code></li> <li>TIME : how long has this job been running for</li> <li>TIME_LIMIT : how long will the job be allowed to run</li> <li>QOS : which queue of the cluster is the job. Queues are a way to organize jobs in different categories of resource usage.</li> <li>NODELIST(REASON) : which worker node(s) is the job running on.</li> </ul> <p>You can have more info on squeue documentation</p> <p>Now, you will you will want to execute your <code>myScript.sh</code> script as a job on the cluster.</p> <p>This can be done with the <code>sbatch</code> command.</p> <p>The script can stay the same (for now), but there are an important aspect of <code>sbatch</code> we want to handle: the script will not be executing in our terminal directly, but on the worker node. </p> <p>That means that there is no screen to print to. So to still view the output of the script, SLURM will write the printed output in a file which we will be able to read when the job is finished (with <code>more</code> or <code>less</code>). By default SLURM will name this file something like <code>slurm-&lt;jobid&gt;.out</code>, which is not very informative to us; so instead of keeping the default we will give our own output file name to <code>sbatch</code> with the option <code>-o</code>. For example I will name it <code>myOutput.o</code>.</p> <p>In the terminal, navigate to where you have you <code>myscript.sh</code> file on the distant server and type</p> <pre><code>sbatch -o=myOutput.o  myScript.sh\n</code></pre> <p>You should see an output like : <pre><code>sbatch: Submitted batch job 41\n</code></pre></p> <p>Letting you know that your job has the jobid 41 (surely your will be different).</p> <p>Directly after this, quickly type:</p> <pre><code>squeue\n</code></pre> <p>If you were fast enough, then you will see your script <code>PENDING</code> or <code>RUNNING</code>.</p> <p>If not, then that means that your script has finished running. You do not know yet if it succeded or failed. To check this you need to have a look at the output file, <code>myOutput.o</code> in our case.</p> <p>If everything worked you will see the normal output of your script.  Otherwise you will see some error messages.</p>"},{"location":"days/server_login/#specifying-resources-needed-to-slurm","title":"Specifying resources needed to SLURM","text":"<p>When submitting the previous job, we did not specify to SLURM about our resource needs, which means that SLURM assigned it the default resources:  * 1 hour  * 1 cpu  * 1 GB</p> <p>Often, we will want something different from that, and so we will use options in order to specify what we need.</p> <p>For example:</p> <ul> <li><code>--time=00:30:00</code> : time reserved for the job : 30min. </li> <li><code>--mem=2G</code> : memory for the job: 2GB</li> <li><code>--cpus-per-task=4</code> : 4 CPUs for the job </li> </ul> <p>So your <code>sbatch</code> command line will start to get a bit long and unwieldy. It can also be difficult to remember exactly how much RAM and time we need for each script.</p> <p>So SLURM provides a fairly simple way to add this information to the scripts, by adding lines starting with <code>#SBATCH</code> after the first line.</p> <p>For use it could look like this:</p> <pre><code>#!/usr/bin/bash\n#SBATCH --job-name=test\n#SBATCH --time=00:30:00\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=2G\n#SBATCH -o test_log.o\n\n\necho \"looking at the size of the elements of /shared/data/\"\nsleep 15 # making the script wait for 15 seconds - this is just so we can see it later on. \n# `du` is \"disk usage\", a command that returns the size of a folder structure.\ndu -h -d 2 /shared/data/\n</code></pre> <p>I also added the following options :</p> <ul> <li><code>#SBATCH --job-name=test</code> : the job name</li> <li><code>#SBATCH -o test_log.o</code> : file to write output or error messages</li> </ul> <p>We would then submit the script with a simple <code>sbatch myScript.sh</code> without additional options.</p> <p>Example</p> <p>Create a new file named <code>mySbatchScript.sh</code>, copy the code above into it, save, then submit this file to the job scheduler using the following command :</p> <pre><code>    sbatch mySbatchScript.sh\n</code></pre> <p>Use the command <code>squeue</code> to monitor the jobs submitted to the cluster. </p> <p>Check the output of your job in the output file <code>test_log.o</code>.</p> <p>Note</p> <p>When there are a lot of jobs, <code>squeue -u &lt;username&gt;</code> will limit the list to those of the specified user.</p>"},{"location":"days/server_login/#advanced-cluster-usage-loading-modules","title":"Advanced cluster usage : loading modules","text":"<p>During our various analysis, we will call upon numerous software.</p> <p>Fortunately, in most cases we do not have to install each of these ourselves onto the cluster : they have already been packaged, prepared and made available to you or your code. </p> <p>However, by default, these are not loaded, and you have to explicitly load the module containing the software you want in your script (or in the interactive shell session).</p> <p>Question: Why aren\u2019t all the modules already pre-loaded ?</p> Answer <p>Many toolsets have dependencies toward different, sometimes incompatible libraries. Packaging each tool independently and loading them separately circumvents this as you only load what you need, and you can always unload a toolset if you need to load another, incompatible, toolset.</p> <p>Modules are managed with the <code>module</code> command.</p> <p>Basic commands are :</p> <ul> <li><code>module list</code> : lists currently loaded modules</li> <li><code>module load &lt;modulename&gt;</code> alias <code>ml &lt;modulename&gt;</code> : loads module <code>&lt;modulename&gt;</code></li> <li><code>module unload &lt;modulename&gt;</code>  : unloads module <code>&lt;modulename&gt;</code></li> <li><code>module purge</code> : unloads all loaded modules</li> <li><code>module avail</code> : lists all modules available for loading</li> <li><code>module keyword &lt;KW&gt;</code> : lists all modules available for loading which contains <code>&lt;KW&gt;</code></li> </ul> <p>Try it for yourself: soon, we will need the fastqc software.  If we type in the terminal: <pre><code>fastqc --help\n</code></pre> this should give you an error, telling you there is no such command.</p> <p>We will try to find a module containing our desired software using <code>module keyword</code></p> <pre><code>module keyword fastqc\n</code></pre> <p>which will output: <pre><code>-------------------------------------------------------------------------------------------------------------------------------------\n\nThe following modules match your search criteria: \"fastqc\"\n-------------------------------------------------------------------------------------------------------------------------------------\n\n\n  fastqc: fastqc/0.11.5, fastqc/0.11.8, fastqc/0.11.9 \n    FastQC is a quality control application for high throughput sequence data. It reads in sequence data in a variety of formats and\n    can either provide an interactive application to review the results of several different QC checks, or create an HTML based\n    report which can be integrated into a pipeline. - Homepage: http://www.bioinformatics.babraham.ac.uk/projects/fastqc/\n\n-------------------------------------------------------------------------------------------------------------------------------------\n\nTo learn more about a package execute:\n\n   $ module spider Foo\n\nwhere \"Foo\" is the name of a module.\n\nTo find detailed information about a particular package you\nmust specify the version if there is more than one version:\n\n   $ module spider Foo/11.1\n\n-------------------------------------------------------------------------------------------------------------------------------------\n</code></pre></p> <p>This tells us that a module name <code>fastqc</code> exists. It has different version available (0.11.5,0.11.8,0.11.9).  The default is the rightmost one, which works well for us.</p> <p>So, if we load this module before executing fastqc: <pre><code>ml fastqc # shortcut for \"module load fastqc\"\nfastqc --help\n</code></pre> Now you should not have any error, and you should see the help test of <code>fastqc</code></p> <p>Note</p> <p>our module provider is ComputeCanada, which has a lot of available software. To avoid storing all these on our cluster, each time a new module is loaded, it is fetched first on the Compute Canada servers, so sometimes it can take a bit of time to load a module for the first time.</p>"},{"location":"days/server_login/#advanced-cluster-usage-job-array","title":"Advanced cluster usage : job array","text":"<p>Often, we have to repeat a similar analysis on a number of files, or for a number of different parameters. Rather than writing each sbatch script individually, we can rely on job arrays to facilitate our task.</p> <p>The idea is to have a single script which will execute itself several times. Each of these execution is called a task, and they are all the same for one variable which whose value changes from 1 to the number of tasks in the array.</p> <p>We typically use this variable, named <code>$SLURM_ARRAY_TASK_ID</code> to fetch different lines of a file containing information on the different tasks we want to run (in general, different input file names).</p> <p>Say you want to execute a command, on 10 files (for example, map the reads of 10 samples).</p> <p>You first create a file containing the name of your files (one per line); let\u2019s call it <code>readFiles.txt</code>.</p> <p>Then, you write an sbatch array job script:</p> <pre><code>#!/usr/bin/bash\n#SBATCH --job-name=test_array\n#SBATCH --time=00:30:00\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=1G\n#SBATCH -o test_array_log.%a.o\n#SBATCH --array 1-10%5\n\n\necho \"job array id\" $SLURM_ARRAY_TASK_ID\n\n# sed -n &lt;X&gt;p &lt;file&gt; : retrieve line &lt;X&gt; of file\n# so the next line grabs the file name corresponding to our job array task id and stores it in the variable ReadFileName \nReadFileName=`sed -n ${SLURM_ARRAY_TASK_ID}p readFiles.txt`\n\n# here we would put the mapping command or whatever\necho $ReadFileName\n</code></pre> <p>Some things have changed compared to the previous sbatch script :</p> <ul> <li><code>#SBATCH --array 1-10%5</code> : will spawn independent tasks with ids from 1 to 10, and will manage them so that at most 5 run at the same time.</li> <li><code>#SBATCH -o test_array_log.%a.o</code> : the <code>%a</code> will take the value of the array task id. So we will have 1 log file per task (so 10 files).</li> <li><code>$SLURM_ARRAY_TASK_ID</code> : changes value between the different tasks. This is what we use to execute the same script on different files (using <code>sed -n ${SLURM_ARRAY_TASK_ID}p</code>)</li> </ul> <p>Furthermore, this script uses the concept of bash variable.</p> <p>Many things could be said on that, but I will keep it simple with this little demo code:</p> <pre><code>foo=123                # Initialize variable foo with 123\n#   !warning! it will not work if you put spaces in there\n\n# we can then acces this variable content by putting a $ sign in front of it:\n\necho $foo              # Print variable foo, sensitive to special characters\necho ${foo}            # Another way to print variable foo, not sensitive to special characters\n\nOUTPUT=`wc -l du -h -d 2 /shared/data/` # puts the result of the command \n#   between `` in variable OUTPUT\n\necho $OUTPUT           # print variable output\n</code></pre> <p>So now, this should help you understand the trick we use in the array script:</p> <pre><code>ReadFileName=`sed -n ${SLURM_ARRAY_TASK_ID}p readFiles.txt`\n</code></pre> <p>Where, for example for task 3, <code>${SLURM_ARRAY_TASK_ID}</code> is equal to 3.</p> <p>We feed this to <code>sed</code>, so that it grabs the 3rd line of <code>readFiles.txt</code>, and we put that value into the <code>ReadFileName</code>.</p>"},{"location":"days/trimming/","title":"Sequence trimming","text":"<p>Following a QC analysis on sequencing results, one may detect stretches of low quality bases along reads, or a contamination by adapter sequence. Depending on your research question and the software you use for mapping, you may have to remove these bad quality / spurious sequences out of your data.</p> <p>During this block, you will learn to :</p> <ul> <li>trim your data with trimmomatic</li> </ul>"},{"location":"days/trimming/#material","title":"Material","text":"<p> Download the presentation</p> <p>Trimmomatic website</p>"},{"location":"days/trimming/#to-trim-or-not-to-trim","title":"to trim or not to trim ?","text":"<p>There are several ways to deal with poor quality bases or adapter contamination in reads, and several terms are used in the field, sometimes very loosely. We can talk about:</p> <ul> <li>Trimming: to remove a part of, or the entirety of, a read (for quality reasons).<ul> <li>Hard trimming: trim with a high threshold (eg. remove everything with QUAL&lt;30).</li> <li>Soft trimming: trim with a low threshold (eg. remove everything with QUAL&lt;10).</li> </ul> </li> <li>Clipping: to remove the end part of a read (typically because of adapter content).<ul> <li>Hard clipping: actually removing the end of the read from the file (ie. with trimmomatic).</li> <li>Soft clipping: ignoring the end of the read at mapping time (ie. what STAR does).</li> </ul> </li> </ul> <p>If the data will be used to perform transcriptome assembly, or variant analysis, then it must be trimmed.</p> <p>In contrast, for applications based on counting reads, such as Differential Expression analysis, most aligners, such as STAR, HISAT2, salmon, and kallisto, can handle bad quality sequences and adapter content by soft-clipping, and consequently they usually do not need trimming. In fact, trimming can be detrimental to the number of successfully quantified reads [William et al. 2016].</p> <p>Nevertheless, it is usually recommended to perform some amount of soft clipping (eg. kallisto, salmon ).</p> <p>If possible, we recommend to perform the mapping for both the raw data and the trimmed one, in order to compare the results for both, and choose the best.</p> <p>Question: what could be a good metric to choose the best between the trimmed and untrimmed?</p> Answer <p>The number of uniquely mapped reads is generally what would matter in differential expression analysis. Of course, this means that you can only choose after you have mapped both the trimmed and the untrimmed reads.</p>"},{"location":"days/trimming/#trimming-with-trimmomatic","title":"trimming with Trimmomatic","text":"<p>The trimmomatic website gives very good examples of their software usage for both paired-end (<code>PE</code>) and single-end (<code>SE</code>) reads. We recommend you read their quick-start section attentively.</p> <p>Task 1: </p> <ul> <li> <p>Conduct a soft trimming on the mouseMT data</p> <ul> <li>name the output folder : <code>030_trim/</code>.</li> <li>Adapter sequences can be found in <code>/shared/data/DATA/adapters/</code>.</li> <li>unlike fastqc, you will have to launch trimmomatic for each sample separately</li> <li>to facilitate QC afterward, add the following at the end of your trimmomatic command (substituting <code>&lt;sample name&gt;</code>):             <code>2&gt; 030_trim/trim_out.&lt;sample name&gt;.log</code>             This will same part of the output of trimmomatic to the same folder as the trimmed reads, which multiQC will be able to use afterward.</li> </ul> </li> </ul> <p>Warning</p> <p>trimmomatic is a Java-based program, and thus must be run by passing its .jar file to the Java interpreter:</p> <pre><code>ml  trimmomatic\njava -jar $EBROOTTRIMMOMATIC/trimmomatic-0.39.jar\n</code></pre> trimmomatics sbatch script <p>We chose the following option:</p> <ul> <li>SLIDINGWINDOW:3:25 Perform a sliding window trimming, cutting once the average quality within the window falls below a threshold.</li> <li>3  : windowSize: specifies the number of bases to average across</li> <li>25 : requiredQuality: specifies the average quality required.</li> <li>ILLUMINACLIP:/shared/home/SHARED/DATA/adapters/TruSeq3-PE.fa:2:30:10 Cut adapter and other Illumina-specific sequences from the read.</li> <li>Cut adapter and other illumina-specific sequences from the read.</li> <li>2  : seedMismatches: specifies the maximum mismatch count which will still allow a full match to be performed</li> <li>30 : palindromeClipThreshold: specifies how accurate the match between the two \u2018adapter ligated\u2019 reads must be for PE palindrome read alignment.</li> <li>10 : simpleClipThreshold: specifies how accurate the match between any adapter etc. sequence must be against a read.</li> </ul> <pre><code>#!/usr/bin/bash\n#SBATCH --job-name=trim_mouseMT\n#SBATCH --time=01:00:00\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=1G\n#SBATCH -o trim_mouseMT.o\n\nml  trimmomatic\n\n## creating output folder, in case it does not exists\nmkdir -p 030_trim\n\n## we store the input folder in a variable, \n## to access its value, we will write $INPUT_FOLDER\nINPUT_FOLDER=/shared/data/DATA/mouseMT\n\n## by ending a line with \\ we can continue the same command on the line below\n\njava -jar $EBROOTTRIMMOMATIC/trimmomatic-0.39.jar SE -phred33 \\\n$INPUT_FOLDER/sample_a1.fastq \\\n030_trim/sample_a1.trimmed.fastq \\\nILLUMINACLIP:/shared/data/DATA/adapters/TruSeq3-PE.fa:2:30:10 \\\nSLIDINGWINDOW:3:25 2&gt; 030_trim/trim_out.sample_a1.log\n\njava -jar $EBROOTTRIMMOMATIC/trimmomatic-0.39.jar SE -phred33 \\\n$INPUT_FOLDER/sample_a2.fastq \\\n030_trim/sample_a2.trimmed.fastq \\\nILLUMINACLIP:/shared/data/DATA/adapters/TruSeq3-PE.fa:2:30:10 \\\nSLIDINGWINDOW:3:25 2&gt; 030_trim/trim_out.sample_a2.log\n\njava -jar $EBROOTTRIMMOMATIC/trimmomatic-0.39.jar SE -phred33 \\\n$INPUT_FOLDER/sample_a3.fastq \\\n030_trim/sample_a3.trimmed.fastq \\\nILLUMINACLIP:/shared/data/DATA/adapters/TruSeq3-PE.fa:2:30:10 \\\nSLIDINGWINDOW:3:25 2&gt; 030_trim/trim_out.sample_a3.log\n\njava -jar $EBROOTTRIMMOMATIC/trimmomatic-0.39.jar SE -phred33 \\\n$INPUT_FOLDER/sample_a4.fastq \\\n030_trim/sample_a4.trimmed.fastq \\\nILLUMINACLIP:/shared/data/DATA/adapters/TruSeq3-PE.fa:2:30:10 \\\nSLIDINGWINDOW:3:25 2&gt; 030_trim/trim_out.sample_a4.log\n\n\njava -jar $EBROOTTRIMMOMATIC/trimmomatic-0.39.jar SE -phred33 \\\n$INPUT_FOLDER/sample_b1.fastq \\\n030_trim/sample_b1.trimmed.fastq \\\nILLUMINACLIP:/shared/data/DATA/adapters/TruSeq3-PE.fa:2:30:10 \\\nSLIDINGWINDOW:3:25 2&gt; 030_trim/trim_out.sample_b1.log\n\njava -jar $EBROOTTRIMMOMATIC/trimmomatic-0.39.jar SE -phred33 \\\n$INPUT_FOLDER/sample_b2.fastq \\\n030_trim/sample_b2.trimmed.fastq \\\nILLUMINACLIP:/shared/data/DATA/adapters/TruSeq3-PE.fa:2:30:10 \\\nSLIDINGWINDOW:3:25 2&gt; 030_trim/trim_out.sample_b2.log\n\njava -jar $EBROOTTRIMMOMATIC/trimmomatic-0.39.jar SE -phred33 \\\n$INPUT_FOLDER/sample_b3.fastq \\\n030_trim/sample_b3.trimmed.fastq \\\nILLUMINACLIP:/shared/data/DATA/adapters/TruSeq3-PE.fa:2:30:10 \\\nSLIDINGWINDOW:3:25 2&gt; 030_trim/trim_out.sample_b3.log\n\njava -jar $EBROOTTRIMMOMATIC/trimmomatic-0.39.jar SE -phred33 \\\n$INPUT_FOLDER/sample_b4.fastq \\\n030_trim/sample_b4.trimmed.fastq \\\nILLUMINACLIP:/shared/data/DATA/adapters/TruSeq3-PE.fa:2:30:10 \\\nSLIDINGWINDOW:3:25 2&gt; 030_trim/trim_out.sample_b4.log\n</code></pre> <p>On the cluster, this script is also in <code>/shared/data/Solutions/mouseMT/030_trim.sh</code></p> alternative sbatch using an array job <p>First create a file named <code>sampleNames.txt</code>, containing the sample names:</p> <p><pre><code>sample_a1\nsample_a2\nsample_a3\nsample_a4\nsample_b1\nsample_b2\nsample_b3\nsample_b4\n</code></pre> it can also be found in the cluster at <code>/shared/data/Solutions/mouseMT/sampleNames.txt</code></p> <p><pre><code>#!/usr/bin/bash\n#SBATCH --job-name=trim_mouseMT_array\n#SBATCH --time=01:00:00\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=1G\n#SBATCH -o trim_mouseMT.%a.o\n#SBATCH --array=1-8%8\n\nml  trimmomatic\n\n## creating output folder, in case it does not exists\nmkdir -p 030_trim\n\nINPUT_FOLDER=/shared/data/DATA/mouseMT\n\n## each job grab a specific line from sampleNames.txt\nSAMPLE=`sed -n ${SLURM_ARRAY_TASK_ID}p sampleNames.txt`\n\n\njava -jar $EBROOTTRIMMOMATIC/trimmomatic-0.39.jar SE -phred33 \\\n                                         $INPUT_FOLDER/${SAMPLE}.fastq \\\n                                         030_trim/${SAMPLE}.trimmed.fastq \\\n                                         ILLUMINACLIP:/shared/data/DATA/adapters/TruSeq3-PE.fa:2:30:10 \\\n                                         SLIDINGWINDOW:3:25 2&gt; 030_trim/trim_out.${SAMPLE}.log\n</code></pre> On the cluster, this script is also in  <code>/shared/data/Solutions/mouseMT/030_trim_array.sh</code></p> <p>Task 2: </p> <ul> <li>Use the the following script to run a QC analysis on your trimmmed reads  and compare with the raw ones.</li> </ul> <p><pre><code>#!/usr/bin/bash\n#SBATCH --job-name=map-multiqc\n#SBATCH --time=00:30:00\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=1G\n#SBATCH -o multiqc-map-trim.o\n\n## fastQC on trimmed fastq files\nfastqc 030_trim/*.fastq -o 030_trim\n\n\n## multiqc on the fastQC reports AND the trimmomatic logs\nmultiqc -n 032_multiqc_mouseMT_trimmed.html -f --title trimmed_fastq 030_trim/\n</code></pre> on the cluster, you can find this script in : <code>/shared/data/Solutions/mouseMT/032_multiqc_trimmed.sh</code></p> <p>Note</p> <p>the script above presumes that you have successfully trimmed the reads. </p> <p>If not, you can grab them on the cluster in <code>/shared/data/Solutions/mouseMT/030_trim/</code></p> trimmed reads multiqc report <p> Download the multiqc report</p> <p>Note the second section, Trimmomatic, which lets you know the number/percentage of reads dropped</p>"}]}
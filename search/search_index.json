{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction to RNA-Seq: from quality control to pathway analysis","text":""},{"location":"#learning-outcomes","title":"Learning outcomes","text":""},{"location":"#general-learning-outcomes","title":"General learning outcomes","text":"<p>After this course, you will be able to:</p> <ul> <li>Describe advantages and pitfalls of RNA sequencing.</li> <li>Design your own experiment.</li> <li>Perform the downstream analysis using command line software   (QC, mapping, counting, differential expression analysis, pathway analysis, etc).</li> <li>Critically assess the quality of your results at each step of the downstream analysis.</li> <li>Detect significantly differentially-expressed genes between conditions.</li> </ul>"},{"location":"#learning-experiences","title":"Learning experiences","text":"<p>To reach the learning outcomes, we will use lectures and exercises.  During lectures, please do not hesitate to ask questions as we progress through the slides. During exercises, you are free to discuss things with other participants. </p> <p>Exercises are provided with solutions. How you use them to your advantage is up to you.</p>"},{"location":"cheatsheets/","title":"Cheatsheets","text":"<p> Linux Command Line Cheat Sheet</p> <p>About the \u201cLinux Command Line Cheat Sheet\u201d:</p> <ul> <li>only page1 is absolutely need</li> <li>\u201cShell Input and Output\u201d of page3 is good to know</li> </ul> <p>SLURM cluster Cheat Sheet</p> <p> Keyboards with some special characters highlighted</p>"},{"location":"course_schedule/","title":"Course schedule","text":""},{"location":"course_schedule/#day-1","title":"Day 1","text":"start end subject 9:00 10:30 RNAseq - technologies and design 10:30 10:45 BREAK 10:45 12:00 Server login + unix fresh up 12:00 13:00 LUNCH BREAK 13:00 13:45 Server login + unix fresh up 13:45 15:00 Quality control 15:00 15:15 BREAK 15:15 15:45 Quality control - continued 15:45 17:00 Sequence Trimming and adapter removal"},{"location":"course_schedule/#day-2","title":"Day 2","text":"start end subject 9:00 9:15 Recap of yesterday 09:15 10:30 Reads mapping  10:30 10:45 BREAK 10:45 11:30 Reads mapping - continued 11:30 12:00 Differential Expression Inference  12:00 13:00 LUNCH BREAK 13:30 15:00 Differential Expression Inference - continued  15:00 15:15 BREAK 15:15 17:00 Enrichment analysis"},{"location":"precourse/","title":"Prerequisites","text":"<p>On top of a thirst for knowledge, and a working Internet connection, here is what you will need for the course : </p>"},{"location":"precourse/#ngs","title":"NGS","text":"<p>As announced in the course registration webpage, we expect participants to already have a basic knowledge in Next Generation Sequencing (NGS) techniques. </p>"},{"location":"precourse/#unix","title":"UNIX","text":"<p>Practical knowledge of the UNIX command line is also required to be able to follow this course, given that that the tools used to process sequenced reads use this interface.</p> <p>If you are unsure about your capabilities or feel a bit rusty, we strongly recommend you spend some time practicing before the course : in our experience, the more comfortable you are with UNIX, the more you will be able to focus on the RNA-seq during the course, and the more you will gain from it.</p> <p>You may refer to the SIB\u2019s UNIX e-learning module, or follow our small UNIX crash course page</p>"},{"location":"precourse/#r","title":"R","text":"<p>A basic knowledge of the R language is required to perform most analytical steps after reads have been mapped and quantified : differential gene expression, gene set enrichment, over-representation/pathway analysis.</p> <p>If you are not familiar with R, you can follow our small R crash course page, but given the depth of the subject, we also recommend you attend a dedicated course, such as the SIB First Steps with R course, or you can pick one from glittr.org</p>"},{"location":"slides_notes/","title":"Slides notes","text":"<p>This document contains notes about the course slides.</p> <p>This is a somewhat internal document, so expect a fairly draft-ish and concise style.</p>"},{"location":"slides_notes/#01-overview","title":"01 Overview","text":"<p>slides 3 - 7</p> <ul> <li>the central dogma of molecular biology is known to be not so simple</li> <li>RNA is not only a messenger but may have other roles</li> <li>most of these elements interacts and regulates one-another </li> <li>alternative splicing in eukaryots adds a layer of possibilities to all this</li> <li>main takeaway maybe : measuring RNA is a proxy for protein levels, which is a proxy for protein activity , which is a proxy for the physiological state of the cell</li> </ul> <p>slides 8 - 9</p> <ul> <li>non-exhaustive list of sequencing possibilities : list on Lior Pachter\u2019s blog</li> </ul> <p>slides 10-12</p> <ul> <li>RNAseq, the challenges</li> <li>slide 10 : from human gff, includes ncRNAs</li> <li> <p>slide 11 : </p> <ul> <li>data source: gTex (V8 gene TPMs)</li> <li>left: 1 sample -&gt; from 1 to $10^5$ TPM </li> <li>right: 50 random samples -&gt; 10% of genes contribute 90% of the transcripts</li> <li>NB: mammalian cell 10-30pg RNA/cell , around 360 000 mRNA molecules (source )</li> </ul> </li> <li> <p>slide 12 : important considerations as well</p> </li> </ul> <p>slide 13</p> <ul> <li>Illumina : market leader 50-600bp (generally 50-100), 0.1 (nextseq) to 3 (Hiseq) billion reads</li> <li>Ion torrent : 600bp, 260M reads</li> <li>Pacbio : 10-30kb N50 , 4M CCS reads </li> <li>nanopore : theory single molecule, practice: variable (N50 &gt;100kb on ultra-long kit, up to 4.2Mb )</li> </ul> <p>slides 14-22 : describe different technologies</p> <p>Ion Torrent :</p> <ul> <li>cell sequentially flooded with A T G C </li> </ul> <p>PacBio SMRT</p> <ul> <li>DNApol at bottom of Zero-Mode-Waveguide </li> <li>fluorescent dye on dNTPs</li> </ul> <p>Illumina seq :</p> <ul> <li>formation of clusters with the same sequence </li> <li>SBS : labelled nucleotides have reversible terminators, so only 1 base is incorporated at a time.</li> <li>slide 23: paired end sequencing</li> <li>slide 24: stranded sequencing</li> <li>slide 25-26: RIN , RNA purification</li> <li>slide 27-34: sequencing depth and replicates</li> <li>slide 33-34: this pattern applies to low- mid- and high- expressors (see their supp doc)</li> <li>slide 35-42: schematic analysis</li> <li>slide 35 : before the sequencing</li> <li>slide 36 : basic analysis</li> <li>slide 37 : basic analysis with trimmed reads</li> <li>slide 38 : main QC steps</li> <li>slide 39 : QC steps provide feedback on previous steps</li> <li>slide 40 : analysis for variant calling / isoform descriptions / \u2026</li> <li>slide 41 : when no reference genome: de novo assembly</li> <li>slide 42 : the analysis which we\u2019ll do during this course</li> </ul>"},{"location":"slides_notes/#02-quality-control","title":"02 Quality control","text":"<p>slide 04 : illumina doc</p> <ul> <li>control bit: 0 when none of the control bits are on, otherwise it is an even number. On HiSeq X and NextSeq systems, control specification is not performed and this number is always 0.</li> </ul> <p>slide 16-\u2026 : interpretation of fastQC report</p> <ul> <li>Some additionnal help for specific problems:<ul> <li>https://sequencing.qcfail.com/articles/position-specific-failures-of-flowcells/</li> <li>https://sequencing.qcfail.com/articles/positional-sequence-bias-in-random-primed-libraries/</li> <li>https://sequencing.qcfail.com/articles/read-through-adapters-can-appear-at-the-ends-of-sequencing-reads/</li> <li>https://sequencing.qcfail.com/articles/sudden-loss-of-base-call-quality/</li> <li></li> </ul> </li> </ul>"},{"location":"slides_notes/#03-trimming","title":"03 trimming","text":"<ul> <li>slide 02-05 : enumerating reasons we may want to trim</li> <li>slide 04 : adapter can be present if, for instance, insert size is shorter than sequenced length. cf. this post</li> <li>slide 06-11 : different cases where we may trim</li> </ul>"},{"location":"slides_notes/#04-mapping","title":"04 mapping","text":"<p>slides 4-8:</p> <ul> <li>bowtie2</li> <li>STAR</li> <li>tophat2</li> <li>RSEM</li> <li>cufflinks</li> <li>salmon</li> <li>kallisto</li> <li>tximport</li> <li>featurecount</li> <li>stringtie</li> <li>GATK variant calling pipeline</li> </ul>"},{"location":"slides_notes/#05-de","title":"05 DE","text":"<ul> <li>slide 3-9 : challenges for RNAseq </li> <li>slide 4: sequencing depth varies accross libraries<ul> <li>left : 100 samples from the Gtex V8 dataset</li> <li>right : samples from a random binomial with and without a library size factor applied</li> </ul> </li> <li>slide 5: most of the expression is taken by very few genes + a lot of genes have 0 reads  (plots: data from 100 samples from the Gtex V8 dataset )</li> <li>slide 6: small number of samples. 10k simulations of negative binomial draws</li> <li>slide 7-9 : xkcd.com/882</li> <li>slide 10-11 : input</li> <li>slide 12-14 : very good blog post on RPKM and TPM</li> <li>slide 15: filtering: <ul> <li>image source max and mean refer to CPM thresholds ; CPM 1: genes with a CPM less than one in more than half the samples are filtered</li> <li>DESeq2 filtering</li> <li>egdeR filtering: section 2.7 of edgeR doc</li> </ul> </li> </ul> <p>slide 16 : source</p> <pre><code>* TC: Total count (CPM) - UQ: Upper Quartile - Med: median - Q: quantile\n* top left: coef of variation in housekeeping genes in H. sapiens data\n* top right: average false-positive rate over 10 independent datasets simulated with varying proportions of differentially expressed genes (from 0% to 30% for each normalization method). \n* bottom:\n    * distribution: distribution inter samples look the same\n    * Intra-variance: intra group variance \n    * Housekeeping : coef of variation in 30 housekeeping genes, which are presumed to be similarly expressed across conditions\n    * clustering: similarity of DE genes with other methods\n    * false positive rate : see above\n</code></pre> <p>slide 17: normalization</p> <p>From this biostar post</p> <p>EdgeR: Trimmed Mean of M-values (TMM):</p> <p>Based on the hypothesis that most genes are not DE. </p> <p>The TMM factor is computed for each lane, with one lane being considered as a reference sample and the others as test samples.  For each test sample, TMM is computed as the weighted mean of log ratios between this test and the reference, after exclusion of the most expressed genes and the genes with the largest log ratios. </p> <p>According to the hypothesis of low DE, this TMM should be close to 1. If it is not, its value provides an estimate of the correction factor that must be applied to the library sizes (and not the raw counts) in order to fulfill the hypothesis.  [source: https://www.ncbi.nlm.nih.gov/pubmed/22988256]</p> <p>DESeq2</p> <p>DESeq:  is based on the hypothesis that most genes are not DE. </p> <pre><code>   the median of the ratio, for each gene, of its read count over its geometric mean across all lanes.\n\n   The underlying idea is that non-DE genes should have similar read counts across samples, leading to a ratio of 1.\n\n   Assuming most genes are not DE, the median of this ratio for the lane provides an estimate of the correction factor that should be applied to all read counts of this lane to fulfill the hypothesis.\n\n       [source: https://www.ncbi.nlm.nih.gov/pubmed/22988256]\n</code></pre> <p>slide 19: NB model</p> <pre><code>* [image source](https://doi.org/10.1186/gb-2010-11-10-r106)\n* orange line is the fit w(q)\n* purple line show the variance implied by the Poisson distribution \n* dashed orange line is the variance estimate used by edgeR.\n</code></pre>"},{"location":"slides_notes/#06-enrichment","title":"06 Enrichment","text":"<ul> <li>slide 08: geneontology</li> <li>slide 09: reactome</li> <li>slide 10: GSEA-msigdb</li> <li> <p>slide 11: KEGG</p> </li> <li> <p>slides 15 to 21 : GSEA </p> </li> </ul> <p>GSEA is used a lot, but it comes in many flavour, with a large number of options and it is not always easy to understand what is happening.</p> <p>\u201cThe enrichment score (ES) represents the degree to which a set S is over-represented at the top or bottom of the ranked list L. The score is calculated by walking down the list L, increasing a running-sum statistic when we encounter a gene in S and decreasing when it is not encountered. The magnitude of the increment depends on the gene statistics (e.g., correlation of the gene with phenotype). The ES is the maximum deviation from zero encountered in the random walk; it corresponds to a weighted Kolmogorov-Smirnov(KS)-like statistic (Subramanian et al. 2005).\u201d (from clusterProfiler book)</p> <p>Then the ES is normalized (NES) in order to compute a p-value.</p> <ul> <li> <p>in the base method (Subramanian 2005) for each gene sets they create a number of permutated dataset for which they compute an ES, and they then compare the ES on the original data to the distribution of permutated ES for that set. 2 flavours of permutation are described : \u201csample permutation\u201d and \u201cgene permutation\u201d. </p> <ul> <li>The sample permutation is only possible if the expression data for each sample is given to the method. It is recommended to have at least 7 samples per condition for it to make sense. </li> <li>The gene permutation is performed from the ranking metric directly. Hence it is sometimes called \u201cpreranked-GSEA\u201d and to my knowledge this is the most often used permutation scheme of the 2.</li> </ul> </li> <li> <p>in fGSEA (Korotkevich et al. 2021) several tricks are used to make the p-value computation of \u201cpreranked-GSEA\u201d faster and more accurate for low p-values. Consequently it has become the default GSEA method in some libraries such as clusterProfiler.</p> </li> </ul> <p>Finally, a parameter to consider when performing (preranked-)(f)GSEA is which metric to use to rank genes.  The most commons are logFC , -log10(pvalues) * logFC , or some signed test statistics (eg. t-test or Wald statistic).</p>"},{"location":"days/DE/","title":"Differential Expression Inference","text":"<p>Once the reads have been mapped and counted, one can assess the differential expression of genes between different conditions.</p> <p>During this lesson, you will learn to :</p> <ul> <li>describe the different steps of data normalization and modelling commonly used for RNA-seq data.</li> <li>detect significantly differentially-expressed genes using either edgeR or DESeq2.</li> </ul>"},{"location":"days/DE/#material","title":"Material","text":"<p> Download the presentation</p> <p>Rstudio website</p> <p>edgeR user\u2019s guide</p> <p>DESeq2 vignette</p>"},{"location":"days/DE/#differential-expression-inference","title":"Differential Expression Inference","text":"<p>Let\u2019s analyze the <code>mouseMT</code> toy dataset together.</p> <p>First, here is the code to load the reads counts into R as a count matrix:</p> <pre><code>folder  = \"/data/Solutions/mouseMT/042_d_STAR_map_raw/\"\n\n# we skip the 4 first lines, which contains \n# N_unmapped , N_multimapping , N_noFeature , N_ambiguous   \n\nsample_a1_table = read.table(paste0( folder , \"sample_a1\" , \".ReadsPerGene.out.tab\") , row.names = 1 , skip = 4 )\nhead( sample_a1_table )\n</code></pre> <pre><code>                    V2      V3      V4\nENSMUSG00000064336  0       0       0   \nENSMUSG00000064337  0       0       0   \nENSMUSG00000064338  0       0       0   \nENSMUSG00000064339  0       0       0   \nENSMUSG00000064340  0       0       0   \nENSMUSG00000064341  4046    1991    2055    \n</code></pre> <p>We are interested in the first columns, which contains counts for unstranded reads</p> <p>Let\u2019s use a loop to automatize the reading: <pre><code>raw_counts = data.frame( row.names =  row.names(sample_a1_table) )\n\nfor( sample in c('a1','a2','a3','a4','b1','b2','b3','b4') ){\nsample_table = read.table(paste0( folder , \"sample_\" , sample , \".ReadsPerGene.out.tab\") , row.names = 1 , skip = 4 )\n\nraw_counts[sample] = sample_table[ row.names(raw_counts) , \"V2\" ]\n\n}\n\nhead( raw_counts )\n</code></pre> <pre><code>                    a1      a2      a3      a4  b1  b2  b3  b4\nENSMUSG00000064336  0       0       0       0   0   0   0   0\nENSMUSG00000064337  0       0       0       0   0   0   0   0\nENSMUSG00000064338  0       0       0       0   0   0   0   0\nENSMUSG00000064339  0       0       0       2   0   0   0   0\nENSMUSG00000064340  0       0       0       0   0   0   0   0\nENSMUSG00000064341  4046    4098    4031    1   449 515 13  456\n</code></pre></p> DESeq2 analysis <pre><code>library(DESeq2)\nlibrary(ggplot2)\nlibrary(pheatmap)\n</code></pre> edgeR analysis <pre><code># setup\nlibrary(edgeR)\nlibrary(ggplot2)\n</code></pre> <p>From ensembl gene ids to gene names</p> <p>We can convert between different gene ids using the <code>bitr</code> function from <code>clusterProfiler</code> <pre><code>library(clusterProfiler)\nlibrary(org.Mm.eg.db)\n\ngenes_universe &lt;- bitr(rownames(allGenes), fromType = \"ENSEMBL\",\ntoType = c(\"ENTREZID\", \"SYMBOL\"),\nOrgDb = \"org.Mm.eg.db\")\ngenes_universe\n</code></pre> <pre><code>             ENSEMBL ENTREZID SYMBOL\n1 ENSMUSG00000064341    17716    ND1\n2 ENSMUSG00000064354    17709   COX2\n3 ENSMUSG00000064345    17717    ND2\n4 ENSMUSG00000064368    17722    ND6\n5 ENSMUSG00000064351    17708   COX1\n6 ENSMUSG00000064358    17710   COX3\n7 ENSMUSG00000065947    17720   ND4L\n8 ENSMUSG00000064363    17719    ND4\n9 ENSMUSG00000064357    17705   ATP6\n</code></pre></p> <p>Here is the list of orgDb packages. For non-model organisms it will be more complex.</p>"},{"location":"days/DE/#setting-up-the-experimental-design","title":"setting up the experimental design","text":"<pre><code>#note: levels let's us define the reference levels\ntreatment &lt;- factor( c(rep(\"a\",4), rep(\"b\",4)), levels=c(\"a\", \"b\") )\ncolData &lt;- data.frame(treatment, row.names = colnames(raw_counts))\ncolData\n</code></pre> <pre><code>    treatment\na1  a           \na2  a           \na3  a           \na4  a           \nb1  b           \nb2  b           \nb3  b           \nb4  b\n</code></pre>"},{"location":"days/DE/#creating-the-deseq-data-object-and-some-qc","title":"creating the DESeq data object and some QC","text":"<p><pre><code>dds &lt;- DESeqDataSetFromMatrix(\ncountData = raw_counts, colData = colData, design = ~ treatment)\ndim(dds)\n</code></pre> <pre><code>[1] 37  8\n</code></pre></p> <p>Filter low count genes. </p> <p>Here, we will apply a very soft filter and keep genes with at least 1 read in at least 4 samples (size of the smallest group).</p> <p><pre><code>idx &lt;- rowSums(counts(dds, normalized=FALSE) &gt;= 1) &gt;= 4\ndds.f &lt;- dds[idx, ]\ndim(dds.f)\n</code></pre> <pre><code>[1] 13  8\n</code></pre></p> <p>We go from 37 to 13 genes</p> <p>We perform the estimation of dispersions  <pre><code>dds.f &lt;- DESeq(dds.f)\n</code></pre> <pre><code>estimating size factors\nestimating dispersions\ngene-wise dispersion estimates\nmean-dispersion relationship\n-- note: fitType='parametric', but the dispersion trend was not well captured by the\n   function: y = a/x + b, and a local regression fit was automatically substituted.\n   specify fitType='local' or 'mean' to avoid this message next time.\nfinal dispersion estimates\nfitting model and testing\n</code></pre></p> <p>PCA plot of the samples: <pre><code># blind : whether to blind the transformation to the experimental design. \n#   - blind=TRUE : comparing samples in a manner unbiased by prior information on samples, \n#                  for example to perform sample QA (quality assurance).\n#   - blind=FALSE: should be used for transforming data for downstream analysis, \n#                  where the full use of the design information should be made.\n\nvsd &lt;- varianceStabilizingTransformation(dds.f, blind=TRUE )\npcaData &lt;- plotPCA(vsd, intgroup=c(\"treatment\"))\npcaData + geom_label(aes(x=PC1,y=PC2,label=name))\n</code></pre></p> <p></p> <p>OK, so a4 and b3 are quite different from the rest.</p> <ul> <li>a4 was expected from the QC</li> <li>b3 we did not expect until now</li> </ul> <p>If we did the analysis with them, here is what we get: <pre><code>res &lt;- results(dds.f)\nsummary( res )\n</code></pre> <pre><code>out of 13 with nonzero total read count\nadjusted p-value &lt; 0.1\nLFC &gt; 0 (up)       : 0, 0%\nLFC &lt; 0 (down)     : 0, 0%\noutliers [1]       : 7, 54%\nlow counts [2]     : 0, 0%\n(mean count &lt; 16)\n[1] see 'cooksCutoff' argument of ?results\n[2] see 'independentFiltering' argument of ?results\n</code></pre></p> <p>So, let\u2019s eliminate these two samples.</p>"},{"location":"days/DE/#analysis-without-the-outliers","title":"analysis without the outliers","text":"<pre><code>raw_counts_no_outliers = raw_counts[ , !( colnames(raw_counts) %in% c('a4','b3') ) ]\n\ntreatment &lt;- factor( c(rep(\"a\",3), rep(\"b\",3)), levels=c(\"a\", \"b\") )\ncolData &lt;- data.frame(treatment, row.names = colnames(raw_counts_no_outliers))\ncolData </code></pre> <pre><code>    treatment\na1  a           \na2  a           \na3  a           \nb1  b           \nb2  b           \nb4  b\n</code></pre> <p><pre><code>dds &lt;- DESeqDataSetFromMatrix(\ncountData = raw_counts_no_outliers, colData = colData, design = ~ treatment)\ndim(dds)\n</code></pre> <pre><code>[1] 37  6\n</code></pre></p> <p>Filter low count genes: now the smallest group is 3 <pre><code>idx &lt;- rowSums(counts(dds, normalized=FALSE) &gt;= 1) &gt;= 3\ndds.f &lt;- dds[idx, ]\ndim(dds.f)\n</code></pre> <pre><code>[1] 12  6\n</code></pre> 12 genes remaining</p> <p>We perform the estimation of dispersions  <pre><code>dds.f &lt;- DESeq(dds.f)\n</code></pre> <pre><code>estimating size factors\nestimating dispersions\ngene-wise dispersion estimates\nmean-dispersion relationship\nfinal dispersion estimates\nfitting model and testing\n</code></pre></p> <p>PCA plot of the samples: <pre><code>vsd &lt;- varianceStabilizingTransformation(dds.f, blind=TRUE )\npcaData &lt;- plotPCA(vsd, intgroup=c(\"treatment\"))\npcaData + geom_label(aes(x=PC1,y=PC2,label=name))\n</code></pre> </p> <p>It looks much better. Seems like PC1 captures the group effect</p> <p>We plot the estimate of the dispersions <pre><code># * black dot : raw\n# * red dot : local trend\n# * blue : corrected\nplotDispEsts(dds.f)\n</code></pre></p> <p></p> <p>There is so few genes that this does not look super nice here</p> <p>For the Ruhland2016 dataset it looks like:</p> <p></p> <p>This plot is not easy to interpret. It represents the amount of dispersion at different levels of expression. It is directly linked to our ability to detect differential expression.</p> <p>Here it looks about normal compared to typical bulk RNA-seq experiments : the dispersion is comparatively larger for lowly-expressed genes.</p> <p><pre><code># extracting results for the treatment versus control contrast\nres &lt;- results(dds.f)\nsummary( res )\n</code></pre> <pre><code>out of 12 with nonzero total read count\nadjusted p-value &lt; 0.1\nLFC &gt; 0 (up)       : 1, 8.3%\nLFC &lt; 0 (down)     : 2, 17%\noutliers [1]       : 0, 0%\nlow counts [2]     : 0, 0%\n(mean count &lt; 1)\n[1] see 'cooksCutoff' argument of ?results\n[2] see 'independentFiltering' argument of ?results\n</code></pre></p> <p>We can have a look at the coefficients of this model <pre><code>head(coef(dds.f)) # the second column corresponds to the difference between the 2 conditions\n</code></pre> <pre><code>                   Intercept treatment_b_vs_a\nENSMUSG00000064341 12.084282      -3.33601412\nENSMUSG00000064345  6.112479      -0.99101369\nENSMUSG00000064351  3.757967       0.64546475\nENSMUSG00000064354 10.209339       1.44160442\nENSMUSG00000064357 11.763707      -0.06245774\nENSMUSG00000064358  6.026334      -0.26447858\n</code></pre></p> <p>Here, it contains an intercept and a coefficient for the difference between the two groups.</p> <p>MA plot: <pre><code>res.lfc &lt;- lfcShrink(dds.f, coef=2, res=res)\nDESeq2::plotMA(res.lfc)\n</code></pre> </p> <p>Volcano plot: <pre><code>FDRthreshold = 0.01\nlogFCthreshold = 1.0\n# add a column of NAs\nres.lfc$diffexpressed &lt;- \"NO\"\n# if log2Foldchange &gt; 1 and pvalue &lt; 0.01, set as \"UP\" \nres.lfc$diffexpressed[res.lfc$log2FoldChange &gt; logFCthreshold &amp; res.lfc$padj &lt; FDRthreshold] &lt;- \"UP\"\n# if log2Foldchange &lt; 1 and pvalue &lt; 0.01, set as \"DOWN\"\nres.lfc$diffexpressed[res.lfc$log2FoldChange &lt; -logFCthreshold &amp; res.lfc$padj &lt; FDRthreshold] &lt;- \"DOWN\"\n\nggplot( data = data.frame( res.lfc ) , aes( x=log2FoldChange , y = -log10(padj) , col =diffexpressed ) ) + geom_point() + geom_vline(xintercept=c(-logFCthreshold, logFCthreshold), col=\"red\") +\ngeom_hline(yintercept=-log10(FDRthreshold), col=\"red\") +\nscale_color_manual(values=c(\"blue\", \"grey\", \"red\"))\n\ntable(res.lfc$diffexpressed)\n</code></pre> <pre><code>DOWN   NO   UP \n   1   10    1 \n</code></pre> </p> <p>Heatmap: <pre><code>vsd.counts &lt;- assay(vsd)\n\ntopVarGenes &lt;- head(order(rowVars(vsd.counts), decreasing = TRUE), 20)\nmat  &lt;- vsd.counts[ topVarGenes, ] #scaled counts of the top genes\nmat  &lt;- mat - rowMeans(mat)  # centering\npheatmap(mat)\n</code></pre> </p>"},{"location":"days/DE/#saving-results-to-file","title":"saving results to file","text":"<p>note: a CSV file can be imported into Excel <pre><code>write.csv( res ,'051_r_mouseMT.DESeq2.results.csv' )\n</code></pre></p>"},{"location":"days/DE/#experimental-design","title":"experimental design","text":"<p>note: levels lets us define the reference levels</p> <p><pre><code>treatment &lt;- factor( c(rep(\"a\",4), rep(\"b\",4)), levels=c(\"a\", \"b\") )\nnames(treatment) = colnames(raw_counts)\n\ntreatment\n</code></pre> <pre><code>a1 a2 a3 a4 b1 b2 b3 b4 \n a  a  a  a  b  b  b  b \nLevels: a b\n</code></pre></p>"},{"location":"days/DE/#edger-object-preprocessing-and-qc","title":"edgeR  object preprocessing and QC","text":"<p>Creating the edgeR DGE object and filtering low-count genes.</p> <p><pre><code>dge.all &lt;- DGEList(counts = raw_counts , group = treatment)  dge.f.design &lt;- model.matrix(~ treatment)\n\n# filtering by expression level. See ?filterByExpr for details\nkeep &lt;- filterByExpr(dge.all)\ndge.f &lt;- dge.all[keep, keep.lib.sizes=FALSE]\ntable( keep )\n</code></pre> <pre><code>keep\nFALSE  TRUE \n   28     9 \n</code></pre></p> <p><pre><code>#normalization\ndge.f &lt;- calcNormFactors(dge.f)\ndge.f$samples\n</code></pre> <pre><code>    group   lib.size    norm.factors\na1  a       13799       1.2101311   \na2  a       13649       1.2130900   \na3  a       13938       1.2131513   \na4  a       6831        0.1058474   \nb1  b       13703       1.3614563   \nb2  b       13627       1.3728761   \nb3  b       162         2.0759281   \nb4  b       13687       1.3671996   \n</code></pre> We represent the distances between the samples using MDS:</p> <p><pre><code>plotMDS( dge.f , col = c('cornflowerblue','forestgreen')[treatment] )\n</code></pre> </p> <p>OK, so a4 and b3 are quite different from the rest.</p> <ul> <li>a4 was expected from the QC</li> <li>b3 we did not expect until now</li> </ul> <p>If we did the analysis with them, here is what we get: <pre><code># estimate of the dispersion\ndge.f &lt;- estimateDisp(dge.f,dge.f.design , robust = T)\n# testing for differential expression. \ndge.f.et &lt;- exactTest(dge.f)\ntopTags(dge.f.et)\n</code></pre> <pre><code>                    logFC       logCPM      PValue      FDR\nENSMUSG00000065947  -5.06512022 13.51727    0.006432974 0.05110489\nENSMUSG00000064351  -7.46603064 20.14951    0.011356643 0.05110489\nENSMUSG00000064345  3.35236454  15.28962    0.050902734 0.13212506\nENSMUSG00000064341  -2.67947447 16.69126    0.058722251 0.13212506\nENSMUSG00000064354  1.58127747  16.60293    0.263189690 0.42743645\nENSMUSG00000064368  1.75395362  12.51557    0.284957631 0.42743645\nENSMUSG00000064358  -0.10554311 11.49478    0.912516094 0.96610797\nENSMUSG00000064363  -0.08216781 17.90743    0.950808801 0.96610797\nENSMUSG00000064357  0.06455233  17.18842    0.966107973 0.96610797\n</code></pre></p> <p>no gene is significantly DE.</p> <p>So, let\u2019s eliminate these two samples.</p>"},{"location":"days/DE/#analysis-without-the-outliers_1","title":"analysis without the outliers","text":"<p><pre><code>raw_counts_no_outliers = raw_counts[ , !( colnames(raw_counts) %in% c('a4','b3') ) ]\nhead( raw_counts_no_outliers )\n</code></pre> <pre><code>                    a1      a2      a3      b1  b2  b4\nENSMUSG00000064336  0       0       0       0   0   0\nENSMUSG00000064337  0       0       0       0   0   0\nENSMUSG00000064338  0       0       0       0   0   0\nENSMUSG00000064339  0       0       0       0   0   0\nENSMUSG00000064340  0       0       0       0   0   0\nENSMUSG00000064341  4046    4098    4031    449 515 456\n</code></pre></p> <p><pre><code>treatment &lt;- factor( c(rep(\"a\",3), rep(\"b\",3)), levels=c(\"a\", \"b\") )\ncolData &lt;- data.frame(treatment, row.names = colnames(raw_counts_no_outliers))\ncolData </code></pre> <pre><code>    treatment\na1  a           \na2  a           \na3  a           \nb1  b           \nb2  b           \nb4  b\n</code></pre></p> <p><pre><code>dge.all &lt;- DGEList(counts = raw_counts_no_outliers , group = treatment)  dge.f.design &lt;- model.matrix(~ treatment)\n\n# filtering by expression level. See ?filterByExpr for details\nkeep &lt;- filterByExpr(dge.all)\ndge.f &lt;- dge.all[keep, keep.lib.sizes=FALSE]\ntable( keep )\n</code></pre> <pre><code>keep\nFALSE  TRUE \n   28     9 \n</code></pre></p> <p>We compute the normalization factor for each library: <pre><code>#normalization\ndge.f &lt;- calcNormFactors(dge.f)\ndge.f$samples\n</code></pre> <pre><code>    group   lib.size    norm.factors\na1  a       13799       0.9437444   \na2  a       13649       0.9277668   \na3  a       13938       0.9412032   \nb1  b       13703       1.0672149   \nb2  b       13627       1.0651230   \nb4  b       13687       1.0675095   \n</code></pre></p> <p>We represent the distances between the samples using MDS:</p> <p><pre><code>plotMDS( dge.f , col = c('cornflowerblue','forestgreen')[treatment] )\n</code></pre> </p> <p>It looks much better. Seems like PC1 captures the group effect.</p> <p>We now fit the model:</p> <p><pre><code># estimate of the dispersion\ndge.f &lt;- estimateDisp(dge.f,dge.f.design , robust = T)\nplotBCV(dge.f)\n</code></pre> </p> <p>There are so few genes that this does not look super nice here.</p> <p>Here is how it looks like on the Ruhland2016 data:</p> <p></p> <p>This plot is not easy to interpret. It represents the amount of biological variation at different levels of expression. It is directly linked to our ability to detect differential expression.</p> <p>Here it looks about normal compared to other bulk RNA-seq experiments : the variation is comparatively larger for lowly expressed genes.</p> <p><pre><code># testing for differential expression. \n# This method is recommended when you only have 2 groups to compare\ndge.f.et &lt;- exactTest(dge.f)\ntopTags(dge.f.et) # printing the genes where the p-value of differential expression if the lowest\n</code></pre> <pre><code>                    logFC           logCPM      PValue          FDR\nENSMUSG00000064341  -3.2728209590   17.34360    0.000000e+00    0.000000e+00\nENSMUSG00000064354  1.5046106466    17.35950    0.000000e+00    0.000000e+00\nENSMUSG00000064345  -0.9251244495   11.92467    4.542291e-08    1.362687e-07\nENSMUSG00000064368  0.7262191019    10.55226    1.337410e-02    3.009174e-02\nENSMUSG00000064351  0.7031516899    10.50496    2.006905e-02    3.612429e-02\nENSMUSG00000064358  -0.1964770294   12.14888    2.110506e-01    3.165759e-01\nENSMUSG00000065947  0.2623477474    10.00748    4.831813e-01    5.851717e-01\nENSMUSG00000064363  -0.0127783014   18.61543    5.201527e-01    5.851717e-01\nENSMUSG00000064357  0.0006819028    17.93963    9.833826e-01    9.833826e-01\n</code></pre></p> <p>We can see 3 genes with FDR &lt; 0.01 and 2 others with 0.01 &lt; FDR &lt; 0.05.</p> <p><pre><code>summary(decideTests(dge.f.et , p.value = 0.01)) # let's use 0.01 as a threshold\n</code></pre> <pre><code>        b-a\nDown     2\nNotSig   6\nUp       1\n</code></pre></p> <p>Let\u2019s plot these: <pre><code>## plot all the logFCs versus average count size. Significantly DE genes are  colored\nplotMD(dge.f.et)\n# lines at a log2FC of 1/-1, corresponding to a shift in expression of x2 \nabline(h=c(-1,1), col=\"blue\")\nabline(h=c(0), col=\"grey\")\n</code></pre> </p> <p>Volcano plot</p> <p><pre><code>allGenes = topTags(dge.f.et , n = nrow(dge.f.et$table) )$table\n\nFDRthreshold = 0.01\nlogFCthreshold = 1.0\n# add a column of NAs\nallGenes$diffexpressed &lt;- \"NO\"\n# if log2Foldchange &gt; 1 and pvalue &lt; 0.01, set as \"UP\" \nallGenes$diffexpressed[allGenes$logFC &gt; logFCthreshold &amp; allGenes$FDR &lt; FDRthreshold] &lt;- \"UP\"\n# if log2Foldchange &lt; 1 and pvalue &lt; 0.01, set as \"DOWN\"\nallGenes$diffexpressed[allGenes$logFC &lt; -logFCthreshold &amp; allGenes$FDR &lt; FDRthreshold] &lt;- \"DOWN\"\n\nggplot( data = allGenes , aes( x=logFC , y = -log10(FDR) , col =diffexpressed ) ) + geom_point() + geom_vline(xintercept=c(-logFCthreshold, logFCthreshold), col=\"red\") +\ngeom_hline(yintercept=-log10(FDRthreshold), col=\"red\") +\nscale_color_manual(values=c(\"blue\", \"grey\", \"red\"))\n</code></pre> </p>"},{"location":"days/DE/#writing-the-table-of-results","title":"writing the table of results","text":"<pre><code>write.csv( allGenes , '052_r_mouseMT.edgeR.results.csv')\n</code></pre>"},{"location":"days/DE/#differential-expression-task","title":"Differential Expression - Task","text":"<p>Use either edgeR or DESeq2 to conduct a differential expression analysis.</p> <p>You may play with either of the following datasets:</p> <ul> <li>Ruhland2016<ul> <li>simple 1 factor design</li> <li><code>/data/Solutions/Ruhland2016/countFiles/featureCounts_Ruhland2016.counts.txt</code></li> <li>  Ruhland2016 count matrix</li> </ul> </li> <li>the Liu2015 dataset:<ul> <li>simple 1 factor design</li> <li><code>/data/Solutions/Liu2015/countFiles/featureCounts_Liu2015.counts.txt</code></li> <li>  Liu2015 count matrix</li> </ul> </li> <li> <p>Tuch 2010 dataset</p> <ul> <li>2 factors design : 3 patients (8, 33, and 51) each had 1 sample from tumor tissue (T) and normal tissue (N) sequenced. </li> <li>the goal is to find the difference between tumor and normal while taking the patient into account.</li> <li><code>/data/Solutions/Tuch2010/Tuch_et_al_2010_counts.csv</code></li> <li>  Tuch 2010 count matrix</li> </ul> </li> <li> <p>Mansingh 2024 dataset</p> <ul> <li>A complex design with 36 mice from two genotypes (KO,WT) and collected at 6 time points (T0,T4,T8,T12,T16,T20), with 3 technical replicate per mouse (108 samples in total)</li> <li>The goal is to investigate the effect of the genotype on the circadian cycle (represented with the different time points)</li> <li><code>/data/Solutions/Mansingh2024/Mansingh2024_expression_matrix.txt</code></li> <li>  Mansingh 2024 count matrix</li> <li>The mice ids should be understood as follow : <code>HL3YTBGX5_4_3__7_CTRL_ZT4</code> means:<ul> <li>replicate <code>HL3YTBGX5</code></li> <li>mouse <code>7</code></li> <li>genotype WT (<code>CTRL</code>)</li> <li>time point 4 (<code>ZT4</code>)</li> </ul> </li> </ul> </li> </ul> <p>Note</p> <ul> <li>Generally, users find the syntax and workflow of DESeq2 easier for getting started.</li> <li>If you have the time, conduct a differential expression analysis using both DESeq2 and edgeR.</li> <li> <p>Follow the vignettes/user\u2019s guide! They are the most up-to-date documents, and generally contain everything a newcomer might need, including worked-out examples.</p> </li> <li> <p>when dealing with more than one factor, you will need a model matrix to specify the experimental design to the library, and to craft your contrasts of interest. The ExploreModelMatrix package may help you a lot in that regard.</p> </li> </ul>"},{"location":"days/DE/#ruhland2016-deseq2-correction","title":"Ruhland2016 - DESeq2 correction","text":"<p>DESeq2 vignette</p> read in the data <pre><code># setup\nlibrary(DESeq2)\nlibrary(ggplot2)\n\n\n# reading the counts files - adapt the file path to your situation\nraw_counts &lt;-read.table('/data/Solutions/Ruhland2016/countFiles/featureCounts_Ruhland2016.counts.txt' , skip=1 , sep=\"\\t\" , header=T)\n\n# setting up row names as ensembl gene ids\nrow.names(raw_counts) = raw_counts$Geneid\n\n## looking at the beginning of that table\nraw_counts[1:5,1:5]\n\n# removing these first columns to keep only the sample counts\nraw_counts = raw_counts[ ,  -1:-6  ] # changing column names\nnames( raw_counts) = gsub('_.*', '', gsub('.*.SRR[0-9]{7}_', '', names(raw_counts) ) )\n\n# some checking of what we just read\nhead(raw_counts); tail(raw_counts); dim(raw_counts)\ncolSums(raw_counts) # total number of counted reads per sample\n</code></pre> <p>output:</p> <pre><code>                       EtOH1.counts EtOH2.counts EtOH3.counts TAM1.counts TAM2.counts TAM3.counts\nENSMUSG00000000001         6726         5150         5362        4867        5982        5527\nENSMUSG00000000003            0            0            0           0           0           0\nENSMUSG00000000028           84          162          127         130         260         136\nENSMUSG00000000031          116         4890          153          81         113         239\nENSMUSG00000000037           35           24           41          13          11          21\nENSMUSG00000000049            4            5            2           4           4           5\n                   EtOH1.counts EtOH2.counts EtOH3.counts TAM1.counts TAM2.counts TAM3.counts\nENSMUSG00000107387            0            0            0           0           0           0\nENSMUSG00000107388           20           32           28          16           8          30\nENSMUSG00000107389            0            0            1           0           0           0\nENSMUSG00000107390            2            0            0           3           2           3\nENSMUSG00000107391            0            0            0           0           0           0\nENSMUSG00000107392            0            0            0           0           0           0\n[1] 46078     6\n</code></pre> <p>there are 46\u2018078 genes and 6 samples.</p> preprocessing <p><pre><code>## telling DESeq2 what the experimental design was\n# note: by default, the 1st level is considered to be the reference/control/WT/...\ntreatment &lt;- factor( c(rep(\"EtOH\",3), rep(\"TAM\",3)), levels=c(\"EtOH\", \"TAM\") )\ncolData &lt;- data.frame(treatment, row.names = colnames(raw_counts))\ncolData\n</code></pre> output: <pre><code>             treatment\nEtOH1.counts      EtOH\nEtOH2.counts      EtOH\nEtOH3.counts      EtOH\nTAM1.counts        TAM\nTAM2.counts        TAM\nTAM3.counts        TAM\n</code></pre></p> <pre><code>## creating the DESeq data object &amp; positing the model\ndds &lt;- DESeqDataSetFromMatrix(\ncountData = raw_counts, colData = colData, design = ~ treatment)\ndim(dds)\n\n## filter low count genes. Here, only keep genes with at least 2 samples where there are at least 5 reads.\nidx &lt;- rowSums(counts(dds, normalized=FALSE) &gt;= 5) &gt;= 2\ndds.f &lt;- dds[idx, ]\ndim(dds.f)\n\n# we go from 55414 to 19378 genes\n</code></pre> <p>Around 19k genes pass our minimum expression threshold, quite typical for a bulk Mouse RNA-seq experiment.</p> estimate dispersion / model fitting <pre><code># we perform the estimation of dispersions \ndds.f &lt;- DESeq(dds.f)\n\n# we plot the estimate of the dispersions\n# * black dot : raw\n# * red dot : local trend\n# * blue : corrected\nplotDispEsts(dds.f)\n\n# extracting results for the treatment versus control contrast\nres &lt;- results(dds.f)\n</code></pre> <p></p> <p>This plot is not easy to interpret. It represents the amount of dispersion at different levels of expression. It is directly linked to our ability to detect differential expression.</p> <p>Here it looks about normal compared to typical bulk RNA-seq experiments : the dispersion is comparatively larger for lowly expressed genes.</p> looking at the results <pre><code># adds estimate of the LFC the results table. \n# This shrunk logFC estimate is more robust than the raw value\n\nhead(coef(dds.f)) # the second column corresponds to the difference between the 2 conditions\nres.lfc &lt;- lfcShrink(dds.f, coef=2, res=res)\n\n#plotting to see the difference.  \npar(mfrow=c(2,1))\nDESeq2::plotMA(res)\nDESeq2::plotMA(res.lfc)\n# -&gt; with shrinkage, the significativeness and logFC are more consistent\npar(mfrow=c(1,1))\n</code></pre> <p></p> <p>Without the shrinkage, we can see that for low counts we can see a high log-fold change but non significant (ie. we see a large difference but with variance is also so high that this observation may be due to chance only).</p> <p>The shrinkage corrects this and the relationship between logFC and significance is smoother.</p> <p><pre><code># we apply the variance stabilising transformation to make the read counts comparable across libraries\n# (nb : this is not needed for DESeq DE analysis, but rather for visualisations that compare expression across samples, such as PCA. This replaces normal PCA scaling)\nvst.dds.f &lt;- vst(dds.f, blind = FALSE)\nvst.dds.f.counts &lt;- assay(vst.dds.f)\n\nplotPCA(vst.dds.f, intgroup = c(\"treatment\"))\n</code></pre> </p> <p>The first axis (58% of the variance) seems linked to the grouping of interest.</p> <pre><code>## ggplot2-based volcano plot\nlibrary(ggplot2)\n\nFDRthreshold = 0.01\nlogFCthreshold = 1.0\n# add a column of NAs\nres.lfc$diffexpressed &lt;- \"NO\"\n# if log2Foldchange &gt; 1 and pvalue &lt; 0.01, set as \"UP\" \nres.lfc$diffexpressed[res.lfc$log2FoldChange &gt; logFCthreshold &amp; res.lfc$padj &lt; FDRthreshold] &lt;- \"UP\"\n# if log2Foldchange &lt; 1 and pvalue &lt; 0.01, set as \"DOWN\"\nres.lfc$diffexpressed[res.lfc$log2FoldChange &lt; -logFCthreshold &amp; res.lfc$padj &lt; FDRthreshold] &lt;- \"DOWN\"\n\nggplot( data = data.frame( res.lfc ) , aes( x=log2FoldChange , y = -log10(padj) , col =diffexpressed ) ) + geom_point() + geom_vline(xintercept=c(-logFCthreshold, logFCthreshold), col=\"red\") +\ngeom_hline(yintercept=-log10(FDRthreshold), col=\"red\") +\nscale_color_manual(values=c(\"blue\", \"grey\", \"red\"))\n\ntable(res.lfc$diffexpressed)\n</code></pre> <pre><code> DOWN    NO    UP \n  131 19002   245 \n</code></pre> <p></p> <pre><code>library(pheatmap)\ntopVarGenes &lt;- head(order(rowVars(vst.dds.f.counts), decreasing = TRUE), 20)\nmat  &lt;- vst.dds.f.counts[ topVarGenes, ] #scaled counts of the top genes\nmat  &lt;- mat - rowMeans(mat)  # centering\npheatmap(mat)\n</code></pre> <p></p> <pre><code># saving results to file\n# note: a CSV file can be imported into Excel\nwrite.csv( res ,'Ruhland2016.DESeq2.results.csv' )\n</code></pre>"},{"location":"days/DE/#ruhland2016-edger-correction","title":"Ruhland2016 - EdgeR correction","text":"<p>edgeR user\u2019s guide</p> read in the data <pre><code>library(edgeR)\nlibrary(ggplot2)\n\n# reading the counts files - adapt the file path to your situation\nraw_counts &lt;- read.table('/data/Solutions/Ruhland2016/countFiles/featureCounts_Ruhland2016.counts.txt' , skip=1 , sep=\"\\t\" , header=T)\n\n# setting up row names as ensembl gene ids\nrow.names(raw_counts) = raw_counts$Geneid\n\n# removing these first columns to keep only the sample counts\nraw_counts = raw_counts[ ,  -1:-6  ] # changing column names\nnames( raw_counts) = gsub('_.*', '', gsub('.*.SRR[0-9]{7}_', '', names(raw_counts) ) )\n\n# some checking of what we just read\nhead(raw_counts); tail(raw_counts); dim(raw_counts)\ncolSums(raw_counts) # total number of counted reads per sample\n</code></pre> edgeR object preprocessing <pre><code># setting up the experimental design AND the model\n#  -&gt; the first 3 samples form a group, the 3 remaining are the other group\ntreatment &lt;-  c(rep(\"EtOH\",3), rep(\"TAM\",3))\ndge.f.design &lt;- model.matrix(~ treatment)\n\n# creating the edgeR DGE object\ndge.all &lt;- DGEList(counts = raw_counts , group = treatment)  # filtering by expression level. See ?filterByExpr for details\nkeep &lt;- filterByExpr(dge.all)\ndge.f &lt;- dge.all[keep, keep.lib.sizes=FALSE]\ntable( keep )\n</code></pre> <pre><code>keep\nFALSE  TRUE \n39702 15712 \n</code></pre> <p>Around 16k genes are sufficiently expressed to be retained.</p> <pre><code>#normalization\ndge.f &lt;- calcNormFactors(dge.f)\ndge.f$samples\n</code></pre> <p>Each sample has been associated with a normalization factor.</p> edgeR model fitting <p><pre><code># estimate of the dispersion\ndge.f &lt;- estimateDisp(dge.f,dge.f.design , robust = T)\nplotBCV(dge.f)\n</code></pre> </p> <p>This plot is not easy to interpret. It represents the amount of biological variation at different levels of expression. It is directly linked to our ability to detect differential expression.</p> <p>Here it looks about normal compared to other bulk RNA-seq experiments : the variation is comparatively larger for lowly expressed genes.</p> <pre><code># testing for differential expression. \n# This method is recommended when you only have 2 groups to compare\ndge.f.et &lt;- exactTest(dge.f)\ntopTags(dge.f.et) # printing the genes where the p-value of differential expression if the lowest\n</code></pre> <pre><code>Comparison of groups:  TAM-EtOH \n                       logFC   logCPM       PValue          FDR\nENSMUSG00000050272 -8.522762 4.988067 2.554513e-28 3.851950e-24\nENSMUSG00000075014  3.890079 5.175181 2.036909e-25 1.535728e-21\nENSMUSG00000009185  3.837786 6.742422 1.553964e-22 7.810743e-19\nENSMUSG00000075015  3.778523 3.274463 2.106799e-22 7.942107e-19\nENSMUSG00000028339 -5.692069 6.372980 4.593720e-16 1.385374e-12\nENSMUSG00000040111 -2.141221 6.771538 4.954522e-15 1.245154e-11\nENSMUSG00000041695  4.123972 1.668247 6.057909e-15 1.304960e-11\nENSMUSG00000072941  3.609170 7.080257 1.807618e-14 3.407135e-11\nENSMUSG00000000120 -6.340146 6.351489 2.507019e-14 4.200371e-11\nENSMUSG00000034981  3.727969 5.244841 3.934957e-14 5.933521e-11\n</code></pre> <pre><code># see how many genes are DE\nsummary(decideTests(dge.f.et , p.value = 0.01)) # let's use 0.01 as a threshold\n</code></pre> <pre><code>         TAM-EtOH \nDown     109\nNotSig 15393\nUp       210\n</code></pre> <p>The comparison is TAM-EtOH, so \u201cUp\u201d, corresponds to a higher in group TAM compared to group EtOH.</p> edgeR looking at differentially-expressed genes <pre><code>## plot all the logFCs versus average count size. Significantly DE genes are  colored\npar(mfrow=c(1,1))\nplotMD(dge.f.et)\n# lines at a log2FC of 1/-1, corresponding to a shift in expression of x2 \nabline(h=c(-1,1), col=\"blue\") </code></pre> <p></p> <p><pre><code>## Volcano plot\nallGenes = topTags(dge.f.et , n = nrow(dge.f.et$table) )$table\n\nFDRthreshold = 0.01\nlogFCthreshold = 1.0\n# add a column of NAs\nallGenes$diffexpressed &lt;- \"NO\"\n# if log2Foldchange &gt; 1 and pvalue &lt; 0.01, set as \"UP\" \nallGenes$diffexpressed[allGenes$logFC &gt; logFCthreshold &amp; allGenes$FDR &lt; FDRthreshold] &lt;- \"UP\"\n# if log2Foldchange &lt; 1 and pvalue &lt; 0.01, set as \"DOWN\"\nallGenes$diffexpressed[allGenes$logFC &lt; -logFCthreshold &amp; allGenes$FDR &lt; FDRthreshold] &lt;- \"DOWN\"\n\nggplot( data = allGenes , aes( x=logFC , y = -log10(FDR) , col =diffexpressed ) ) + geom_point() + geom_vline(xintercept=c(-logFCthreshold, logFCthreshold), col=\"red\") +\ngeom_hline(yintercept=-log10(FDRthreshold), col=\"red\") +\nscale_color_manual(values=c(\"blue\", \"grey\", \"red\"))\n</code></pre> </p> <pre><code>## writing the table of results\nwrite.csv( allGenes , 'Ruhland2016.edgeR.results.csv')\n</code></pre> edgeR extra stuff <pre><code># how to extract log CPM\nlogcpm &lt;- cpm(dge.f, prior.count=2, log=TRUE)\n</code></pre> <pre><code># there is another fitting method reliying on quasi-likelihood, which is useful when the model is more complex (ie. more than 1 factor with 2 levels)\ndge.f.QLfit &lt;- glmQLFit(dge.f, dge.f.design)\ndge.f.qlt &lt;- glmQLFTest(dge.f.QLfit, coef=2)\n\n# you can see the results are relatively different. The order of genes changes a bit, and the p-values are more profoundly affected\ntopTags(dge.f.et)\ntopTags(dge.f.qlt)\n\n## let's see how much the two methods agree:\npar(mfrow=c(1,2))\nplot( dge.f.et$table$logFC , dge.f.qlt$table$logFC,\nxlab = 'exact test logFC',\nylab = 'quasi-likelihood test logFC')\n\nprint( paste('logFC pearson correlation coefficient :' , cor(dge.f.et$table$logFC ,dge.f.qlt$table$logFC) ) )\n\nplot( log10(dge.f.et$table$PValue ), log10(dge.f.qlt$table$PValue) ,\nxlab = 'exact test p-values (log10)',\nylab = 'quasi-likelihood test p-values (log10)')\n\nprint( paste( \"P-values spearman correlation coefficient\",\ncor( log10(dge.f.et$table$PValue ), log10(dge.f.qlt$table$PValue) , method = 'spearman' )))\n</code></pre> <pre><code>\"logFC pearson correlation coefficient : 0.999997655536736\"\n\"P-values spearman correlation coefficient 0.993238670517236\"\n</code></pre> <p></p> <p>The logFC are highly correlated. FDRs show less correlation but their ranks are highly correlated : they come in a very similar order.</p>"},{"location":"days/DE/#tuch-2010-edger-correction","title":"Tuch 2010 - EdgeR correction","text":"<p>We refer you here to section 4.1 of edgeR\u2019s vignette.</p>"},{"location":"days/DE/#mansingh-2024-correction","title":"Mansingh 2024 - correction","text":"<p>Here you can download more or less the script we used to analyze this data in the paper.</p> <p>You will see that the analysis is fairly complex, with exclusion of outliers, accounting for technical batch effect, \u2026</p> <p>Also, this code covers enrichment too</p> <p>  Mansingh 2024 analysis script</p>"},{"location":"days/DE/#additional-importing-counts-from-salmon-with-tximport","title":"Additional : importing counts from salmon with <code>tximport</code>","text":"<p>The <code>tximport</code> R packages offers a fairly simple set of functions to get transcript-level expression quantification from salmon or kallisto into a differential gene expression analysis.</p> <p>Task : import salmon transcript-level quantification in R in order to perform a DE analysis on it using either edgeR or DESeq2. Additional: compare the results with the ones obtained from STAR-aligned reads.</p> <ul> <li>The tximport vignette is a very good guide for this task.</li> <li>If you have not computed them, you can find files with expression quantifications in <code>/data/Solutions/Ruhland2016/</code></li> </ul>"},{"location":"days/R_crash/","title":"R crash course","text":"<p>The goal of this crash-course is to get you acquainted with all the basic concepts and syntax of the R programming language.</p> <p>Mastering R is not something done in a single day, and so the goal is not to make you into R guru\u2019s, but to give you enough groundings so that, armed with a good cheatsheet and with a bit of patience, you should be able to understand what happens in the R codes used when conducting a differential expression or enrichment analysis.</p> <p>This crash course can be followed along either on a distant Rstudio server (the teacher will tell you how to connect there), or on your own machine.</p> <p>Warning</p> <p>If you choose to follow along on your own machine, please download and install Rstudio before the course.</p> <p>You will learn to :</p> <ul> <li>recognize the basic R elements (variables, functions, arguments,\u2026)</li> <li>read data from csv/tsv files into R</li> <li>perform basic mathematical and statistical computations with R functions </li> <li>represent your data in (beautiful) plots</li> </ul> <p>So, start your Rstudio (or connect to the Rstudio server), and let\u2019s dive into R.</p>"},{"location":"days/R_crash/#meet-rstudio","title":"meet Rstudio","text":"<p>Once you have opened Rstudio you should see something a bit like this:</p> <p></p> <p>Except in your case:</p> <ul> <li>the Editor (upper-left) will be absent (because you have not opened a file yet)</li> <li>the Environment (upper-right) will be empty </li> </ul> <p>Anyhow, the 4 panels respecttively let you access:</p> <ol> <li>upper-left : Editor - see and edit file (mostly scripts)</li> <li>lower-left : Console - things typed there will be interpreted as code (after you hit the Enter key)<ul> <li>Console : for R code</li> <li>Terminal : for UNIX command lines</li> </ul> </li> <li>upper-right : Environment/History - you can review which variable exists at the moment and which commands you executed before there</li> <li>lower-right : <ul> <li>Files - to see, navigate, and interact with your files</li> <li>Plots - to show the plots you will create</li> <li>Help - to display the help pages when you ask for them</li> </ul> </li> </ol>"},{"location":"days/R_crash/#get-the-data","title":"Get the data","text":"<p> Download the data</p> <ol> <li>Download the data above </li> <li>Move the data where you want to conduct the practicals (if you are on the server, follow the teacher\u2019s intructions)</li> <li>unzip the data (otherwise you will not be able to access it)</li> </ol>"},{"location":"days/R_crash/#the-basics-operators-variables-functions-vectors","title":"The basics : operators, variables, functions, vectors","text":"<p>You can start typing some R code directly in the Console, and after you hit enter it will be interpreted as R code. </p> <p>Some command will display a result, and other won\u2019t (but they may have another effect like creating a new variable, or changing a variable\u2019s content)</p> <p>For example, typing:</p> <pre><code>5+7\n</code></pre> <p>gives:</p> <pre><code>[1] 12\n</code></pre> <p>whereas : <pre><code>x &lt;- 128.5\n</code></pre> gives nothing, but a new variable <code>x</code> is now in your environment.</p>"},{"location":"days/R_crash/#variable","title":"Variable","text":"<p>Variables are generally containers for data. It can be simple data, like a single number, or more complex like a table or a set of DE results.</p> <ul> <li>variables are created when we assign them a value, either with the symbol <code>&lt;-</code> or <code>=</code></li> <li> <p>variables are identified by their name, which:</p> <ul> <li>cannot start with a number</li> <li>can contain the <code>.</code> or <code>_</code> special characters</li> </ul> </li> <li> <p>you can see the value of a variable by calling it\u2019s name in the console</p> </li> <li>variables values can be changed with assignment too</li> <li>deleting a variable is possible, but not so common</li> </ul> <p>Note</p> <p>the assignment is always from right to left:</p> <ul> <li><code>x = 9</code> OK</li> <li><code>9 = x</code> NOT OK</li> </ul> <p>Try it out: <pre><code>x &lt;- 128.5\nx\n\ny = -12\ny\n\nmy.var2 = 160\nmy.var2\n</code></pre></p> <p>As we have seen we can use operators to perform simple operations</p> <ul> <li><code>+</code> : addition</li> <li><code>-</code> : subtraction</li> <li><code>*</code> : multiplication</li> <li><code>/</code> : division</li> <li><code>^</code> : power</li> <li><code>()</code> : parenthesis work exactly as in math</li> </ul> <p>You can use variables and numbers together.</p> <pre><code>x / y\n\nz = x^2 / (y + 5)\nz\n</code></pre> <p>Note</p> <p>the code above will work only when the variable contain numerical values. If they contain something else (like text for instance), then you will likely get an error.</p>"},{"location":"days/R_crash/#function","title":"Function","text":"<p>Functions are bits of code packaged for easy use.</p> <p>Like variables, they have a name (actually, they are variables), and they can be called, meaning that we execute the code in the function, with the syntax </p> <pre><code>function_name( ... arguments ... )\n</code></pre> <p>Arguments are variables that we give to the function to modify their behavior.</p> <p>Each function has their own arguments, sometimes none, often several, some mandatory, and some facultative.</p> <pre><code># BTW if you start a line with a # then it is a \"comment\": not interpreted by R as code\n\n# function that takes no argument:\ngetwd()\n\n# function that takes 1 argument:\nabs(-11)\nabs(x)\nlog(x)\n\n# when there is more than 1 argument they are separated by comma\nmin( x , y )\n\n# some arguments are facultative and can be declared by name\nlog( x  )\nlog( x , base = 2 )\n</code></pre> <p>You don\u2019t have to guess which argument a function expect: use <code>?function_name</code> to look up its documentation.</p> <pre><code>?sqrt\n</code></pre>"},{"location":"days/R_crash/#vectors","title":"Vectors","text":"<p>Variables can contain any sort of data (or functions), and one of the most common is the vector, which is a list of elements of the same type (numbers, text, \u2026 more on that later).</p> <pre><code>y &lt;- c( 12 , 15 , 137 , 4 )\ny\n</code></pre> <p>You can do operation and call functions with numbers seamlessly:</p> <pre><code>y + 10\n\nlog(y)\n\n\nlog(y) + x^2\n\n\n# many function actually expect a vector as argument\nmean(y)\n\nmedian(y)\n</code></pre> <p>A single element of a vector can be accessed with <code>[]</code>:</p> <pre><code>y[1] # 1st element\n\ny[3] # 3rd element\n\n\ny[100] # 100th element -&gt; does not exist -&gt; you get NA = Not Acquired\n</code></pre> <p>A vector type determine its behavior. </p> <p>You can learn it with the <code>class</code> function:</p> <pre><code>class(y)\n\n\na = c('conditionA',\"conditionB\") # character vector; NB: ' ' or \" \" are both valid\nclass(a)\n\n\nb = c(TRUE,FALSE,TRUE,TRUE) # boolean vector ; NB: all character needs to be UPPERCASE here\nclass(b)\n</code></pre> <p>We will mostly play with numeric vectors, but it is important to know about the rest.</p> <p>If you try to apply a function for numeric to a character vector you may get an error.</p> <pre><code>log(a)\n</code></pre> <p>Error in log(a) : non-numeric argument to mathematical function</p> <p>Whenever you encounter an error, take to time to read it as it is often informative.</p>"},{"location":"days/R_crash/#exercise-variables-vectors-and-functions","title":"Exercise - variables, vectors and functions","text":"<ul> <li>create a vector named vector1 containing the numbers 2984, 4682, 1932 ,45 ,12,135</li> <li>compute the sum of all elements in your vector using the <code>sum()</code> function</li> <li>create a new vector named vector2 containing the elements of vector1 divided by the sum you just obtained</li> </ul> Solution <pre><code>vector1 = c( 2984, 4682, 1932 ,45 ,12,135 )\n\nvector1.sum = sum( vector1 )\nvector1.sum\n\nvector2 = vector1 / vector1.sum\nvector2\n</code></pre>"},{"location":"days/R_crash/#reading-and-manipulating-tabular-data","title":"Reading and manipulating tabular data","text":""},{"location":"days/R_crash/#working-directory","title":"Working directory","text":"<p>This is when things become a bit more complex.</p> <p>You code is executing from someplace in your computer (often by default it will be your \u201chome\u201d directory), we call this the working directory.</p> <p>Use <code>getwd()</code> to learn what it is:</p> <p><pre><code>getwd()\n</code></pre> <code>[1] \"/home/wandrille\"</code></p> <p>Then imagine you have a csv file named <code>iris.csv</code>, somewhere in the computer.</p> <p>If it is in the same folder as my working directory, then I can read the file directly my something like <pre><code>read.csv(\"iris.csv\")\n</code></pre></p> <p>Otherwise if it is let\u2019s say in <code>/home/wandrille/R_crash_course_data/</code>. You can see that the beginning here is the same, so, relative to my working directory the file is in subfolder <code>R_crash_course_data/</code>.</p> <p>Then we need to use: <pre><code>read.csv(\"R_crash_course_data/iris.csv\")\n</code></pre></p> <p>If you have made a typo in the file name or pointed to the wrong folder, you will get an error like:</p> <p> Error in file(file, \"rt\") : cannot open the connection  In addition: Warning message:  In file(file, \"rt\") :    cannot open file 'iris.csv': No such file or directory  </p> <p>Which is a very common error.</p>"},{"location":"days/R_crash/#basic-reading-and-manipulating","title":"Basic reading and manipulating","text":"<p>You can read a simple csv file, with a header line to give column names, with the command:</p> <pre><code>read.csv(\"R_crash_course_data/iris.csv\")\n</code></pre> <p>But this prints the file content on the screen, what you usually want is to store it in a variable:</p> <pre><code>iris = read.csv('iris.csv')\niris\n</code></pre> <p><code>iris</code> is an object of class <code>data.frame</code>, whic contains tabular data.</p> <p>A data frame is like a collection of vectors, where each vector forms a column; they are a very common variable type for any form of tabular data.</p> <pre><code># show the first few lines of the data frame\nhead( iris )\n\n# show the size of the data frame\ndim( iris )\n\n# show the column names of the data frame\nnames( iris )\n</code></pre> <p>We can access columns individually: <pre><code>## access a single column, the result is a vector\niris$Sepal.Length\n\n## access a single column, but the result is a data frame with 1 column\niris['Sepal.Length']\n</code></pre></p> <p>We can apply the functions we have seen before to individual columns, as well as several more which can be applied to the whole dataframe at once.</p> <p><pre><code>mean( iris$Sepal.Length )\n\nsummary( iris$Sepal.Length )\n\nsummary(iris)\n</code></pre> Notice the last column, <code>Species</code>, which is a character vector and does not have a very useful summary.</p> <p>This can sometimes cause further problems, for instance:</p> <pre><code>colMeans(iris)\n</code></pre> <p>Error in colMeans(iris) : 'x' must be numeric</p> <p>This can be resolved by making a copy of the data frame with the numeric columns only <pre><code>iris_numerical = iris[ , c( \"Sepal.Length\", \"Sepal.Width\", \"Petal.Length\", \"Petal.Width\" ) ]\niris_numerical\n\ncolMeans(iris_numerical)\nrowMeans(iris_numerical)\n</code></pre></p> <p>As we are starting to accumulate a lot of code, it would be a good moment to start a script.</p> <p>A script is simply a text file containing R commands.</p> <p>To create one, use <code>Ctrl+Alt+Shift+N</code>, or go to <code>Flie &gt; New File &gt; R script</code>.</p> <p>This will open a new file on the top-left section of Rstudio, where you can write some code.</p> <p>Important reminder</p> <p>remember to save your file, and to save often</p> <p>The code you write in the script can be send to the console using <code>Ctrl+Enter</code> or clicking on the run button:</p> <p></p>"},{"location":"days/R_crash/#exercise-basic-reading-and-manipulating","title":"Exercise - basic reading and manipulating","text":"<ul> <li>read the csv file \u201cdiamonds.csv\u201d in a dataframe</li> <li>what are the name of the columns of this dataframe?</li> <li>use the median function to compute the median carat in this data (<code>carat</code> column)</li> <li>use summary to get the minimum and maximum price </li> </ul> Solution <pre><code>diamonds = read.csv('diamonds.csv')\ncolnames(diamonds)\nmedian( diamonds$carat )\nsummary( diamonds$price )\n</code></pre>"},{"location":"days/R_crash/#more-on-reading-files","title":"More on reading files","text":"<p>When the file you want to read in the in the current working directory you have to specify to R where it can find the file: <pre><code>diamonds = read.csv('data/diamonds.csv')\n</code></pre></p> <p>Some files are not in csv format: <pre><code>diamonds = read.csv('data/diamonds.tsv')\nhead( diamonds )\n</code></pre> <pre><code>            carat.cut.color.clarity.depth.table.price.x.y.z\n1      0.23\\tIdeal\\tE\\tSI2\\t61.5\\t55\\t326\\t3.95\\t3.98\\t2.43\n2    0.21\\tPremium\\tE\\tSI1\\t59.8\\t61\\t326\\t3.89\\t3.84\\t2.31\n3       0.23\\tGood\\tE\\tVS1\\t56.9\\t65\\t327\\t4.05\\t4.07\\t2.31\n4     0.29\\tPremium\\tI\\tVS2\\t62.4\\t58\\t334\\t4.2\\t4.23\\t2.63\n5       0.31\\tGood\\tJ\\tSI2\\t63.3\\t58\\t335\\t4.34\\t4.35\\t2.75\n6 0.24\\tVery Good\\tJ\\tVVS2\\t62.8\\t57\\t336\\t3.94\\t3.96\\t2.48\n</code></pre></p> <p>Then, we need to use an appropriate function.  <pre><code># read.table is a very modular function to read most tables in a text file\ndiamonds = read.table('data/diamonds.tsv')\nhead(diamonds)\n</code></pre> <pre><code>     V1      V2    V3      V4    V5    V6    V7   V8   V9  V10\n1 carat     cut color clarity depth table price    x    y    z\n2  0.23   Ideal     E     SI2  61.5    55   326 3.95 3.98 2.43\n3  0.21 Premium     E     SI1  59.8    61   326 3.89 3.84 2.31\n4  0.23    Good     E     VS1  56.9    65   327 4.05 4.07 2.31\n5  0.29 Premium     I     VS2  62.4    58   334  4.2 4.23 2.63\n6  0.31    Good     J     SI2  63.3    58   335 4.34 4.35 2.75\n</code></pre></p> <p><pre><code># by default, read.table does not treat the first line as a header\n# we set the argument header to TRUE to change this behavior\ndiamonds = read.table('data/diamonds.tsv' , header = TRUE)\nhead(diamonds)\n</code></pre> <pre><code>  carat       cut color clarity depth table price    x    y    z\n1  0.23     Ideal     E     SI2  61.5    55   326 3.95 3.98 2.43\n2  0.21   Premium     E     SI1  59.8    61   326 3.89 3.84 2.31\n3  0.23      Good     E     VS1  56.9    65   327 4.05 4.07 2.31\n4  0.29   Premium     I     VS2  62.4    58   334 4.20 4.23 2.63\n5  0.31      Good     J     SI2  63.3    58   335 4.34 4.35 2.75\n6  0.24 Very Good     J    VVS2  62.8    57   336 3.94 3.96 2.48\n</code></pre></p> <p>You can learn more in the documentation of <code>read.table</code> <pre><code>?read.table\n</code></pre></p> <p>Arguably, the main arguments are:</p> <ul> <li><code>sep</code> : separator between fields</li> <li><code>header</code> : does the first line contain column names?</li> <li><code>row.names</code> : set to 1 to have the first column interpreted as row names. Useful when reading expression matrices.</li> </ul>"},{"location":"days/R_crash/#selection","title":"Selection","text":"<p>Some columns contain text data (<code>character</code>) <pre><code>iris$Species\n</code></pre></p> <p>We can get a \u201csummary\u201d with the <code>table</code> function: <pre><code>table( iris$Species )\n</code></pre></p> <p>We can use a comparison operator to test conditions on data. These comparisons are the basis for selecting parts of our data.</p> <p>The main comparison operators are:</p> <ul> <li><code>==</code> : tests equality</li> <li><code>&lt;</code> or <code>&gt;</code> : under or above</li> <li><code>&lt;=</code> : under or equal</li> <li><code>&gt;=</code> : above or equal</li> </ul> <p><pre><code>5 &gt; 10\n</code></pre> <pre><code>FALSE\n</code></pre> Indeed 5 is not greater than 10</p> <p>In practise, here is what we do: <pre><code>iris$Species == 'setosa'\n\niris$Petal.Length &gt; 5\n</code></pre></p> <p>Both of these commands give a vector of booleans values (FALSE/TRUE). These vector can be given to the <code>[]</code> operator which will select the elements which are <code>TRUE</code>.</p> <pre><code>iris$Species[ iris$Species == 'setosa' ]\n</code></pre> <p>Petal lengths of the <code>Iris setosa</code> samples: <pre><code>iris$Petal.Length[ iris$Species == 'setosa' ]\n</code></pre></p> <p>We can do the same for data frames.  Because data frames are 2D tables (with rows and columns), the syntaxt is a bit different: <code>dataframe[ selected_rows , selected_columns ]</code></p> <p>If selected rows or columns is left empty, then all elements are selected.</p> <pre><code>iris[ iris$Species == 'setosa' , ]\n\niris[ iris$Species == 'setosa' , 'Petal.Length' ]\n</code></pre> <p>With this, we can use these selection to make further computations, such as computing the mean petal length for different species:</p> <pre><code>mean( iris[ iris$Species == 'setosa' , 'Petal.Length' ] )\nmean( iris[ iris$Species == 'virginica' , 'Petal.Length' ] )\n</code></pre> <p>Finally, in some case we want to combine some filters. This can be done with :</p> <ul> <li><code>&amp;</code> : and - select elements that satisfy both filters</li> <li><code>|</code> : or - select elements that satisfy either of the two filters</li> </ul> <pre><code># iris of the setosa species AND with a petal length above 1.5\niris[ iris$Species == 'setosa' &amp; iris$Petal.Length&gt;1.5 , ]\n</code></pre>"},{"location":"days/R_crash/#exercise-selection","title":"Exercise - Selection","text":"<ol> <li>read the csv file \u201cDEresults.csv\u201d in a dataframe</li> <li>how many elements have a significant adjusted p-value (column <code>padj</code>)?</li> <li>how many elements have an absolute value of <code>log2FoldChange &gt; 1</code>?</li> <li>how many elements have a significant adjusted p-value (column <code>padj</code>) and a positive <code>log2FoldChange</code>?</li> </ol> Solution <pre><code># 1. reading the file\ndf = read.csv( 'DEresults.csv' )\n\n# 2. selecting significant adjusted p-value\ntable( df$padj  &lt; 0.05 )\n\n# 3. selecting absolute values of log2 Fold-Change above 1\ntable( abs( df$log2FoldChange ) &gt; 1 )\n\n# OR alternatively\ntable( df$log2FoldChange &gt; 1 | df$log2FoldChange &lt; -1 )\n\n# 4. significant adjusted p-value and positive log2 Fold-Change\ntable( df$log2FoldChange &gt; 0 &amp; df$padj  &lt; 0.05 )\n\n# OR alternatively we can get a two by two table\ntable( df$padj &lt; 0.05  , df$log2FoldChange &gt; 0 )\n</code></pre>"},{"location":"days/R_crash/#writing-data-frame-to-files","title":"Writing data frame to files","text":"<p>Let\u2019s create a new data frame from the <code>Iris virginica</code> only :</p> <pre><code>virginica_iris = iris[ iris$Species == 'virginica' , ]\nhead( virginica_iris )\n</code></pre> <p>Then we write it with <code>write.csv</code>: <pre><code>write.csv( virginica_iris , 'virginica.csv' , row.names = FALSE )\n</code></pre></p> <p><code>write.table</code> exists as well, and generally they have the same arguments as <code>read.csv</code> and <code>read.table</code>.</p>"},{"location":"days/R_crash/#creating-new-columns","title":"Creating new columns","text":"<p>By dividing the sepal width by the sepal height, we get a ratio for each sample: <pre><code>iris$Sepal.Width / iris$Sepal.Height\n</code></pre></p> <p>We can set it as a new column in the data frame simple by doing an assignment to a column:</p> <pre><code>iris$ratio = iris$Sepal.Length / iris$Sepal.Width\nhead( iris )\n</code></pre>"},{"location":"days/R_crash/#plotting-with-ggplot2","title":"plotting with ggplot2","text":"<p>There exists several ways of creating plots in R, but arguably the most common is the ggplot2 library.</p>"},{"location":"days/R_crash/#detour-libraries","title":"detour: libraries","text":"<p>A library is a set of external functions, variables, \u2026 which have been packaged together.</p> <p>Because there are thousands of these libraries, they are not installed by default when you first install R,  and they are not loaded in your R session.</p> <p>So to use them you need to :</p> <ol> <li>ensure they are installed (you need to install them only once)</li> <li>load them in your session (you need to do this for each R session you start)</li> </ol>"},{"location":"days/R_crash/#installing-a-library","title":"installing a library","text":"<p>Note</p> <p>The Rstudio instance on the server should have all the libraries you will need already installed, so there is no need for you to perform these installations there.</p> <p>For our purpose R libraries can be found in two main repositories:  * CRAN : generalist repository     * install with <code>install.packages(\"package_name\")</code>  * Bioconductor : specialised in bioinformatics libraries     * install with <code>BiocManager::install(\"package_name\")</code>     * <code>BiocManager</code> itself may need to be installed from CRAN with <code>install.packages(\"BiocManager\")</code></p> <p>In RNAseq analysis yt is common to mix libraries from both sources ( our package for plotting or doing a PCA comes from CRAN, and our package that retrieves genome annotation is from Bioconductor).</p> <p>Note</p> <p>note the quotes around the library name when calling <code>install.packages</code> or <code>BiocManager::install</code></p>"},{"location":"days/R_crash/#loading-a-library","title":"loading a library","text":"<p>Irrespective of the repository the library came from, once it is installed in your R session you can load it with :</p> <pre><code>#loading ggplot2\nlibrary(ggplot2)\n</code></pre> <p>Note</p> <p>note the absence of quotes around the library name when calling <code>library</code></p>"},{"location":"days/R_crash/#first-steps-in-ggplot2","title":"First steps in ggplot2","text":"<p>We want to represent the relationship between petal length and petal width.</p> <p>The basic call to ggplot2 is made with the <code>ggplot</code> function, which typically takes to arguments:</p> <ul> <li>a dataframe containing the data we want to represent</li> <li>an aesthetic : a call to the <code>aes</code> function defines which elements of the data we want to map to graphical elements (x-axis, y-axis, color, \u2026)</li> </ul> <pre><code>ggplot( iris , aes( x = Petal.Length , y = Petal.Width)  )\n</code></pre> <p>At this stage, we have a graphical window, with axes defines and all, but we need to add graphical element on top of it.</p> <p>This is done by saving the plot in a variable, and then adding (with <code>+</code>) the results of ggplot2 functions to it.</p> <pre><code>p = ggplot( iris , aes( x = Petal.Length , y = Petal.Width)  )\np\n</code></pre> <p><code>geom_...</code> functions usually add geometric objects : line, points, bars, \u2026  <pre><code># geom_point() set up point\np + geom_point()\n</code></pre></p> <pre><code># p is unchanged\np\n</code></pre> <pre><code># p is changed\np = p + geom_point()\np\n</code></pre>"},{"location":"days/R_crash/#exercise-a-simple-line","title":"Exercise - a simple line","text":"<p>Copy-paste and run the following line: <pre><code>df2 &lt;- data.frame(time=1:100,value=sin((1:100)*0.1) + rnorm(100)*0.1)\n</code></pre> It creates a <code>data.frame</code> with two columns.</p> <p>Using ggplot, create a plot of this data with <code>time</code> as x-axis and <code>value</code> as y-axis.</p> <ol> <li>make a scatter plot with : <code>geom_point()</code> </li> <li>make a line plot with : <code>geom_line()</code> </li> <li>use both <code>geom_point()</code> and <code>geom_line()</code></li> </ol> Solution <pre><code>df2 &lt;- data.frame(time=1:100,value=sin((1:100)*0.1) + rnorm(100)*0.1)\n\np = ggplot(df2 , aes(x=time , y =  value))\n\n#1.\np + geom_point()\n#2.\np + geom_line()\n#3.\np + geom_point() + geom_line()\n</code></pre>"},{"location":"days/R_crash/#adding-some-color","title":"Adding some color","text":"<p>Adding color can be as simple a specifying which column to map to color in <code>aes()</code></p> <pre><code>## categorical color\np1 = ggplot( iris , aes( x = Petal.Length , y = Petal.Width , color = Species)  ) + geom_point()\np1\n</code></pre> <pre><code>## continuous color\np2 = ggplot( iris , aes( x = Petal.Length , y = Petal.Width , color = Sepal.Length)  ) + geom_point()\np2\n</code></pre> <p>Of course, we often want to change the default color scheme:</p> <pre><code>p1 + scale_color_manual(values=c(\"#69b3a2\", \"purple\", \"black\"))\np1 + scale_color_brewer(palette = \"Spectral\")\n\np2 + scale_color_gradient(low=\"blue\", high=\"red\")\n</code></pre> <p>These are just a couple of example, you can read more in this nice guide on ggplot2 colors</p> <p>Rather than point colors, you can also change the general theme of the plot: <pre><code>p1\np1 + theme_dark()\np1 + theme_minimal()\n</code></pre></p> <p>More on themes</p>"},{"location":"days/R_crash/#changing-axis-labels-and-titles","title":"changing axis labels and titles","text":"<pre><code>p1\np1 + xlab( \"petal length (cm)\") +\nylab( \"petal width (cm)\") +\nggtitle('Petal measurements of some iris')\n</code></pre>"},{"location":"days/R_crash/#more-plots-the-sky-is-the-limit","title":"more plots - the sky is the limit","text":"<pre><code>p = ggplot( iris , aes( x = Species , y = Petal.Length ) )\n# violin plot\np + geom_violin() + geom_jitter()\n\n# box plot\np + geom_boxplot() + geom_jitter()\n\n# histogram plot\nggplot( iris , aes( x = Petal.Length ) ) + geom_histogram()\n</code></pre> <pre><code># getting some data for a line plot\ndf2 &lt;- data.frame(supp=rep(c(\"VC\", \"OJ\"), each=3),\ndose=rep(c(\"D0.5\", \"D1\", \"D2\"),2),\nlen=c(6.8, 15, 33, 4.2, 10, 29.5))\nhead(df2)\n</code></pre> <pre><code># line plot with different lines are done with the group aesthetic\nggplot(data=df2, aes(x=dose, y=len, group=supp)) +\ngeom_line(aes(linetype=supp))+ # locally mapping supp to the linetype aesthetic\ngeom_point()\n</code></pre> <ul> <li>ggplot2 reference</li> <li>r-graph-gallery</li> </ul> <p>Warning</p> <p>When plotting, you will quickly realize that one could spend an eternity tuning this or that about the graphical element to make them look just the way they want. </p> <p>If you do not want to lose too much time with it, my advice is that most of the time during your analysis you just want to look at data and it does not need to look perfect, and figures often change a lot before the final version of the manuscript.</p> <p>So during the analysis itself, I would keep the tuning to a minimum as the default themes usually make a good job; and I would invest more time getting pretty pictures only once we have decided on precise figures or slides.</p>"},{"location":"days/R_crash/#recapitulative-exercise","title":"Recapitulative exercise","text":"<p>We are going, step-by-step, to build a volcano plot out of Differential expression data we used in the previous exercise</p> <ol> <li>read the csv file \u201cDEresults.csv\u201d in a dataframe</li> </ol> Solution <pre><code>df = read.csv( 'DEresults.csv' )\n</code></pre> <ol> <li>create a scatter plot with the logFoldChange as x, and the adjusted p-value (padj) as y</li> </ol> Solution <pre><code>ggplot(df , aes(x = log2FoldChange ,y = padj )  ) +\ngeom_point()\n</code></pre> <ol> <li>update this plot using the function : scale_y_log10() . what happens?</li> </ol> Solution <pre><code>ggplot(df , aes(x = log2FoldChange ,y = padj )  ) + geom_point() + scale_y_log10()\n</code></pre> <p>Next we want to color points in grey if they are not significant, and in black if they are. for this we are going to create a new column in the data frame which contains TRUE or FALSE depending on whether the gene significant and then we will use this column to color the points</p> <ol> <li>create a column in your data frame which is TRUE when the adjusted p-value is below 0.01 and the absolute log2FoldChange is above 1</li> </ol> Solution <pre><code>df$signif = df$padj &lt; 0.01 &amp; abs( df$log2FoldChange )  &gt; 1\n</code></pre> <ol> <li>color the points in your volcano plot according to this new column</li> </ol> Solution <pre><code>volcano = ggplot(df , aes(x = log2FoldChange ,y = padj , colour = signif )  ) + geom_point() + scale_y_log10()\nvolcano\n</code></pre> <ol> <li>change the colors so that non-significant point are in grey and significant in black (or anything pleasant to you)</li> </ol> Solution <pre><code>volcano = ggplot(df , aes(x = log2FoldChange ,y = padj , colour = signif )  ) + geom_point() + scale_y_log10() +\nscale_color_manual( values = c('grey','black') )\nvolcano\n</code></pre> <p>Next we would like to add vertical and horizontal lines to show the thrsholds we have applied on the adjusted p-value and log2FoldChange</p> <ol> <li>search the internet on how to add horizontal lines in ggplot, and then add dashed horizontal and vertical lines at padj=0.01, log2Folchange=1 and log2FoldChange=-1</li> </ol> Solution <pre><code>volcano + geom_hline( yintercept = 0.01 , linetype='dashed') + geom_vline( xintercept = c(-1,1) , linetype='dashed' )\n</code></pre> <p>extra question</p> <ol> <li>try to find a way to add to the volcano plot the gene name (column SYMBOL) of genes with an absolute logFoldChange &gt; 5</li> </ol> <p>Note</p> <p>There are several way of doing this and it is not that immediate, take your time, experiment, search the web).</p> Solution - part 1 <pre><code># creating a new column that will have empty text for all genes except the ones which satisfy our condition\ndf$label = ''\ndf$label[ abs( df$log2FoldChange ) &gt; 5 ] = df[ abs( df$log2FoldChange ) &gt; 5 , 'SYMBOL' ]\n</code></pre> Solution - part 2 <pre><code>ggplot(df , aes(x = log2FoldChange ,y = padj , colour = signif , label = label)  ) + geom_point() + scale_y_log10() +\nscale_color_manual( values = c('grey','black') ) +\ngeom_hline( yintercept = 0.01 , linetype='dashed') +\ngeom_vline( xintercept = c(-1,1) , linetype='dashed' ) +\ngeom_text( )\n</code></pre> Alternative solution <p>inspiration source <pre><code># install.packages(\"ggrepel\")\nlibrary(ggrepel)\nggplot(df , aes(x = log2FoldChange ,y = padj , colour = signif , label = label)  ) + geom_point() + scale_y_log10() +\nscale_color_manual( values = c('grey','black') ) +\ngeom_hline( yintercept = 0.01 , linetype='dashed') +\ngeom_vline( xintercept = c(-1,1) , linetype='dashed' ) +\ngeom_label_repel(aes(label = df$label ))\n\n\n# another technique, giving only a subset of the data to the function which ad the labels\nggplot(df , aes(x = log2FoldChange ,y = padj , colour = signif , label = label)  ) + geom_point() + scale_y_log10() +\nscale_color_manual( values = c('grey','black') ) +\ngeom_hline( yintercept = 0.01 , linetype='dashed') +\ngeom_vline( xintercept = c(-1,1) , linetype='dashed' ) +\ngeom_text_repel(\ndata = subset(df, abs( log2FoldChange ) &gt; 5 ),\naes(label = SYMBOL))\n</code></pre></p>"},{"location":"days/counting/","title":"Counting","text":"<p>Read counting refers to the quantification of an \u201cexpression level\u201d, or abundance, from reads mapped onto a reference genome/transcriptome. This expression level can take several forms, such as a count, or a fraction (RPKM/FPKM/TPM), and concern different entities (exons, transcripts, genes) depending on your biological application.</p> <p>During this lesson, you will learn to:</p> <ul> <li>differentiate between different levels of counting and their relevance for different questions.</li> <li>perform read counting at the gene level for Differential Gene expression.</li> </ul>"},{"location":"days/counting/#material","title":"Material","text":"<p> Download the presentation</p> <p>featureCounts website</p>"},{"location":"days/counting/#read-counting-with-featurecounts","title":"Read counting with featureCounts","text":"<p>The featureCount website provides several useful command-line examples to get started. For more details on the algorithm behavior (with multi/overlapping reads for instance), you can refer to the package\u2019s User\u2019s guide (go to the read summarization chapter).</p> <p>Task : </p> <ul> <li>Decide which parameters are appropriate for counting reads from the Ruhland dataset. Assume you are interested in determining which genes are differentially expressed.</li> <li>Count the reads from one of your BAM files using featureCount.</li> <li> <p>How do the featureCount-derived counts compare to those from STAR ?</p> </li> <li> <p>You can find bam files at <code>/shared/data/Solutions/Liu2015/STAR_Liu2015</code> and <code>/shared/data/Solutions/Ruhland2016/STAR_Ruhland2016</code></p> </li> <li>featureCount requirements : 400M RAM / BAM file</li> <li>featureCount requirements : 2 min CPU time / BAM file</li> </ul> featureCounts script <pre><code>#!/usr/bin/bash\n#SBATCH --job-name=featurecount\n#SBATCH --time=00:30:00\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=4G\n#SBATCH -o count.o\n#SBATCH -e count.e\n\n\nG_GTF=/shared/data/DATA/Mus_musculus.GRCm39.105.gtf\n\ninFOLDER=/shared/data/Solutions/Ruhland2016/STAR_Ruhland2016\noutFOLDER=featureCOUNT_Ruhland2016\n\nml subread\n\nmkdir -p $outFOLDER\n\nfeatureCounts -T 8 -a $G_GTF -t exon -g gene_id -o featureCounts_Ruhland2016.counts.txt \\\n                                    $inFOLDER/SRR3180535_EtOH1_1.fastq.gzAligned.sortedByCoord.out.bam \\\n                                    $inFOLDER/SRR3180536_EtOH2_1.fastq.gzAligned.sortedByCoord.out.bam \\\n                                    $inFOLDER/SRR3180537_EtOH3_1.fastq.gzAligned.sortedByCoord.out.bam \\\n                                    $inFOLDER/SRR3180538_TAM1_1.fastq.gzAligned.sortedByCoord.out.bam \\\n                                    $inFOLDER/SRR3180539_TAM2_1.fastq.gzAligned.sortedByCoord.out.bam \\\n                                    $inFOLDER/SRR3180540_TAM3_1.fastq.gzAligned.sortedByCoord.out.bam\n</code></pre> comparison with STAR counts <p>You can use this little R script to check they are the same :</p> <pre><code>fc = read.table( \"featureCounts_SRR3180535_EtOH1_1.counts.txt\" , header =T)\nrownames( fc ) = fc$Geneid\nhead( fc )\n\nstar = read.table( \"SRR3180535_EtOH1_1.fastq.gzReadsPerGene.out.tab\")\nrownames( star ) = star$V1\nhead( star )\n\n\nstar_count = star[ rownames( fc ) , 'V2' ]\nfC_count = fc$STAR_Ruhland2016.SRR3180535_EtOH1_1.fastq.gzAligned.sortedByCoord.out.bam\nplot(log10( star_count + 1),\n    log10(fC_count+1) )\n\nquantile( star_count  - fC_count)\n</code></pre>"},{"location":"days/design/","title":"RNAseq - technologies and design","text":"<p>Designing your experiment is the first step.  Design is crucial as it conditions the sort of questions that you can ask from your data, as well as the confidence you may have in the answers.</p> <p>Knowing about the sequencing technologies, their strengths and limitations, as well as the RNA-seq analysis pipeline, are the keys to designing a successful RNA-seq experiment.</p> <p>After having completed this chapter, you will be able to:</p> <ul> <li>describe different sequencing technologies and their application in RNA-seq.</li> <li>differentiate between technical and biological replicates.</li> <li>choose an appropriate sequencing depth and number of replicates depending on your scientific question.</li> </ul>"},{"location":"days/design/#material","title":"Material","text":"<p> Download the presentation</p>"},{"location":"days/enrichment/","title":"Enrichment analysis","text":"<p>Once the reads have been mapped and counted, one can assess the differential expression of genes between different conditions.</p> <p>During this lesson, you will learn to :</p> <ul> <li>perform downstream analysis on gene sets, such as annotation (e.g. GO terms or Reactome pathways) over-representation.</li> </ul>"},{"location":"days/enrichment/#material","title":"Material","text":"<p> Download the presentation</p> <p>Rstudio website</p> <p>clusterProfiler vignette/e-book</p>"},{"location":"days/enrichment/#downstream-analysis-over-representation-analysis","title":"Downstream analysis : over-representation analysis","text":"<p>Having lists of differentially-expressed genes is quite interesting in itself, however when there are many DE genes, it can be interesting to map these results  onto curated sets of genes associated with known biological functions.</p> <p>Here, we propose to use clusterProfiler, which regroups several enrichment detection algorithms onto several databases.</p> <p>We recommend you get inspiration from their very nice vignette/e-book to perform your own analyses.</p> <p>If you do not have a list of DE genes from your previous analysis, you may use the following table:</p> <p>  Ruhland2016 DESeq2 results</p> <p>The proposed correction will concern these.</p> Ruhland2016 analysis with clusterProfiler <p>We begin by reading the results of the DE analysis. Adapt this to your own analysis. Beware that edgeR and DESeq2 use different column names in their result tables (log2FoldChange/logFC , padj/FDR).</p> <pre><code>library(AnnotationHub)\nlibrary(AnnotationDbi)\nlibrary(clusterProfiler)\nlibrary(ReactomePA)\n\nlibrary(org.Mm.eg.db)\n\n\nres = read.csv( 'Ruhland2016.DESeq2.results.csv'  , row.names=1)\n#let's define significance as padj &lt;0.01 &amp; abs(lfc) &gt; 1\nres$sig = abs(res$log2FoldChange)&gt;1 &amp; res$padj&lt;0.01\n\ntable( res$sig )\n</code></pre> <p>Number of non-significant/significant genes </p> <pre><code> FALSE  TRUE \n 17370   391 \n</code></pre> <p>Translating gene ENSEMBL names to their entrezID (this is what clusterProfiler uses), as well as Symbol (named used by most biologist).</p> <pre><code>genes_universe &lt;- bitr(rownames(res), fromType = \"ENSEMBL\",\ntoType = c(\"ENTREZID\", \"SYMBOL\"),\nOrgDb = \"org.Mm.eg.db\")\n\nhead( genes_universe )\n\n#ENSEMBL ENTREZID  SYMBOL\n#2 ENSMUSG00000033845    27395  Mrpl15\n#4 ENSMUSG00000025903    18777  Lypla1\n#5 ENSMUSG00000033813    21399   Tcea1\n#7 ENSMUSG00000002459    58175   Rgs20\n#8 ENSMUSG00000033793   108664 Atp6v1h\n#9 ENSMUSG00000025907    12421  Rb1cc1\n\ndim(genes_universe)\n# 15443     3\n\nlength(rownames(res))\n# 18012\n</code></pre> <pre><code>genes_DE &lt;- bitr(rownames(res)[which( res$sig==T )], fromType = \"ENSEMBL\",\ntoType = c(\"ENTREZID\", \"SYMBOL\"),\nOrgDb = \"org.Mm.eg.db\")\ndim(genes_DE)\n# 382   3\n</code></pre> <p><pre><code># GO \"biological process (BP)\" enrichment\nego_bp &lt;- enrichGO(gene          = as.character(unique(genes_DE$ENTREZID)),\nuniverse      = as.character(unique(genes_universe$ENTREZID)),\nOrgDb         = org.Mm.eg.db,\nont           = \"BP\",\npAdjustMethod = \"BH\",\npvalueCutoff  = 0.01,\nqvalueCutoff  = 0.05,\nreadable      = TRUE)\n# couple of minutes to run\n\nhead(ego_bp)\ndotplot(ego_bp, showCategory = 20)\n# sample plot, but with adjusted p-value as x-axis\n#dotplot(ego_bp, x = \"p.adjust\", showCategory = 20)\n</code></pre> </p> <p><pre><code># Reactome pathways enrichment\nreactome.enrich &lt;- enrichPathway(gene=as.character(unique(genes_DE$ENTREZID)),\norganism = \"mouse\",\npAdjustMethod = \"BH\",\nqvalueCutoff = 0.01,\nreadable=T,\nuniverse = genes_universe$ENTREZID)\n# &lt;1 minute to run\n\n\ndotplot(reactome.enrich, x = \"p.adjust\")\n</code></pre> </p>"},{"location":"days/mapping/","title":"Reads mapping","text":"<p>Once you are happy with your read sequences in your FASTQ files, you can use a mapper software to align the reads to the genome and thereby find where they originated from.</p> <p>At the end of this lesson, you will be able to :</p> <ul> <li>identify the differences between a local aligner and a pseudo aligner.</li> <li>perform genome indexing appropriate to your data.</li> <li>map your RNA-seq data onto a genome.</li> </ul>"},{"location":"days/mapping/#material","title":"Material","text":"<p> Download the presentation</p> <p>STAR website</p>"},{"location":"days/mapping/#building-a-reference-genome-index","title":"Building a reference genome index","text":"<p>Before any mapping can be achieved, you must first index the genome want to map to. </p> <p>To do this with STAR, you need two files:</p> <ul> <li>a fasta file containing the sequences of the chromosome (or genome contigs)</li> <li>a gtf file containing annotations (ie. where the genes and exons are)</li> </ul> <p>We will be using the Ensembl references, with their accompanying GTF annotations.</p> <p>Note</p> <p>While the data are already on the server here, in practice or if you are following this course without a teacher, you can grab the reference genome data from the Ensembl ftp website.</p> <p>In particular, you will want a mouse DNA fasta file and gtf file.</p> <p>Take note of the genome sequence and annotation versions, you will need this in your paper\u2019s methods section!</p> <p>Task : Using STAR, build a genome index for the mouse mitochondrial chromosome.</p> <ul> <li>.fasta and .gtf files are in : <code>/data/DATA/Mouse_MT_genome/</code>.</li> <li>create the index in the folder <code>041_d_STAR_mouseMT_reference</code></li> <li>this job should require less than 4Gb and 10min to run. </li> </ul> <p>STAR basic parameter for genome index generation</p> <p>From the manual. Refer to it for more details</p> <ul> <li><code>--runMode genomeGenerate</code> : running STAR in index generation mode</li> <li><code>--genomeDir &lt;/path/to/genomeDir&gt;</code> : output folder for the index</li> <li><code>--genomeFastaFiles &lt;/path/to/genome/fasta1&gt;</code> : chromosome sequences fasta file (can be several files)</li> <li><code>--sjdbGTFfile &lt;/path/to/annotations.gtf&gt;</code> : annotation gtf file</li> <li><code>--runThreadN &lt;NumberOfThreads&gt;</code> : number of threads to run on </li> <li><code>--sjdbOverhang &lt;ReadLength-1&gt;</code> : length of the genomic sequence around the annotated junctions to be used in constructing the splice junctions database. Ideally : read length - 1.</li> </ul> <p>Additionally, because the genome is so small here (we only use the mitochondrial chromosome after all), you will need the following advanced option:</p> <ul> <li><code>--genomeSAindexNbases 5</code> : must be scaled to <code>min(14, log2(GenomeLength)/2 - 1)</code>, so 5 in our case</li> </ul> <p>Note</p> <p>While your indexing job is running, you can read ahead in STAR\u2019s manual to prepare the next step : mapping your reads onto the indexed reference genome.</p> STAR indexing script <pre><code>#!/usr/bin/bash\n# indexing the mouse mitochondrial genome\n\nG_FASTA=/data/DATA/Mouse_MT_genome/Mus_musculus.GRCm39.dna.chromosome.MT.fa\nG_GTF=/data/DATA/Mouse_MT_genome/Mus_musculus.GRCm39.MT.gtf\n\n\nmkdir -p 041_d_STAR_mouseMT_reference\n\nSTAR --runMode genomeGenerate \\\n--genomeDir 041_d_STAR_mouseMT_reference \\\n--genomeFastaFiles $G_FASTA \\\n--sjdbGTFfile $G_GTF \\\n--runThreadN 4 \\\n--genomeSAindexNbases 5 \\\n--sjdbOverhang 99 </code></pre> <p>It can be found on the server at <code>/data/Solutions/mouseMT/041_s_star_index.sh</code></p> <p>Extra task : Determine how you would add an additional feature to your reference, for example for a novel transcript not described by the standard reference.</p> Answer <p>You would edit the gtf file to add your additional feature(s), following the proper format.</p> <p>Note</p> <p>In case you\u2019ve got multiple FASTA files for your genome (eg, 1 per chromosome), you may just list them with the <code>genomeFastaFiles</code> option as follow:</p> <p><code>--genomeFastaFiles /path/to/genome/fasta1.fa /path/to/genome/fasta2.fa /path/to/genome/fasta3.fa ...</code></p>"},{"location":"days/mapping/#mapping-reads-onto-the-reference","title":"Mapping reads onto the reference","text":"<p>Task : Using STAR, align the raw FASTQ files of the mouseMT dataset against the mouse mitochondrial reference you just created.</p> <ul> <li>if were not able to complete the previous task, you can use the index in <code>/data/Solutions/mouseMT/041_d_STAR_mouseMT_reference</code> .</li> <li>search the STAR manual for the option to output a BAM file sorted by coordinate.</li> <li>search the STAR manual for the option to output a geneCounts file.</li> <li>put the results in folder <code>042_d_STAR_map_raw/</code> .</li> </ul> STAR basic parameters for mapping <p>Taken again from the manual:</p> <ul> <li><code>--genomeDir &lt;/path/to/genomeDir&gt;</code> : folder where you have put the genome index</li> <li><code>--readFilesIn &lt;/path/to/read1&gt;</code> : path to a fastq file. If the reads are paired, then also include the path to the second fastq file</li> <li><code>--runThreadN &lt;NumberOfThreads&gt;</code>: number of threads to run on.</li> <li><code>--outFileNamePrefix  &lt;prefix&gt;</code> : prefix of the output files, typically something like <code>output_directory/sampleName</code> . </li> </ul> <p>Note</p> <p>Take the time to read the parts of the STAR manual which concern you: a bit of planning ahead can save you a lot of time-consuming/headache-inducing trial-and-error on your script.</p> <p>Warning</p> <p>Mapping reads and generating a sorted BAM from one of the mouseMT FASTQ file will take less than a minute and very little RAM, but on a real dataset it should take from 15 minutes to an hour per sample and require at least 30GB of RAM.</p> STAR mapping script <p>We will be using a job array to map each file in different job that will run at the same time.</p> <p>First create a file named <code>sampleNames.txt</code>, containing the sample names:</p> <p><pre><code>sample_a1\nsample_a2\nsample_a3\nsample_a4\nsample_b1\nsample_b2\nsample_b3\nsample_b4\n</code></pre> it can also be found in the server at <code>/data/Solutions/mouseMT/sampleNames.txt</code></p> <p>Then for our script:</p> <p><pre><code>#!/usr/bin/bash\n# aligning mouseMT reads with STAR\n\n\nmkdir -p 042_d_STAR_map_raw\n\nfor SAMPLE in `cat sampleNames.txt`\ndo\nFASTQ_NAME=/data/DATA/mouseMT/${SAMPLE}.fastq\n\n STAR --runThreadN 4 --genomeDir 041_d_STAR_mouseMT_reference \\\n--outSAMtype BAM SortedByCoordinate \\\n--outFileNamePrefix  042_d_STAR_map_raw/${SAMPLE}. \\\n--quantMode GeneCounts \\\n--readFilesIn $FASTQ_NAME\ndone\n</code></pre> it can also be found in the server at <code>/data/Solutions/mouseMT/042_s_STAR_map_raw.sh</code></p> <p>and its results can be found at <code>/data/Solutions/mouseMT/042_d_STAR_map_raw/</code></p> <p>The options of STAR are :</p> <ul> <li><code>--runThreadN 4</code> : 4 threads to go faster.</li> <li><code>--genomeDir 041_STAR_reference</code> : path of the genome to map to.</li> <li><code>--outSAMtype BAM SortedByCoordinate</code> : output a coordinate-sorted BAM file.</li> <li><code>--outFileNamePrefix 042_STAR_map_raw/${SAMPLE}.</code> : prefix of output files.</li> <li><code>--quantMode GeneCounts</code> : will create a file with counts of reads per gene.</li> <li><code>--readFilesIn $FASTQ_NAME</code> : input read file.</li> </ul>"},{"location":"days/mapping/#qc-on-the-aligned-reads","title":"QC on the aligned reads","text":"<p>You can call MultiQC on the STAR output folder to gather a report on the individual alignments.</p> <p>Task : use <code>multiqc</code> to generate a QC report on the results of your mapping.</p> <ul> <li>Evaluate the alignment statistics. Do you consider this to be a good alignment?</li> <li>How many unmapped reads are there? Where might this come from, and how would you determine this?</li> <li>What could you say about library strandedness ? </li> </ul> script and answers <p><pre><code>#!/usr/bin/bash\n# multiqc on the mapping results\n\nmultiqc -n 043_r_multiqc_mouseMT_mapped_raw.html -f --title mapped_raw 042_d_STAR_map_raw/\n</code></pre> it can also be found in the server at <code>/data/Solutions/mouseMT/043_s_multiqc_map_raw.sh</code></p> <p> Download the report </p>"},{"location":"days/mapping/#comparison-of-mapping-the-trimmed-reads","title":"Comparison of mapping the trimmed reads","text":"<p>After having mapped the raw reads, we also map the trimmed reads and then compare the results to decide which one we want to use for the rest of our analysis.</p> <p>We will spare you the mapping of the trimmed reads, and let you directly download the mapping multiqc report:</p> <p> trimmed reads mapping  report </p> For the curious: scripts for the mapping of trimmed reads <p><pre><code>#!/usr/bin/bash\n# mapping trimmed mouseMT reads\n\n\n\nmkdir -p 044_d_STAR_map_trimmed\n\nfor SAMPLE in `cat sampleNames.txt`\ndo\n\nFASTQ_NAME=030_d_trim/${SAMPLE}.trimmed.fastq\n\n STAR --runThreadN 4 --genomeDir 041_d_STAR_mouseMT_reference \\\n--outSAMtype BAM SortedByCoordinate \\\n--outFileNamePrefix  044_d_STAR_map_trimmed/${SAMPLE}_trimmed. \\\n--quantMode GeneCounts \\\n--readFilesIn $FASTQ_NAME\ndone\n</code></pre> it can also be found in the server at <code>/data/Solutions/mouseMT/044_s_STAR_map_trimmed.sh</code></p> <p><pre><code>#!/usr/bin/bash\n# multiqc of the mapped trimmed read\n\nmultiqc -n 045_r_multiqc_mouseMT_mapped_trimmed.html -f --title mapped_trimmed 044_d_STAR_map_trimmed/\n</code></pre> it can also be found in the server at <code>/data/Solutions/mouseMT/045_s_multiqc_mouseMT_mapped_trimmed.sh</code></p>"},{"location":"days/mapping/#qc-report-of-mapping-for-the-liu2015-and-ruhland2016-dataset","title":"QC report of mapping for the Liu2015 and Ruhland2016 dataset","text":"<p>Liu2015</p> <p>Take the time to look at the following reports: </p> <p> Liu2015 raw reads mapping  report   Liu2015 trimmed reads mapping  report </p> <p>Which one would you choose? </p> <p>Ruhland</p> <p> Ruhland2016 raw reads mapping  report </p>"},{"location":"days/mapping/#additional-pseudo-aligning-with-salmon","title":"ADDITIONAL : pseudo-aligning with salmon","text":"<p>salmon website</p> <p>Salmon can allow you to quantify transcript expression without explicitly aligning the sequenced reads onto the reference genome with its gene and splice junction annotations, but instead to a simplification of the corresponding transcriptome, thus saving computational resources.</p> <p>We refer you to the tool\u2019s documentation in order to see how the reference index is computed.</p> <p>Task : run salmon to quantify the expression of either the Ruhland or Liu dataset. </p> <ul> <li>Use the tool documentation to craft your command line.</li> <li>precomputed indices can be found in <code>/data/DATA/Mouse_salmon_index</code> and <code>/data/DATA/Human_salmon_index</code>.</li> </ul> <p>Warning</p> <p>Please check with the teacher before you launch these tasks, because they require intensive resources (~6G of RAM and 40 minutes per fastq file).</p> script <p><pre><code>#!/usr/bin/bash\n# pseudo alignment of the Ruhland2016 reads with salmon\n\n\ndataDIR=/data/DATA/Ruhland2016\n\nsourceFILE=Ruhland2016.fastqFiles.txt\n\n\ngenomeDIR=/data/DATA/Mouse_salmon_index\n\nfor fastqFILE in `cat $sourceFILE`\ndo\noutDIR=033_d_salmon_Ruhland2016_${fastqFILE%.*}\n\n mkdir -p $outDIR\n\n salmon quant -i $genomeDIR -l A \\\n              -r $dataDIR/$fastqFILE \\\n              -p 4 --validateMappings --gcBias --seqBias \\\n              -o $outDIR\ndone\n</code></pre> it can also be found in the server at <code>/data/Solutions/Ruhland2016/033_s_salmon_Ruhland2016.sh</code></p>"},{"location":"days/quality_control/","title":"Quality control","text":"<p>Quality Control is the essential first step to perform once you receive your data from your sequencing facility, typically as <code>.fastq</code> or <code>.fastq.gz</code> files.</p> <p>During this session, you will learn to :</p> <ul> <li>create QC report for a single file with fastqc</li> <li>aggregate multiple QC reports using multiqc</li> <li>interpret the QC reports for an entire RNA-seq experiment</li> </ul> <p>Note</p> <p>Although we aim to present tools as stable as possible, software evolve and their precise interface can change with time. We strongly recommend you consult each command\u2019s help page or manual before launching them. To this end, we provide links to each tool\u2019s website. </p> <p>This can also be useful to you if you are following this course without access to a compute server and have to install these tools on your machine.</p>"},{"location":"days/quality_control/#material","title":"Material","text":"<p> Download the presentation</p> <p>FastQC website</p> <p>MultiQC website</p>"},{"location":"days/quality_control/#meet-the-datasets","title":"Meet the datasets","text":"<p>We will be working with three datasets. The first is a small toy dataset for experimentation, the other two correspond to actual data:</p> <ul> <li> <p>toy dataset: RNAseq of mice mitochondrial mRNA</p> <ul> <li>8 samples : 4 in group A and 4 in group B</li> <li>single-end, 100bp reads</li> <li>on the server: <code>/data/DATA/mouseMT/</code></li> <li> fastq-files</li> </ul> </li> <li> <p>Liu et al. (2015) \u201cRNA-Seq identifies novel myocardial gene expression signatures of heart failure\u201d Genomics 105(2):83-89 https://doi.org/10.1016/j.ygeno.2014.12.002</p> <ul> <li>Gene Expression Omnibus id: GSE57345</li> <li>Samples of Homo sapiens heart left ventricles : 3 with heart failure, 3 without</li> <li>paired-end, 100bp reads</li> <li>on the server: <code>/data/DATA/Liu2015/</code></li> </ul> </li> <li> <p>Ruhland et al. (2016) \u201cStromal senescence establishes an immunosuppressive microenvironment that drives tumorigenesis\u201d Nature Communications 7:11762 https://dx.doi.org/10.1038/ncomms11762</p> <ul> <li>Gene Expression Omnibus id: GSE78128</li> <li>Samples of Mus musculus skin fibroblasts : 3 non-senescent (EtOH), 3 senescent (TAM)</li> <li>single-end, 50bp reads</li> <li>on the server: <code>/data/DATA/Ruhland2016/</code></li> </ul> </li> </ul>"},{"location":"days/quality_control/#fastqc-a-report-for-a-single-fastq-file","title":"FastQC : a report for a single fastq file","text":"<p>FastQC is a nice tool to get a variety of QC measures from files such as <code>.fastq</code>, <code>.bam</code> or <code>.sam</code> files. </p> <p>Although it has many options, the default parameters are often enough for our purpose :</p> <pre><code>fastqc -o &lt;output_directory&gt; file1.fastq file2.fastq ... fileN.fastq\n</code></pre> <p>FastQC is reasonably intelligent, and will try to recognise the file format and uncompress it if necessary (so no need to decompress manually).</p> <p>So, let\u2019s apply fastQC to the toy dataset. </p> <p>Task: </p> <p>On the server, create a new folder <code>mouseMT</code> and enter it.</p> <pre><code>mkdir mouseMT\ncd mouseMT\n</code></pre> <p>Then, create here a new text file name <code>010_s_fastqc.sh</code> (with <code>nano</code>, or on your local computer ), and paste the following content in it:</p> <p>Note</p> <p>A note on our choice of naming conventions. Each step is numbered: 010 for fastqc, 020 for multiqc, 030 for trimming,\u2026</p> <p>Sub-steps are then denoted for example 040 is mapping, and so 041 for indexing, 042 for mapping, 043 for QC of the mapping,\u2026</p> <p>The number is followed by a caracter which depends on the type of file/folder:</p> <ul> <li><code>s</code> : script containing the bash commands</li> <li><code>l</code> : logs of the scripts, documenting the software messages</li> <li><code>d</code> : directory containing the results</li> <li><code>r</code> : result file (when they are note in a specific directory)</li> </ul> <pre><code>#!/usr/bin/bash\n# script to perform the QC of the mouseMT fastq files\n\n# creating the output folder\nmkdir -p 010_d_fastqc/\n\nfastqc -o 010_d_fastqc /data/DATA/mouseMT/*.fastq\n</code></pre> <p>Save it, and then submit it to the server:</p> <pre><code>sh 010_s_fastqc.sh &gt; fastqc_mouseMT.o &amp;\n</code></pre> <p>It should take around 15 to 30 seconds in total.</p> <p>Once it has run, look at the content of the job output file, <code>fastqc_mouseMT.o</code>.</p> <p>Check that all analysis were complete and that there were no error.</p> <p>You can also check that the <code>010_fastqc/</code> folder contains several html files:</p> <pre><code>ls 010_d_fastqc/\n</code></pre> <p>output:</p> <pre><code>sample_a1_fastqc.html  sample_a2_fastqc.html  sample_a3_fastqc.html  sample_a4_fastqc.html  sample_b1_fastqc.html  sample_b2_fastqc.html  sample_b3_fastqc.html  sample_b4_fastqc.html\nsample_a1_fastqc.zip   sample_a2_fastqc.zip   sample_a3_fastqc.zip   sample_a4_fastqc.zip   sample_b1_fastqc.zip   sample_b2_fastqc.zip   sample_b3_fastqc.zip   sample_b4_fastqc.zip\n</code></pre> <p>Unfortunately, we cannot consult the html files content directly on the server. </p> <p>We will look at one of these html report on the toy dataset, and one of the pre-computed report from one of our other datasets.</p> <ul> <li> <p>repatriate one of the html report of the mouseMT dataset to your local computer, as well as the one you can find in: <code>/data/Solutions/Liu2015/010_d_fastqc/SRR3180538_TAM1_1_fastqc.html</code> OR     Download them:</p> <ul> <li> a fastQC report from the Ruhland 2016 data</li> <li> a fastQC report from the Liu 2015 data</li> <li> a fastQC report from the mouseMT data</li> </ul> </li> <li> <p>Look at these QC reports in a web browser. What are your conclusions ? Would you want to perform some operations on the reads, such as low-quality bases trimming, removal of adapters ?</p> </li> </ul> <p>Reminder</p> <p>to get the data from the distant server to your machine, you can use the Rstudio file tab on the bottom right.</p> <p>Extra Task: </p> <ul> <li>Write one or more scripts in your home directory that run FastQC analysis on each FASTQ file from the Liu2015 and Ruhland216 datasets. These are accessible at : <code>/data/DATA/Liu2015/</code> and <code>/data/DATA/Ruhland2016/</code>. </li> </ul> <p>Warning</p> <p>Make sure your script writes the fastqc output to a folder within your own home directory.</p> <p>Important points</p> <ul> <li>FastQC RAM requirements : 1Gb is more than enough.</li> <li>FastQC time requirements : ~ 5min / read file.</li> <li>try to make sure FastQC outputs all reports in the same directory, this will save time for the next step ;-).</li> <li>there is no need to copy the read files to your home directory (in fact, it is good practice not to: it would create data redundancy, and we won\u2019t have enough space left on the disk anyway\u2026).</li> </ul> Liu2015 FastQC script <pre><code>#!/usr/bin/bash\n# bash script to perform QC on the Liu2015 fastq files\n\n# creating the output folder\nmkdir -p 010_d_fastqc/\n\nfastqc -o 010_d_fastqc /data/DATA/Liu2015/*.fastq.gz\n</code></pre> <p>on the server, this script is also in <code>/data/Solutions/Liu2015/010_s_fastqc.sh</code></p> <p>This script runs fastqc on each of the .fastq.gz files, sequentially. Note that alternatively, you could have one script per sample.</p> Ruhland2016 FastQC script <pre><code>#!/usr/bin/bash\n# bash script to perform QC on the Ruhland2016 fastq files\n\nmkdir -p 010_d_fastqc/\n\nfastqc -o 010_d_fastqc /data/DATA/Ruhland2016/*.fastq.gz\n</code></pre> <p>on the server, this script is also in <code>/data/Solutions/Ruhland2016/010_s_fastqc.sh</code></p> Interpretation of a report <p> Download an annotated report</p> <p>We also refer you to this nice interpertation guide</p> <p>Pre-computed reports can be found in :</p> <ul> <li><code>/data/Solutions/Ruhland2016/010_d_fastqc/</code></li> <li><code>/data/Solutions/Liu2015/010_d_fastqc/</code></li> <li><code>/data/Solutions/mouseMT/010_d_fastqc/</code></li> </ul>"},{"location":"days/quality_control/#multiqc-grouping-multiple-reports","title":"MultiQC : grouping multiple reports","text":"<p>In practice, you likely will have more than a couple of samples (maybe even more than 30 or 50\u2026) to handle: individually consulting and comparing the QC reports of each would be tedious.</p> <p>MultiQC is a tool that lets you combine multiple reports in a single, interactive document that let you explore your data easily.</p> <p>Here, we will be focusing on grouping FastQC reports, but MultiQC can also be applied to the output or logs of other bioinformatics tools, such as mappers, as we will see later.</p> <p>In its default usage, <code>multiqc</code> only needs to be provided a path where it will find all the individual reports, and it will scan them and write a report named <code>multiqc_report.html</code>.</p> <p>Although the default behaviour is quite appropriate, with a couple of options we get a slightly better control over the output:</p> <ul> <li><code>--interactive</code> : forces the plot to be interactive even when there is a lot of samples (this option can lead to larger html files).</li> <li><code>-n &lt;filename&gt;</code> : specify the name of the output file name.</li> </ul> <p>For instance, a possible command line could be : <pre><code>multiqc -n &lt;output_file.html&gt; --interactive &lt;fastqc reports folder&gt;/\n</code></pre></p> <p>There are many additional parameters which let you customize your report. Use <code>multiqc --help</code> or visit their documentation webpage to learn more.</p> <p>Task: </p> <ul> <li> <p>Write a script to run MultiQC for the toy dataset.</p> <p>To follow the naming convention we started with, you can use the following names:</p> <ul> <li>script: <code>020_s_multiqc.sh</code> </li> <li>output report: <code>020_r_multiqc_mouseMT.html</code></li> </ul> </li> <li> <p>Look at the generated html report. What are your conclusions ?</p> </li> <li> <p>Compare the report of the toy dataset with the following report for the Ruhland2016 and Liu2015 data:</p> <ul> <li> Download the report</li> <li> Download the report</li> </ul> </li> </ul> <p>Info</p> <ul> <li>MultiQC RAM requirements : 1Gb should be more than enough.</li> <li>MultiQC time requirements : ~ 1min / read file.</li> <li>Use <code>multiqc --help</code> to check the different options</li> </ul> mouseMT MultiQC script <p><pre><code>#!/usr/bin/bash\n# multiQC to group QC reports of the mouseMT fastq files\n\nmultiqc -n 020_r_multiqc_mouseMT.html -f --title raw_fastq 010_d_fastqc/\n</code></pre> On the server, this script is also in <code>/data/Solutions/mouseMT/020_s_multiqc.sh</code></p> <p> Download the results of this script</p> Interpretation of the report for the mouseMT data. <p></p> <p>The PHRED quality of reads drop below 30 around base 75. All samples seem affected. One sample seems to stand out a bit</p> <p></p> <p>Mean quality scores are on average fairly high.</p> <p></p> <p>Most samples do not deviate too much, Except for two samples which clearly contains more GC% rich content compared to the other samples. This may be indicative of contamination or just a difference in composition.</p> <p></p> <p>Here we see that some sequences are duplicated, but not many. Normally in the context of RNA-seq, some transcripts are present in a large number of copies in the samples, so we would expect to see more over-represented sequences.  This is not the case here because this is a toy dataset with a very small number of reads.</p> <p></p> <p>We see a clear trend of adapter contamination for one sample as we get closer to the reads\u2019 end. Note the y-scale though : we never go above a 6% content per sample.</p> <p>Overall, we can conclude that one sample in particular stands out. </p> <p>We should note its name and monitor it closely as we go through the rest of our analysis pipeline.</p> <p>At the moment, the analysis steps are independent from sample to sample, so keeping that potential outlier is not a problem. However, when we come to differential analysis we will have to decide if we keep this sample or exclude it.</p> Interpretation of the report for the Liu2015 data. <p>We will interpret the report for the Liu2015 data.</p> <p> Download the report</p> <p></p> <p>The PHRED quality of reads drop below 30 around base 75. All samples seem affected. One sample seems to have some quality drops at specific timepoints/positions.</p> <p></p> <p>Mean quality scores are on average fairly high, but some reads exhibit low values.</p> <p></p> <p>Most samples do not deviate too much from the expected curve. The two samples colored in orange and red have a mode for a very specific value. This may be indicative of contamination, retaining specific rRNA, or adapter sequence content.</p> <p></p> <p>Ns are present at specific positions in specific samples, in particular for one sample. This is reminiscent of the PHRED quality curves at the top of the report. It seems some flowcells had a problem at specific time-point/positions.</p> <p></p> <p>This is colored red because this would be a problem if the data was coming from genomic DNA sequencing. However here we are in the context of RNA-seq : some transcripts are present in a large number of copies in the samples, and consequently it is expected that some sequences are over-represented.</p> <p></p> <p>We see a clear trend of adapter contamination as we get closer to the reads\u2019 end. Note the y-scale though : we never go above a 6% content per sample.</p> <p>Overall, we can conclude that these samples all suffer from some adapter content and a lower quality toward the reads\u2019 second half. Furthermore, a few samples have a peculiar N pattern between bases 20 and 30.</p> <p>It is then strongly advised to either :</p> <ul> <li>perform some trimming : remove adapter sequences + cut reads when average quality becomes too low</li> <li>use a mapper that takes base quality in account AND is able to ignore adapter sequence (and even then, you could try mapping on both trimmed and untrimmed data to see which is the best)</li> </ul> <p>extra Task</p> <p>Write and execute scripts to run a MultiQC for the Liu2015 and the Ruhland2016 datasets.</p> MultiQC script for Ruhland2016 <p>This script fetches the report from the <code>Solutions/</code> folder.</p> <p>You may adapt it to point to your own results if you want.</p> <pre><code>#!/usr/bin/bash\n# multiQC  of Ruhland2016 fastq files\n\nmultiqc -n 020_r_multiqc_Ruhland2016.html -f --title raw_fastq /data/Solutions/Ruhland2016/010_d_fastqc/\n</code></pre> <p>On the server, this script is also in <code>/data/Solutions/Ruhland2016/020_s_multiqc.sh</code></p> MultiQC script for Liu2015 <p>This script fetches the report from the <code>Solutions/</code> folder.</p> <p>You may adapt it to point to your own results if you want.</p> <pre><code>#!/usr/bin/bash\n# multiQC  of Liu2015 fastq files\n\nmultiqc -n 020_r_multiqc_Liu2015.html -f --title raw_fastq /data/Solutions/Liu2015/010_d_fastqc/\n</code></pre> <p>On the server, this script is also in <code>/data/Solutions/Liu2015/020_s_multiqc.sh</code></p>"},{"location":"days/quality_control/#extra-retrieving-published-datasets","title":"EXTRA : retrieving published datasets","text":"<p>Note</p> <p>If you are following this course with a teacher, then the data is already on the server. There is no need to download it again.</p> <p>Most NGS data is deposited at the Short Read Archive (SRA) hosted by the NCBI, with links from the Gene Expression Omnibus (GEO)</p> <p>Several steps are required to retrieve data from a published study :</p> <ol> <li>find GEO or SRA identifier from publication(s).</li> <li>find the \u201crun\u201d identifiers for each sample (SRR).</li> <li>use SRA Toolkit to dump data from the SRR repository to FASTQ files.</li> </ol> <p>For example, on the Liu2015 dataset :</p> <ol> <li>Locate the GEO accession in their publication: GSE57345 </li> <li>Use the NCBI search engine to find this accession : GSE57345</li> <li>This project is made of several sub-projects. Scroll down, and in the table find the Bioproject id : PRJNA246308 </li> <li>Go to the SRA run selector, enter the Bioproject id</li> <li>From the results of your search, select all relevant runs</li> <li>Click on \u201cAccession List\u201d in the Select table </li> </ol> <p></p> <ol> <li>use <code>fastq-dump</code> (part of the SRA Toolkit) on the downloaded accession list. For example, with a very small dataset (45Mb only, but already, this can take about a minute):</li> </ol> <p><code>fastq-dump --gzip --skip-technical --readids --split-files --clip SRR306383</code></p> <p>Note</p> <ul> <li><code>fastq-dump</code> takes a very long time</li> <li>You\u2019ll need to know the nature of the dataset (library type, paired vs single end, etc.) before analysing it.</li> <li>More information about fastq-dump</li> </ul>"},{"location":"days/server_login/","title":"Server login + unix fresh up","text":"<p>To conduct the practicals of this course, we will be using a dedicated distant server with enough computing capabilities to handle RNAseq data amd where all the necessary software has been installed.</p> <p>This matches the reality of most NGS workflows, which cannot be completed in a reasonable time on a single laptop machine. </p> <p>To interact with this server, you will have to connect and log in to it. We will do so through an Rstudio server, which, conveniently,  let\u2019s us start a UNIX terminal to launch bioinformatics tools, retrieve and send files to the server, and use R scripts to analyse our results.</p> <p>This page will cover our first contact with the distant server. </p> <p>You will learn to :</p> <ul> <li>connect to the server.</li> <li>use the command line to perform basic operations.</li> <li>exchange files between the server and your own machine.</li> </ul> <p>Note</p> <p>If you are doing this course on your own, then the distant server provided within the course will not be available.  Feel free to ignore or adapt any of the following steps to your own situation.</p> <p>Warning</p> <p>Everyone is connected to the same server. There are a few guardrails in place that should prevent any one user from crashing the server or hogging all the resources, but no system is fully foolproof. Consequently, before you launch a serious job, always take the time to test it on a smaller dataset in order to estimate if the big job might be too big.</p>"},{"location":"days/server_login/#connect-to-the-server","title":"Connect to the server","text":"<p>Follow the teacher\u2019s instructions to get the rstudio server address, as well as you login and password</p>"},{"location":"days/server_login/#start-a-terminal-on-the-server","title":"Start a terminal on the server","text":""},{"location":"days/server_login/#using-command-line-on-the-cluster","title":"Using command line on the cluster","text":"<p>Now that you are in the head node, it is time to get acquainted with your environment and to prepare the upcoming practicals.  We will also use this as a short reminder about the UNIX command line.</p> <p>Commands are issued using a shell command language. The one on our server is called bash. You can refer to this nice Linux Command Line Cheat Sheet (page 1 in particular) for reviewing common commands.</p> <p>At any time, you can get the file system location (folder/directory) your terminal is currently in, by typing the \u201cprint working directory\u201d command:</p> <pre><code>pwd\n</code></pre> <p>When you start a session on a distant computer, you are placed in your <code>home</code> directory. So the cluster should return something like:</p> <pre><code>/home/&lt;login&gt;\n</code></pre> <p>From then, we are going to do a little step-by-step practical to go through some of bash\u2019s most useful commands for this course.</p>"},{"location":"days/server_login/#creating-a-directory","title":"Creating a directory","text":"<p>practical</p> <p>Use the command line to create a repository called <code>mouseMT</code> where you will put all materials relating to the analysis of the mouseMT dataset.</p> Answer <pre><code>mkdir mouseMT\n</code></pre> <p>practical</p> <p>Move your terminal\u2019s connection to that directory.</p> Answer <pre><code>cd mouseMT\n</code></pre> <p>The directory <code>/data/</code> contains data and solutions for most practicals. </p> <p>practical</p> <p>List the content of the <code>/data/</code> directory.</p> Answer <pre><code>ls /data/\n</code></pre> <p>Note</p> <p>You don\u2019t need to move to that directory to list its contents!</p> <p>practical</p> <p>Copy the script <code>010_s_fastqc.sh</code> from  <code>/data/Solutions/mouseMT</code> into your current directory, and then print the content of this script to the screen.</p> Answer <p><pre><code>cp /data/Solutions/mouseMT/010_s_fastqc.sh .\nmore 010_s_fastqc.sh\n</code></pre> output: <pre><code>#!/usr/bin/bash\n\n# creating the output folder\nmkdir -p 010_d_fastqc/\n\nfastqc -o 010_d_fastqc /data/DATA/mouseMT/*.fastq\n</code></pre></p> <p>We\u2019ll see what all this means soon.</p>"},{"location":"days/server_login/#creating-and-editing-a-file","title":"Creating and editing a file","text":"<p>To edit files on the distant server, we will use the Rstudio file editor. It is primarily designed for R script but it works very well for  other types of files. </p> <p>Note</p> <p>Alternatively, we could use command line editor <code>nano</code>. It is far from the most complete or efficient one, but it can be found on most servers, and is arguably among the easiest command-line file editor to start with.</p> <p>To start editing a file named <code>test.txt</code>, in Rstudio go to <code>File &gt; New File &gt; Text File</code>, or use the shortcut <code>Ctrl+Alt+Shift+N</code></p> <p></p> <p>Type in your favorite movie quote, and then save it with <code>Ctrl+s</code>, give it an appropriate name, like <code>test.txt</code> for example</p> <p>You can check that your modifications were saved from the terminal by typing</p> <pre><code>more test.txt\n</code></pre> <p>Note</p> <p>for the command above to work, your terminal needs to be in the same folder as the file. Take careful note of where you save your files, and where your terminal is running.</p>"},{"location":"days/server_login/#exchanging-files-with-the-server","title":"Exchanging files with the server","text":"<p>Whether you want to transfer some data to the server or retrieve the results of your latest computation, it is important to be able to exchange files with the distant server.</p> <p>Fortunately, Rstudio provides an easy way to do this.</p> <p></p> <p>Note</p> <p>Alternatively, there exists several alternatives, from using the command line tool <code>scp</code> to graphical tools such as fileZilla.</p> <p>practical</p> <p>Retrieve the file <code>test.txt</code>, which you created in the previous practicals, from the distant server to your local machine. </p> <p>practical</p> <p>Create a text file on your local computer (using wordpad on windows, Text Edit on Mac, or gedit on linux). Save that file, and then send it to the distant server.</p> <p>Warning</p> <p>For Windows users, if you edit files on your computer before sending them to the server, you will likely see strange character at the end of the file lines.</p> <p>This is because Windows ends line with \u201c\\r\\n\u201d while Unix uses just \u201c\\n\u201d. This can be solved using the <code>dos2unix</code> command line tool:</p> <p><code>dos2unix &lt;filename&gt;</code></p>"},{"location":"days/server_login/#bash-scripts","title":"bash scripts","text":"<p>So far we have been executing bash commands in the interactive shell. This is the most common way of dealing with our data on the server for simple operations.</p> <p>However, when you have to do some more complex tasks, such as what we will be doing with our RNA-seq data, you will want to use scripts. These are just normal text files which contain bash commands.</p> <p>Scripts :</p> <ul> <li>keep a written trace of your analysis, so they enhance its reproducibility. </li> <li>make it easier to correct something in an analysis (you don\u2019t have to retype the whole command, just change the part that is wrong).</li> <li>often necessary when we want to submit big computing jobs to the cluster.</li> </ul> <p>Create a new text file named <code>myScript.sh</code> on the server (you can use nano or create it on your local machine and later transfer it to the server). </p> <p>Then, type this into the file:</p> <pre><code>#!/usr/bin/bash    \n\n## this is a comment, here to document this script\n## whatever is after # is not interpreted as code.\n\n# the echo command prints whatever text follows to the screen: \necho \"looking at the size of the elements of /data/\"\n\nsleep 15 # making the script wait for 15 seconds - this is just so we can see it later on. \n\n# du : \"disk usage\", a command that returns the size of a folder structure.\ndu -h -d 2 /data/\n</code></pre> <p>The first line is not 100% necessary at this step, but it will be in the next part, so we might as well put it in now. It helps some software know that this file contains bash code. </p> <p>Then to execute the script, navigate in a terminal open on the server to the place where the script is, and execute the following command:</p> <pre><code>sh myScript.sh\n</code></pre> <p>Warning</p> <p>Be sure to execute the script from the folder that it is in. Otherwise you would have to specify in which folder to find the script using its path.</p> <p>This should have printed some information about the size of <code>/data/</code> subfolders to the screen.</p>"},{"location":"days/server_login/#activate-the-conda-environment-containing-bioinformatics-software","title":"Activate the conda environment containing bioinformatics software","text":"<p>As it stands, the terminal you are currently using does not have access to bioinformatics software:</p> <pre><code>fastqc --help\n</code></pre> <p>results in the error : <code>Command 'fastqc' not found, ...</code></p> <p>Indeed, bioinformatics tools are not installed by default on the server.</p> <p>Here, we have prepared the bioinformatics software in a conda environment (conda environments provide a useful way to manage software stacks across plateform).</p> <p>So, first we need to initialize conda:</p> <p><pre><code>conda init\n</code></pre> Then, for the changes to take place we need to start a new terminal :</p> <p></p> <p>Once this is done, you can notice that now you terminal line starts with <code>(base)</code>. This is a sign that conda is activated and that you are currently using its default environment.</p> <p>So finally we can activate the environment containing tools, which is called <code>ngs-tools</code>: <pre><code>conda activate ngs-tools\n</code></pre></p> <p>Now when you run the following command you should see the fastqc help: <pre><code>fastqc --help\n</code></pre></p>"},{"location":"days/server_login/#annex-hpc-clusters","title":"ANNEX : HPC clusters","text":"<p>The following annex discuss HPC clusters, where you do not execute your compute-intensive job directly on the server you connect to,  but rather to submit jobs to a scheduler which then allocate them compute resources on dedicated compute nodes.</p> <p>Having to use such a cluster is quite common in bioinformatics given the size of most modern datasets.</p> <p>Note</p> <p>in this annex we only discuss the SLURM scheduler. Other schedulers use different command lines and a different linguo (although many concepts are similar).</p>"},{"location":"days/server_login/#submitting-jobs","title":"Submitting jobs","text":""},{"location":"days/server_login/#submitting-a-simple-script","title":"Submitting a simple script","text":"<p>Jobs can be submitted to the compute cluster using bash scripts, with an optional little preamble which tells the cluster about your computing resource requirements, and a few additional options.</p> <p>Each time a user submits a job to the cluster, SLURM checks how much resources they asked for, with respect to the amount available right now in the cluster and how much resources are allowed for that user. </p> <p>If there are enough resources available, then it will launch the job on one or several of its worker nodes. If not, then it will wait for the required resources to become available, and then launch the job.</p> <p>To start with, you can have a look at what is happening right now in the cluster with the command:</p> <pre><code>squeue\n</code></pre> <p>On the small course server, there should not be much (if anything), but on a normal cluster you would see something like:</p> <pre><code>JOBID     PARTITION           NAME                USER      STATE     TIME        TIME_LIMIT  QOS       NODELIST(REASON)\n48175517  a100                cryocare            user1     PENDING   0:00        6:00:00     gpu6hours (None)                                  48168955  a100                ecoli_pool2         user2     RUNNING   4:09:18     6:00:00     6hours    sgi77                                   47806032  a100                GAN_final           user3     RUNNING   4-04:10:20  13-20:00:00 projects  sgi64                                   47975434  a100                GAN_256_8192        user3     RUNNING   1-18:21:39  7-00:00:00  1week     sgi71                                   48174629  a100                x-35780_y20062      user4     RUNNING   13:36       7:00:00     gpu1day   sgi74                                   </code></pre> <p>The columns correspond to :</p> <ul> <li>JOBID : the job id, which is the number that SLURM uses to identify any job</li> <li>PARTITION : which part of the cluster is that job executing at</li> <li>NAME : the name of the job</li> <li>USER : which user submitted the job</li> <li>STATE : whether the job is currently <code>RUNNING</code>, <code>PENDING</code>, <code>COMPLETED</code>, <code>FAILED</code></li> <li>TIME : how long has this job been running for</li> <li>TIME_LIMIT : how long will the job be allowed to run</li> <li>QOS : which queue of the cluster does the job belong to. Queues are a way to organize jobs in different categories of resource usage.</li> <li>NODELIST(REASON) : which worker node(s) is the job running on.</li> </ul> <p>You can look up more info on squeue documentation.</p> <p>Now, you will you will want to execute your <code>myScript.sh</code> script as a job on the cluster.</p> <p>This can be done with the <code>sbatch</code> command.</p> <p>The script can stay the same (for now), but there is an important aspect of <code>sbatch</code> we want to handle: the script will not be executing in our terminal directly, but on a worker node.</p> <p>That means that there is no screen to print to. So, in order to still be able to view the output of the script, SLURM will write the printed output into a file which we will be able to read when the job is finished (with <code>more</code> or <code>less</code>). By default, SLURM will name this file something like <code>slurm-&lt;jobid&gt;.out</code>, which is not very informative to us; so, instead of keeping the default, we will give our own output file name to <code>sbatch</code> with the option <code>-o</code>. For example, I will name it <code>myOutput.o</code>.</p> <p>In the terminal, navigate to where you have you <code>myScript.sh</code> file on the distant server and type</p> <pre><code>sbatch -o myOutput.o  myScript.sh\n</code></pre> <p>You should see an output like:</p> <pre><code>sbatch: Submitted batch job 41\n</code></pre> <p>Letting you know that your job has the jobid 41 (surely yours will be different).</p> <p>Directly after this, quickly type:</p> <pre><code>squeue\n</code></pre> <p>If you were fast enough, then you will see your script <code>PENDING</code> or <code>RUNNING</code>.</p> <p>If not, then that means that your script has finished running. You do not know yet if it succeeded or failed. To check this, you need to have a look at the output file, <code>myOutput.o</code> in our case.</p> <p>If everything worked, you will see the normal output of your script.  Otherwis, you will see some error messages.</p>"},{"location":"days/server_login/#specifying-resources-needed-to-slurm","title":"Specifying resources needed to SLURM","text":"<p>When submitting the previous job, we did not specify our resource requirements to SLURM, which means that SLURM assigned it the default:</p> <ul> <li>1 hour</li> <li>1 CPU</li> <li>1 GB of RAM</li> </ul> <p>Often, we will want something different from that, and so we will use options in order to specify what we need.</p> <p>For example:</p> <ul> <li><code>--time=00:30:00</code> : time reserved for the job : 30min. </li> <li><code>--mem=2G</code> : memory for the job: 2GB</li> <li><code>--cpus-per-task=4</code> : 4 CPUs for the job </li> </ul> <p>Your <code>sbatch</code> command line will quickly grow to be long and unwieldy. It can also be difficult to remember exactly how much RAM and time we need for each script.</p> <p>To address this, SLURM provides a fairly simple way to add this information to the scripts themselves, by adding lines starting with <code>#SBATCH</code> after the first line.</p> <p>For our example, it could look like this:</p> <pre><code>#!/usr/bin/bash\n#SBATCH --job-name=test\n#SBATCH --time=00:30:00\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=2G\n#SBATCH -o test_log.o\n\n\necho \"looking at the size of the elements of /data/\"\nsleep 15 # making the script wait for 15 seconds - this is just so we can see it later on. \n# `du` is \"disk usage\", a command that returns the size of a folder structure.\ndu -h -d 2 /data/\n</code></pre> <p>I also added the following options :</p> <ul> <li><code>#SBATCH --job-name=test</code> : the job name</li> <li><code>#SBATCH -o test_log.o</code> : file to write output or error messages</li> </ul> <p>We would then submit the script with a simple <code>sbatch myScript.sh</code> without additional options.</p> <p>Example</p> <p>Create a new file named <code>mySbatchScript.sh</code>, copy the code above into it, save, then submit this file to the job scheduler using the following command :</p> <pre><code>    sbatch mySbatchScript.sh\n</code></pre> <p>Use the command <code>squeue</code> to monitor the jobs submitted to the cluster. </p> <p>Check the output of your job in the output file <code>test_log.o</code>.</p> <p>Note</p> <p>When there are a lot of jobs, <code>squeue -u &lt;username&gt;</code> will limit the list to those of the specified user.</p>"},{"location":"days/server_login/#advanced-cluster-usage-loading-modules","title":"Advanced cluster usage : loading modules","text":"<p>During our various analyses, we will call upon numerous software.</p> <p>Fortunately, in most cases we do not have to install each of these ourselves onto the cluster : they have already been packaged, prepared and made available to you or your code. </p> <p>However, by default, these are not loaded, and you have to explicitly load the module containing the software you want in your script (or in the interactive shell session).</p> <p>Question: Why aren\u2019t all the modules already pre-loaded ?</p> Answer <p>Many toolsets have dependencies toward different, sometimes incompatible libraries. Packaging each tool independently and loading them separately circumvents this, as you only load what you need, and you can always unload a toolset if you need to load another, incompatible, toolset.</p> <p>Modules are managed with the <code>module</code> command.</p> <p>Basic commands are :</p> <ul> <li><code>module list</code> : lists currently loaded modules</li> <li><code>module load &lt;modulename&gt;</code> alias <code>ml &lt;modulename&gt;</code> : loads module <code>&lt;modulename&gt;</code></li> <li><code>module unload &lt;modulename&gt;</code>  : unloads module <code>&lt;modulename&gt;</code></li> <li><code>module purge</code> : unloads all loaded modules</li> <li><code>module avail</code> : lists all modules available for loading</li> <li><code>module keyword &lt;KW&gt;</code> : lists all modules available for loading which contains <code>&lt;KW&gt;</code></li> </ul> <p>Try it for yourself: soon, we will need the fastqc software. </p> <p>If we type in the terminal: <pre><code>fastqc --help\n</code></pre> this should give you an error, telling you there is no such command.</p> <p>We will try to find a module containing our desired software using <code>module keyword</code></p> <pre><code>module keyword fastqc\n</code></pre> <p>which will output: <pre><code>-------------------------------------------------------------------------------------------------------------------------------------\n\nThe following modules match your search criteria: \"fastqc\"\n-------------------------------------------------------------------------------------------------------------------------------------\n\n\n  fastqc: fastqc/0.11.5, fastqc/0.11.8, fastqc/0.11.9 \n    FastQC is a quality control application for high throughput sequence data. It reads in sequence data in a variety of formats and\n    can either provide an interactive application to review the results of several different QC checks, or create an HTML based\n    report which can be integrated into a pipeline. - Homepage: http://www.bioinformatics.babraham.ac.uk/projects/fastqc/\n\n-------------------------------------------------------------------------------------------------------------------------------------\n\nTo learn more about a package execute:\n\n   $ module spider Foo\n\nwhere \"Foo\" is the name of a module.\n\nTo find detailed information about a particular package you\nmust specify the version if there is more than one version:\n\n   $ module spider Foo/11.1\n\n-------------------------------------------------------------------------------------------------------------------------------------\n</code></pre></p> <p>This tells us that a module name <code>fastqc</code> exists. It has different versions available (0.11.5,0.11.8,0.11.9).  The default is the rightmost one, which works well for us.</p> <p>So, if we load this module before executing fastqc: <pre><code>ml fastqc # shortcut for \"module load fastqc\"\nfastqc --help\n</code></pre> Now you should not have any error, and you should see the help test of <code>fastqc</code></p> <p>Note</p> <p>Our module provider is ComputeCanada, which has a lot of available software. To avoid storing all these on our cluster, each time a new module is loaded, it is fetched first from the Compute Canada servers, so sometimes it can take a bit of time to load a module for the first time.</p>"},{"location":"days/server_login/#advanced-cluster-usage-job-array","title":"Advanced cluster usage : job array","text":"<p>Often, we have to repeat a similar analysis on a number of files, or for a number of different parameters. Rather than writing each sbatch script individually, we can rely on job arrays to facilitate our task.</p> <p>The idea is to have a single script which will execute itself several times. Each of these executions is called a task, and they are all the same, save for one variable which whose value changes from 1 to the number of tasks in the array.</p> <p>We typically use this variable, named <code>$SLURM_ARRAY_TASK_ID</code> to fetch different lines of a file containing information on the different tasks we want to run (in general, different input file names).</p> <p>Note</p> <p>In bash, we use variables to store information, such as a file name or parameter value.</p> <p>Variables can be created with a statement such as:</p> <p><code>myVar=10</code></p> <p>where variable <code>myVar</code> now stores the value 10.</p> <p>The variable content can then be accessed with:</p> <p><code>${myVar}</code></p> <p>You do not really need more to understand what follows, but if you are curious, you can consult this small tutorial.</p> <p>Say you want to execute a command, on 10 files (for example, map the reads of 10 samples).</p> <p>You first create a file containing the name of your files (one per line); let\u2019s call it <code>readFiles.txt</code>.</p> <p>Then, you write an sbatch array job script:</p> <pre><code>#!/usr/bin/bash\n#SBATCH --job-name=test_array\n#SBATCH --time=00:30:00\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=1G\n#SBATCH -o test_array_log.%a.o\n#SBATCH --array 1-10%5\n\n\necho \"job array id\" $SLURM_ARRAY_TASK_ID\n\n# sed -n &lt;X&gt;p &lt;file&gt; : retrieve line &lt;X&gt; of file\n# so the next line grabs the file name corresponding to our job array task id and stores it in the variable ReadFileName \nReadFileName=`sed -n ${SLURM_ARRAY_TASK_ID}p readFiles.txt`\n\n# here we would put the mapping command or whatever\necho $ReadFileName\n</code></pre> <p>Some things have changed compared to the previous sbatch script :</p> <ul> <li><code>#SBATCH --array 1-10%5</code> : will spawn independent tasks with IDs from 1 to 10, and will manage them so that at most 5 run at the same time.</li> <li><code>#SBATCH -o test_array_log.%a.o</code> : the <code>%a</code> will take the value of the array task ID. So we will have 1 log file per task (so 10 files).</li> <li><code>$SLURM_ARRAY_TASK_ID</code> : changes value between the different tasks. This is what we use to execute the same script on different files (using <code>sed -n ${SLURM_ARRAY_TASK_ID}p</code>)</li> </ul> <p>Furthermore, this script uses the concept of bash variable.</p> <p>Many things could be said on that, but I will keep it simple with this little demo code:</p> <pre><code>foo=123                # Initialize variable foo with 123\n#   !warning! it will not work if you put spaces in there\n\n# we can then access this variable content by putting a $ sign in front of it:\n\necho $foo              # Print variable foo, sensitive to special characters\necho ${foo}            # Another way to print variable foo, not sensitive to special characters\n\nOUTPUT=`wc -l du -h -d 2 /data/` # puts the result of the command \n#   between `` in variable OUTPUT\n\necho $OUTPUT           # print variable output\n</code></pre> <p>So now, this should help you understand the trick we use in the array script:</p> <pre><code>ReadFileName=`sed -n ${SLURM_ARRAY_TASK_ID}p readFiles.txt`\n</code></pre> <p>Where, for example for task 3, <code>${SLURM_ARRAY_TASK_ID}</code> is equal to 3.</p> <p>We feed this to <code>sed</code>, so that it grabs the 3rd line of <code>readFiles.txt</code>, and we put that value into the <code>ReadFileName</code>.</p>"},{"location":"days/trimming/","title":"Sequence trimming","text":"<p>Following a QC analysis on sequencing results, one may detect stretches of low quality bases along reads, or a contamination by adapter sequence. Depending on your research question and the software you use for mapping, you may have to remove these bad quality / spurious sequences out of your data.</p> <p>During this block, you will learn to :</p> <ul> <li>trim your data with trimmomatic</li> </ul>"},{"location":"days/trimming/#material","title":"Material","text":"<p> Download the presentation</p> <p>Trimmomatic website</p>"},{"location":"days/trimming/#to-trim-or-not-to-trim","title":"to trim or not to trim ?","text":"<p>There are several ways to deal with poor quality bases or adapter contamination in reads, and several terms are used in the field, sometimes very loosely. We can talk about:</p> <ul> <li>Trimming: to remove a part of, or the entirety of, a read (for quality reasons).<ul> <li>Hard trimming: trim with a high threshold (eg. remove everything with QUAL&lt;30).</li> <li>Soft trimming: trim with a low threshold (eg. remove everything with QUAL&lt;10).</li> </ul> </li> <li>Clipping: to remove the end part of a read (typically because of adapter content).<ul> <li>Hard clipping: actually removing the end of the read from the file (ie. with trimmomatic).</li> <li>Soft clipping: ignoring the end of the read at mapping time (ie. what STAR does).</li> </ul> </li> </ul> <p>If the data will be used to perform transcriptome assembly, or variant analysis, then it MUST be trimmed.</p> <p>In contrast, for applications based on counting reads, such as Differential Expression analysis, most aligners, such as STAR, HISAT2, salmon, and kallisto, can handle bad quality sequences and adapter content by soft-clipping, and consequently they usually do not need trimming. In fact, trimming can be detrimental to the number of successfully quantified reads [William et al. 2016].</p> <p>Nevertheless, it is usually recommended to perform some amount of soft trimming (eg. kallisto, salmon ).</p> <p>If possible, we recommend to perform the mapping for both the raw data and the trimmed one, in order to compare the results for both, and choose the best.</p> <p>Question: what could be a good metric to choose the best between the trimmed and untrimmed?</p> Answer <p>The number of uniquely mapped reads is generally what would matter in differential expression analysis. Of course, this means that you can only choose after you have mapped both the trimmed and the untrimmed reads.</p>"},{"location":"days/trimming/#trimming-with-trimmomatic","title":"trimming with Trimmomatic","text":"<p>The trimmomatic website gives very good examples of their software usage for both paired-end (<code>PE</code>) and single-end (<code>SE</code>) reads. We recommend you read their quick-start section attentively.</p> <p>Task 1: </p> <ul> <li> <p>Conduct a soft trimming on the mouseMT data</p> <ul> <li>name the output folder : <code>030_d_trim/</code>.</li> <li>Adapter sequences can be found in <code>/data/DATA/adapters/TruSeq3-PE.fa</code> (adapter sequence source).</li> <li>unlike fastqc, you will have to launch trimmomatic for each sample separately</li> <li>to facilitate QC afterward, add the following at the end of your trimmomatic command (substituting <code>&lt;sample name&gt;</code>):             <code>2&gt; 030_d_trim/trim_out.&lt;sample name&gt;.log</code>             This will send part of the output of trimmomatic to a file in the same folder as the trimmed reads, which multiQC will be able to use afterward.</li> </ul> </li> </ul> <p>Note</p> <p>On real data Trimmomatic requires ~0.5G / CPU of RAM, and ~10 min/read file.</p> <p>Warning</p> <p>trimmomatic is a Java-based program, and thus must be run by passing its .jar file to the Java interpreter:</p> <pre><code>trimmomatic\n</code></pre> trimmomatics script <p>We chose the following option:</p> <ul> <li>SLIDINGWINDOW:3:25 Perform a sliding window trimming, cutting once the average quality within the window falls below a threshold.</li> <li>3  : windowSize: specifies the number of bases to average across</li> <li>25 : requiredQuality: specifies the average quality required.</li> <li>ILLUMINACLIP:/data/DATA/adapters/TruSeq3-PE.fa:2:30:10 Cut adapter and other Illumina-specific sequences from the read.</li> <li>Cut adapter and other illumina-specific sequences from the read.</li> <li>2  : seedMismatches: specifies the maximum mismatch count which will still allow a full match to be performed</li> <li>30 : palindromeClipThreshold: specifies how accurate the match between the two \u2018adapter ligated\u2019 reads must be for PE palindrome read alignment.</li> <li>10 : simpleClipThreshold: specifies how accurate the match between any adapter etc. sequence must be against a read.</li> </ul> <pre><code>#!/usr/bin/bash\n# trimming the mouseMT fastq files\n\n\n## creating output folder, in case it does not exists\nmkdir -p 030_d_trim\n\n## we store the input folder in a variable, \n## to access its value, we will write $INPUT_FOLDER\nINPUT_FOLDER=/data/DATA/mouseMT\n\n## by ending a line with \\ we can continue the same command on the line below\n\ntrimmomatic SE -phred33 \\\n$INPUT_FOLDER/sample_a1.fastq \\\n030_d_trim/sample_a1.trimmed.fastq \\\nILLUMINACLIP:/data/DATA/adapters/TruSeq3-PE.fa:2:30:10 \\\nSLIDINGWINDOW:3:25 2&gt; 030_d_trim/030_l_trim_out.sample_a1.log\n\ntrimmomatic SE -phred33 \\\n$INPUT_FOLDER/sample_a2.fastq \\\n030_d_trim/sample_a2.trimmed.fastq \\\nILLUMINACLIP:/data/DATA/adapters/TruSeq3-PE.fa:2:30:10 \\\nSLIDINGWINDOW:3:25 2&gt; 030_d_trim/030_l_trim_out.sample_a2.log\n\ntrimmomatic SE -phred33 \\\n$INPUT_FOLDER/sample_a3.fastq \\\n030_d_trim/sample_a3.trimmed.fastq \\\nILLUMINACLIP:/data/DATA/adapters/TruSeq3-PE.fa:2:30:10 \\\nSLIDINGWINDOW:3:25 2&gt; 030_d_trim/030_l_trim_out.sample_a3.log\n\ntrimmomatic SE -phred33 \\\n$INPUT_FOLDER/sample_a4.fastq \\\n030_d_trim/sample_a4.trimmed.fastq \\\nILLUMINACLIP:/data/DATA/adapters/TruSeq3-PE.fa:2:30:10 \\\nSLIDINGWINDOW:3:25 2&gt; 030_d_trim/030_l_trim_out.sample_a4.log\n\n\ntrimmomatic SE -phred33 \\\n$INPUT_FOLDER/sample_b1.fastq \\\n030_d_trim/sample_b1.trimmed.fastq \\\nILLUMINACLIP:/data/DATA/adapters/TruSeq3-PE.fa:2:30:10 \\\nSLIDINGWINDOW:3:25 2&gt; 030_d_trim/030_l_trim_out.sample_b1.log\n\ntrimmomatic SE -phred33 \\\n$INPUT_FOLDER/sample_b2.fastq \\\n030_d_trim/sample_b2.trimmed.fastq \\\nILLUMINACLIP:/data/DATA/adapters/TruSeq3-PE.fa:2:30:10 \\\nSLIDINGWINDOW:3:25 2&gt; 030_d_trim/030_l_trim_out.sample_b2.log\n\ntrimmomatic SE -phred33 \\\n$INPUT_FOLDER/sample_b3.fastq \\\n030_d_trim/sample_b3.trimmed.fastq \\\nILLUMINACLIP:/data/DATA/adapters/TruSeq3-PE.fa:2:30:10 \\\nSLIDINGWINDOW:3:25 2&gt; 030_d_trim/030_l_trim_out.sample_b3.log\n\ntrimmomatic SE -phred33 \\\n$INPUT_FOLDER/sample_b4.fastq \\\n030_d_trim/sample_b4.trimmed.fastq \\\nILLUMINACLIP:/data/DATA/adapters/TruSeq3-PE.fa:2:30:10 \\\nSLIDINGWINDOW:3:25 2&gt; 030_d_trim/030_l_trim_out.sample_b4.log\n</code></pre> <p>On the server, this script is also in <code>/data/Solutions/mouseMT/030_s_trim.sh</code></p> alternative script using a for loop <p>First create a file named <code>sampleNames.txt</code>, containing the sample names:</p> <p><pre><code>sample_a1\nsample_a2\nsample_a3\nsample_a4\nsample_b1\nsample_b2\nsample_b3\nsample_b4\n</code></pre> it can also be found in the server at <code>/data/Solutions/mouseMT/sampleNames.txt</code></p> <pre><code>#!/usr/bin/bash\n# trimming the mouseMT fastq files\n\n\n## creating output folder, in case it does not exists\nmkdir -p 030_d_trim\n\nINPUT_FOLDER=/data/DATA/mouseMT\n\n## each job grab a specific line from sampleNames.txt\nfor SAMPLE in `cat sampleNames.txt`\ndo\n trimmomatic SE -phred33 \\\n              $INPUT_FOLDER/${SAMPLE}.fastq \\\n              030_d_trim/${SAMPLE}.trimmed.fastq \\\n              ILLUMINACLIP:/data/DATA/adapters/TruSeq3-PE.fa:2:30:10 \\\n              SLIDINGWINDOW:3:25 2&gt; 030_d_trim/030_l_trim_out.${SAMPLE}.log\ndone\n</code></pre> <p>On the server, this script is also in   <code>/data/Solutions/mouseMT/030bis_s_trim_loop.sh</code></p> <p>Task 2: </p> <ul> <li>Use the the following script to run a QC analysis on your trimmmed reads and compare with the raw ones.</li> </ul> <p><pre><code>#!/usr/bin/bash\n# QC abalysis of the mouseMT trimmed reads\n\n## fastQC on trimmed fastq files\nfastqc 030_d_trim/*.fastq -o 030_d_trim\n\n\n## multiqc on the fastQC reports AND the trimmomatic logs\nmultiqc -n 032_r_multiqc_mouseMT_trimmed.html -f --title trimmed_fastq 030_d_trim/\n</code></pre> On the server, you can find this script in : <code>/data/Solutions/mouseMT/032_s_multiqc_trimmed.sh</code></p> <p>Note</p> <p>The script above presumes that you have successfully trimmed the reads. </p> <p>If not, you can grab them on the server in <code>/data/Solutions/mouseMT/030_d_trim/</code></p> trimmed reads multiqc report <p> Download the multiqc report</p> <p>Note the second section, Trimmomatic, which lets you know the number/percentage of reads dropped</p> <p>Extra Task: if you have the time</p> <p>We have trimmed the Liu2015 data with trimmomatic with the following option:</p> <ul> <li>SLIDINGWINDOW:4:20 </li> <li>MINLEN:36 </li> <li>ILLUMINACLIP:/data/DATA/adapters/TruSeq3-PE.fa:2:30:10 </li> </ul> <p>Download the multiQC report:</p> <p> Liu2015 trim multiqc report</p> <p>Analyze it : what do you think? Did the trimming do its job?</p>"},{"location":"days/unix_crash/","title":"UNIX crash course","text":"<p>The goal of this crash-course is to get you acquainted with some the basic concepts and commands of the UNIX command line.</p> <p>Mastering it is not something done in a single day, and so the goal is not to make you into sysadmins, but to give you enough groundings so that, armed with a good cheatsheet and with a bit of patience, you should be able to run the commands needed to perform a differential expression or enrichment analysis and then be able to interact with your result files.</p> <p>This crash course is made to be followed along either on a distant Rstudio server (the teacher will tell you how to connect there)</p> <p>You will learn to :</p> <ul> <li>navigate the filesystem using common unix commands</li> <li>look at text files and manipulate them </li> <li>use command line tools options</li> <li>redirect command line tools output to files</li> </ul>"},{"location":"days/unix_crash/#start-a-terminal-in-rstudio","title":"start a terminal in Rstudio","text":""},{"location":"days/unix_crash/#filesystem-navigation","title":"filesystem navigation","text":"<p>type commands and then press enter to execute them</p>"},{"location":"days/unix_crash/#pwd","title":"pwd","text":"<p><code>pwd</code> = print working directory : tells you were the ternminal is in the filesystem <pre><code>pwd\n</code></pre></p>"},{"location":"days/unix_crash/#ls","title":"ls","text":"<p><code>ls</code> = list : list the content of the working directory <pre><code>ls\n</code></pre></p> <p>Giving arguments to command - 1 : some commands take arguments (very much like function in R).</p> <p>Arguments are given separated by spaces.</p> <p>For example, <code>ls</code> can take an argument corresponding the name of a directory, in which case it will list the content of that directory rather than the current working directory:</p> <pre><code>ls R_crash_course_data\n</code></pre>"},{"location":"days/unix_crash/#cd","title":"cd","text":"<p><code>cd</code> = change directory : moves the current working directory to the folder given as argument <pre><code>cd R_crash_course_data\n\npwd # check the new working directory\n</code></pre></p> <p>.. is a shortcut for \u201cparent directory\u201d <pre><code>cd .. </code></pre></p> <p>. is a shortcut for \u201ccurrent directory\u201d <pre><code>cd . # we move to the current directory, so we stay in place\n</code></pre></p> <p>cd without argument will take you back to your home directory <pre><code>cd\n</code></pre></p> <p>You can move through multiple folders in one go <pre><code>cd R_crash_course_data/data/\n</code></pre></p> <p>Giving arguments to command - 2 :</p> <p>Some arguments are given by name, in which case they start with <code>-</code> (one letter argument) or <code>--</code> (more than one letter argument).</p> <p>Also some arguments require a value, but some are just \u201cflags\u201d to merely need to be added to the command line to take effect:</p> <pre><code>ls --color=auto\n\nls -l\n</code></pre> <p>One can combine multiple one-letter argument with the same <code>-</code></p> <pre><code>ls -l -h ##equivalent to\nls -lh </code></pre> <p>So in the end you may have a command line looking like </p> <pre><code>ls -lh --color=auto </code></pre> <p>Its output looks like: <pre><code>total 5.3M\n-rw-rw-r-- 1 wandrille wandrille 2.7M Aug 15 13:22 diamonds.csv\n-rw-rw-r-- 1 wandrille wandrille 2.7M Aug 15 13:22 diamonds.tsv\n-rw-rw-r-- 1 wandrille wandrille 1.4K Aug 15 13:22 virginica.csv\n</code></pre></p> <ul> <li>The first line tells you to total size of the files in the folder (but it does not account for sub-folders)</li> </ul> file permissions # of links owner group size last modification filename -rw-rw-r\u2013 1 wandrille wandrille 2.7M Aug   15 13:22 diamonds.csv <p>File permission are read write execute</p> directory? owner permissions group permissions others permission - rw- rw- r\u2013 not a directory can read and write can read and write can read only"},{"location":"days/unix_crash/#moving-files-around","title":"moving files around","text":"<p>First let\u2019s move back to our home folder <pre><code>cd\n</code></pre></p>"},{"location":"days/unix_crash/#mkdir","title":"mkdir","text":"<p><code>mkdir</code> = make directory : creates a directory <pre><code>mkdir unix_exercise\n\nls -lh # to check that the directory was created\n</code></pre></p>"},{"location":"days/unix_crash/#cp","title":"cp","text":"<p><code>cp</code> = copy : copies a file to a given location <pre><code>cp R_crash_course_data/iris.csv .  # copying the iris.csv file to the current directory\n\nls -lh --color=auto  # checking the result\n</code></pre></p> <p>Entire folder can be recursively copied, but you need to add the <code>-r</code> flag <pre><code> cp R_crash_course_data/data/ unix_exercise/ #trying to copy the data/ folder to unix_exercise/\n</code></pre> gives the error: <pre><code>cp: -r not specified; omitting directory 'R_crash_course_data/data/'\n</code></pre></p> <pre><code>cp -r R_crash_course_data/data/ unix_exercise/\n\nls -Rlh --color=auto  unix_exercise ## checking the result; -R makes a recursive ls\n</code></pre>"},{"location":"days/unix_crash/#mv","title":"mv","text":"<p><code>mv</code> = move : moves file to a given location <pre><code>ls # check that iris.csv is here\n\nmv iris.csv unix_exercise # move iris.csv to the unix_exercise/ folder\n\nls # check that iris.csv is not here anymore\nls unix_exercise # check that iris.csv is in unix_exercise/\n</code></pre></p> <p>it can also be used to change the name of a file or folder <pre><code>cd unix_exercise\n\nmv iris.csv IRIS.csv\nls\n</code></pre></p>"},{"location":"days/unix_crash/#rm","title":"rm","text":"<p><code>rm</code> = remove : removes files or folder </p> <p>Warning</p> <p>there is no recycle bin, or trash bin, or whatever. Any deletion is final.</p> <p>Warning</p> <p>I repeat: any deletion is final.</p> <p>let\u2019s create a copy of IRIS.csv to delete it: <pre><code>cp IRIS.csv iris_copy.csv\nls </code></pre></p> <pre><code>rm IRIS.csv\n</code></pre> <p>folder\u2019s can be deleted with the -r option <pre><code>rm -r data\n</code></pre></p>"},{"location":"days/unix_crash/#wildcard-character-the-of-the-show","title":"wildcard character : the * of the show","text":"<p><code>*</code> is the wildcard character, which can subtitute itself to any number of characters.</p> <p>we use it to craft powerful regular expressions that let us apply commands to selected files based on their names:</p> <p><pre><code># copy all files in ../R_crash_course_data/data/ to the current working directory\ncp ../R_crash_course_data/data/* .\n\nls\n</code></pre> <code>diamonds.csv  diamonds.tsv  iris_copy.csv  virginica.csv</code></p> <p>here the <code>*</code> substituted itself for all possibles file names.</p> <p>Where it shine is that it can be combined with other characters:</p> <pre><code>ls -l *.csv ## all csv files\n\nls -l diamonds.* ## all files starting with diamonds\n</code></pre>"},{"location":"days/unix_crash/#exercise-basic-file-system-manipulation","title":"exercise : basic file system manipulation","text":"<ol> <li>change your working directory back to your home directory</li> <li>create a folder named <code>unix_exercise2</code></li> <li>look at the files in a folder named <code>/data/DATA/mouseMT</code>. What is the size of each file? Do you have permission to write there?</li> <li>copy all sample_a files from <code>/data/DATA/mouseMT</code> to <code>unix_exercise2</code></li> <li>look at the files you just copied: do you have write permission with them now?</li> </ol>"},{"location":"days/unix_crash/#looking-at-file-content","title":"looking at file content","text":"<p>Let\u2019s move to the <code>unix_exercise</code> folder <pre><code>cd # to your home directory first\ncd unix_exercise </code></pre></p>"},{"location":"days/unix_crash/#head-tail","title":"head / tail","text":"<p><code>head</code> shows you the first 10 lines of a file <pre><code>head diamonds.csv\n</code></pre> <pre><code>\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"\n0.23,\"Ideal\",\"E\",\"SI2\",61.5,55,326,3.95,3.98,2.43\n0.21,\"Premium\",\"E\",\"SI1\",59.8,61,326,3.89,3.84,2.31\n0.23,\"Good\",\"E\",\"VS1\",56.9,65,327,4.05,4.07,2.31\n0.29,\"Premium\",\"I\",\"VS2\",62.4,58,334,4.2,4.23,2.63\n0.31,\"Good\",\"J\",\"SI2\",63.3,58,335,4.34,4.35,2.75\n0.24,\"Very Good\",\"J\",\"VVS2\",62.8,57,336,3.94,3.96,2.48\n0.24,\"Very Good\",\"I\",\"VVS1\",62.3,57,336,3.95,3.98,2.47\n0.26,\"Very Good\",\"H\",\"SI1\",61.9,55,337,4.07,4.11,2.53\n0.22,\"Fair\",\"E\",\"VS2\",65.1,61,337,3.87,3.78,2.49\n</code></pre></p> <p>with <code>-n</code> you can specify the number of lines to show <pre><code>head -n 3 diamonds.csv\n</code></pre> <pre><code>\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"\n0.23,\"Ideal\",\"E\",\"SI2\",61.5,55,326,3.95,3.98,2.43\n0.21,\"Premium\",\"E\",\"SI1\",59.8,61,326,3.89,3.84,2.31\n</code></pre></p> <p><code>tail</code> shows you the last lines of a file: <pre><code>tail -n 3 diamonds.csv\n</code></pre> <pre><code>0.7,\"Very Good\",\"D\",\"SI1\",62.8,60,2757,5.66,5.68,3.56\n0.86,\"Premium\",\"H\",\"SI2\",61,58,2757,6.15,6.12,3.74\n0.75,\"Ideal\",\"D\",\"SI2\",62.2,55,2757,5.83,5.87,3.64\n</code></pre></p>"},{"location":"days/unix_crash/#more","title":"more","text":"<p><code>more</code> prints a file content in the terminal. When the file is large it prints a page, and you can press  * <code>enter</code> : print one more line  * <code>space</code> : print one more page  * <code>q</code> : stop printing</p> <pre><code>more diamonds.csv\n</code></pre>"},{"location":"days/unix_crash/#wc","title":"wc","text":"<p>wc = word count : counts the number of lines, words, and characters in a file</p> <p><pre><code>wc diamonds.csv\n</code></pre> result: <code>53941   66023 2772143 diamonds.csv</code></p> <p>We use it most often with option <code>-l</code> to get just the number of lines: <pre><code>wc -l iris_copy.csv\n</code></pre> result:  <code>151 iris_copy.csv</code></p>"},{"location":"days/unix_crash/#exercise-lookoing-at-files","title":"exercise : lookoing at files","text":"<ol> <li>look at the first and last lines of <code>/data/DATA/mouseMT/sample_a1.fastq</code></li> <li>we have the logs of a Quality Control tool in file <code>/data/Solutions/Liu2015/010_l_fastqc_Liu2015.o</code>. Look at its content with more or less : does it look like all files where processed OK?</li> <li>how many lines are in each of the fastq files <code>/data/DATA/mouseMT/</code> (hint: use * to make your life easier)</li> </ol>"},{"location":"days/unix_crash/#get-help","title":"get \u2013help","text":"<p>Of course, you cannot guess all commands arguments. </p> <p>You can consult the manual page of some command with <code>man</code>, but not all tools have one\u2026</p> <p>Instead, in general you can get a list of arguments with <code>--help</code> (or sometimes <code>-h</code>)</p> <pre><code>ls --help\n</code></pre>"},{"location":"days/unix_crash/#advanced-but-useful-stuff","title":"advanced but useful stuff","text":""},{"location":"days/unix_crash/#output-redirection","title":"output redirection : &gt;","text":"<p><code>&gt;</code> let\u2019s you make it so that all standard output (but not error messages) of a command go to a file you specify</p> <p>This is very useful when you call a tool whose logs you would like to keep (the logs of many bioinformatics tools can be input files for other tools).</p> <pre><code>wc -l /data/DATA/mouseMT/*.fastq &gt; mouseMT.file_sizes.txt\n\nmore mouseMT.file_sizes.txt\n</code></pre>"},{"location":"days/unix_crash/#emergency-exit-ctrlc","title":"emergency exit : Ctrl+C","text":"<p>Sometimes you will launch a job, which may take a while and you will realize, with horror, that you made a mistake and this is not what you wanted to do!</p> <p><code>Ctrl+C</code> stops the execution of whatever command is curently running.</p> <p>For example, the following command counts number of lines in big files, which takes a while. Let it run for a couple of seconds and then stop it with <code>Ctrl+C</code> <pre><code>wc -l /data/DATA/Liu2015/*\n</code></pre></p>"},{"location":"days/unix_crash/#background-job","title":"background job : &amp;","text":"<p>Ooften you want to launch a job which you know will take a while, but you\u2019d like to be able to continue working while it runs. </p> <p>By adding <code>&amp;</code> at the end of the command line the job will be executed in the background. </p> <pre><code>wc -l /data/DATA/Liu2015/* &gt; Liu2015.file_sizes.txt &amp;\n</code></pre> <p>You can use <code>top</code> or <code>ps -u</code> to see your job running (with <code>top</code>, type <code>q</code> to exit).</p> <p>When the command is finished you will get in your terminal something like:</p> <pre><code>[1]+  Done                    wc -l /data/DATA/Liu2015/* &gt; Liu2015.file_sizes.txt\n</code></pre>"},{"location":"days/unix_crash/#variables","title":"variables","text":"<p>As in R, is it possible to store data in a variable.</p> <p>It is mostly useful to help us manipulate file names more easily.</p> <p>They are declared with <code>=</code> , but you have to be careful: use no spaces </p> <pre><code>INPUT_FOLDER=/data/DATA/mouseMT\n</code></pre> <p>then you can use them with ${variable_name}:</p> <pre><code>ls ${INPUT_FOLDER}/*\n\nls ${INPUT_FOLDER}/sample_a*\n</code></pre> <p>Note</p> <p>You will notice most of my bash variable are UPPERCASE. It is not mandatory but I find it a useful convention.</p> <p>Note</p> <p>instead of <code>${INPUT_FOLDER}</code>, we could write $INPUT_FOLDER but it is a bit more prone to mixup when integrating the variable content within other character, so beware.</p>"},{"location":"days/unix_crash/#loop","title":"loop","text":"<p>Loops let us \u201ceasily\u201d deploy the same command while changing 1 variable.</p> <p>This is very useful to deploy a bioinformatic tool on many input files in a tidy way.</p> <p>For example, imagine we want to deploy a command separately on the mouseMT fatsq files of sample a, and of sample b, it could look something like:</p> <pre><code># I use wc -l as my command here\n\nwc -l /data/DATA/mouseMT/sample_a*.fastq &gt; mouseMT.a.file_sizes.txt\n\nwc -l /data/DATA/mouseMT/sample_b*.fastq &gt; mouseMT.b.file_sizes.txt\n</code></pre> <p>That works, but imagine that you have more than a handful of options and it becomes tedious.</p> <p>Additionnally, speaking from experience it is easy to make small typos that will riun your day here (eg, adapting the input file names, but not the output file name )</p> <p>A for loop in bash looks like:</p> <pre><code>for VAR in a b\ndo # done starts the loop block\n\n## some command, which will be repeated, ${VAR} will have a different value each time\nwc -l /data/DATA/mouseMT/sample_${VAR}*.fastq &gt; mouseMT.${VAR}.file_sizes.txt\n\ndone # done finishes the loop block\n</code></pre>"}]}